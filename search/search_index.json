{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Container Camp 2023 \u00b6 What you will learn about \u00b6 Docker Containers for scientific research \u00b6 Introduction to Docker Finding the right container Containers on High Performance and High Throughput Computing \u00b6 Basics of SingularityCE Using SingularityCE on HPC and HTC What you leave with \u00b6 An understanding of development, maintenence, and deployment of containers within the research lifecycle. One year of free support and consultation on your container builds with CyVerse Research Software Engineers and Data Scientists. A better understanding of and access to the most powerful public research computing infrastructure in the world A better understanding of commercial cloud services and costs and how to best leverage them for your research Twitter hash tag: #cc2023 Funding and Citations: CyVerse is funded entirely by the National Science Foundation under Award Numbers: Please cite CyVerse appropriately when you make use of our resources; see CyVerse citation policy .","title":"Home"},{"location":"#welcome-to-container-camp-2023","text":"","title":" Welcome to Container Camp 2023 "},{"location":"#what-you-will-learn-about","text":"","title":"What you will learn about"},{"location":"#docker-containers-for-scientific-research","text":"Introduction to Docker Finding the right container","title":" Docker Containers for scientific research"},{"location":"#containers-on-high-performance-and-high-throughput-computing","text":"Basics of SingularityCE Using SingularityCE on HPC and HTC","title":" Containers on High Performance and High Throughput Computing"},{"location":"#what-you-leave-with","text":"An understanding of development, maintenence, and deployment of containers within the research lifecycle. One year of free support and consultation on your container builds with CyVerse Research Software Engineers and Data Scientists. A better understanding of and access to the most powerful public research computing infrastructure in the world A better understanding of commercial cloud services and costs and how to best leverage them for your research Twitter hash tag: #cc2023 Funding and Citations: CyVerse is funded entirely by the National Science Foundation under Award Numbers: Please cite CyVerse appropriately when you make use of our resources; see CyVerse citation policy .","title":"What you leave with"},{"location":"cloud/codespaces/","text":"GitHub CodeSpaces \u00b6 For this workshop, we are working in GitHub CodeSpaces . You will be given access to an organization where CodeSpaces have been enabled for the duration of the workshop. What is a 'Development Environment'? A development environment or \"dev environment\" for short, is a place where software can be written and tested without impacting users or \"production environments\" as part of the software lifecycle. Containers-within-containers are another way to think about this paradigm. We create a containerized Integrated Development Environment (IDE) container which has all of our favorite software development tools and visual software (e.g., VSCode) where we can write our program and try out new package installations. Terminology: Development Environment : Environment for development tasks such as designing, programming, debugging, etc. Test Environment : an environment with the same configuration as the \"production environment\" where testing can be done without interrupting the main service, also called Q/A or \"Quality Assurance Environment\". Staging Environment : Where the work from testing is merged into the built system before public release. Production Environment : The environment where users interact with the now-public tools. GitHub CodeSpaces provides you with a fully featured Dev Environment running on Microsoft Azure . You have been granted access to GitHub Education , and can use CodeSpaces and CoPilot during the workshop. CodeSpace links with your GitHub account for a seamless experience working on code in a Git repository. CoPilot is an AI programmer assistant that can help you write code using comments as prompts. How can you get your own educational GitHub & CodeSpaces? You can gain access to discounted GitHub CodeSpaces by enrolling your GitHub account with GitHub Education and then applying the educator discount to your organizations and repos. Not interested in using GitHub based dev environments? Check out GitPod or GitLub Education and Developer Environments How do I use CoPilot? Install and enable CoPilot on your VSCode (locally, logging into GitHub, or remotely in CodeSpace). Try adding a comment to the top of a file, like your new Dockerfile , wait a few seconds # Create a Dockerfile which uses an Ubuntu 22.04 featured base image, FROM ubuntu:22.04 # then install Python 3.10 and pip, RUN apt-get update \\ && apt-get install -y python3.10 python3-pip # and give it an ENTRYPOINT to run Python 3.10 ENTRYPOINT [ \"python3.10\" ] Starting a CodeSpace \u00b6 When a GitHub Organization and Repository have CodeSpaces enabled you will see a \"Code\" button above the README.md Click on the \"Code\" button and start a new CodeSpace Select the size of the CodeSpace you want (2-4 cores and 4GB to 8GB of RAM should be plenty for today) Click \"Create CodeSpace\" You will be taken to a loading screen, and after a few moments (<2 minutes) your browser will change to a VS Code instance in your browser. Notice, the GitHub repository where you initiated the CodeSpace is set as the working directory of the EXPLORER in the upper left side of VS Code interface. You're in your Git repo, and are able to work with Python, Docker, Node, or any one of many featured developer tools. Further, you can install any tools you like!","title":"GitHub CodeSpaces"},{"location":"cloud/codespaces/#github-codespaces","text":"For this workshop, we are working in GitHub CodeSpaces . You will be given access to an organization where CodeSpaces have been enabled for the duration of the workshop. What is a 'Development Environment'? A development environment or \"dev environment\" for short, is a place where software can be written and tested without impacting users or \"production environments\" as part of the software lifecycle. Containers-within-containers are another way to think about this paradigm. We create a containerized Integrated Development Environment (IDE) container which has all of our favorite software development tools and visual software (e.g., VSCode) where we can write our program and try out new package installations. Terminology: Development Environment : Environment for development tasks such as designing, programming, debugging, etc. Test Environment : an environment with the same configuration as the \"production environment\" where testing can be done without interrupting the main service, also called Q/A or \"Quality Assurance Environment\". Staging Environment : Where the work from testing is merged into the built system before public release. Production Environment : The environment where users interact with the now-public tools. GitHub CodeSpaces provides you with a fully featured Dev Environment running on Microsoft Azure . You have been granted access to GitHub Education , and can use CodeSpaces and CoPilot during the workshop. CodeSpace links with your GitHub account for a seamless experience working on code in a Git repository. CoPilot is an AI programmer assistant that can help you write code using comments as prompts. How can you get your own educational GitHub & CodeSpaces? You can gain access to discounted GitHub CodeSpaces by enrolling your GitHub account with GitHub Education and then applying the educator discount to your organizations and repos. Not interested in using GitHub based dev environments? Check out GitPod or GitLub Education and Developer Environments How do I use CoPilot? Install and enable CoPilot on your VSCode (locally, logging into GitHub, or remotely in CodeSpace). Try adding a comment to the top of a file, like your new Dockerfile , wait a few seconds # Create a Dockerfile which uses an Ubuntu 22.04 featured base image, FROM ubuntu:22.04 # then install Python 3.10 and pip, RUN apt-get update \\ && apt-get install -y python3.10 python3-pip # and give it an ENTRYPOINT to run Python 3.10 ENTRYPOINT [ \"python3.10\" ]","title":"GitHub  CodeSpaces"},{"location":"cloud/codespaces/#starting-a-codespace","text":"When a GitHub Organization and Repository have CodeSpaces enabled you will see a \"Code\" button above the README.md Click on the \"Code\" button and start a new CodeSpace Select the size of the CodeSpace you want (2-4 cores and 4GB to 8GB of RAM should be plenty for today) Click \"Create CodeSpace\" You will be taken to a loading screen, and after a few moments (<2 minutes) your browser will change to a VS Code instance in your browser. Notice, the GitHub repository where you initiated the CodeSpace is set as the working directory of the EXPLORER in the upper left side of VS Code interface. You're in your Git repo, and are able to work with Python, Docker, Node, or any one of many featured developer tools. Further, you can install any tools you like!","title":"Starting a CodeSpace"},{"location":"cloud/js2/","text":"Jetstream-2 \u00b6 Create an XSEDE account Request a Jetstream-2 Official Jetstream-2 Documentation","title":"Jetstream-2"},{"location":"cloud/js2/#jetstream-2","text":"Create an XSEDE account Request a Jetstream-2 Official Jetstream-2 Documentation","title":"Jetstream-2"},{"location":"docker/actions/","text":"GitHub Actions and Docker \u00b6 GitHub Actions is a feature that allows automation and execution of workflows invoved in the development of your software and code. Read more on GitHub Actions at the offical GitHub Docs page . Actions can also be used to create and manage your Docker Images. Here you will learn how to can create Actions which build and then push images from GitHub to a Docker Registry of your choice. Prerequisites \u00b6 A GitHub account A Docker account Setting up a Git Repository with Actions for Docker \u00b6 CI/CD Terminology continuous integration builds, tests, and automatically accepts changes to code in a repository continuous delivery delivers code changes to production-ready environments continuous deployment does just that, it deploys changes to the code CI pipeline compiles code, tests it, and makes sure all of your changes work. It should run whenever there is a change (push) to the repository CD pipeline goes one step further and deploys new code into production. We are focusing on GitHub, but there are other platforms which you can explore for building and pushing containers to registries These include GitLab Runners Other types of Continuous Integration are used on code repositories to ensure that code stays functional. Create a GitHub Repository \u00b6 Navigate to your GitHub Account and select New ; Creating README, LICENSE, & .gitignore When you create your new git repository you are asked to create a README , a LICENCE , and a .gitignore file. Go ahead and create all three of these, as they are useful and fundamental to making your repository reproducible. README -- we want to use the README to help future-us when we revisit this repository. Make sure to include detailed instructions LICENSE -- pick a license which is useful for your specific software use case. .gitignore -- use a file which will keep docker project files isolated .gitignore example # Docker project generated files to ignore # if you want to ignore files created by your editor/tools, # please consider a global .gitignore https://help.github.com/articles/ignoring-files .vagrant* bin docker/docker .*.swp a.out *.orig build_src .flymake* .idea .DS_Store docs/_build docs/_static docs/_templates .gopath/ .dotcloud *.test bundles/ .hg/ .git/ vendor/pkg/ pyenv Vagrantfile In the repository, create two nested folders required for a Docker Actions project: A .github/workflows folder: containing required necessary yml files that build the containers through GitHub; In the workflows folder, we're going to add a .yml In the main repository, along with the README and LICENCE file, create another folder called /docker In the /docker folder we're going to put the Dockerfile file necessary to build the image. Link your GitHub and Docker accounts \u00b6 Ensure you can access Docker Hub from any workflows you create: Add your Docker ID as a secret to GitHub. Navigate to your GitHub repository and click Settings > Secrets > New secret . 2. Create a new secret with the name DOCKER_HUB_USERNAME and your Docker ID as value. 3. On DockerHub, create a new Personal Access Token (PAT). To create a new token, go to Docker Hub Settings and then click New Access Token . Name it, and copy the newly generated PAT. Tip Name your Docker Hub Access token the same name as your GitHub repository, it will help with keeping track which GitHub repository is related to which Docker image. On GitHub, return to your repository secrets, and add the PAT as a secret with the name DOCKER_HUB_ACCESS_TOKEN . Setting up a GitHub Action Workflow \u00b6 Now that you have connected your GitHub repository with your Docker account, you are ready to add the necessary files to your repo. Note In this example, we will use the existing Docker Image Alpine. 1. In your GitHub repository, create a file and name in Dockerfile ; In the first line of your Dockerfile paste: FROM alpine:3.14 2. Click the Actions tab and in the search bar, search for docker . Select the docker image workflow (as shown in the image below) Note This will create the .github/workflows repository and necessary yml file required for the GitHub actions. 3. You will be prompted to the docker-image.yml file; paste the following code, and commit your changes. name: Docker Image Small Alpine on: push: branches: [ main ] pull_request: branches: [ main ] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Login to Docker Hub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKER_HUB_USERNAME }} password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }} - name: Build and push uses: docker/build-push-action@v2 with: context: . file: ./Dockerfile push: true tags: ${{ secrets.DOCKER_HUB_USERNAME }}/simplewhale 4. Upon committing and pushing your changes, you can check your Workflows under the Actions tab on GitHub. Note Github will show you when a workflow is building: - An orange dot next to your commit count means that the workflow is running; - A crossed red circle means that your workflow has failed; - A green check means your workflow ran successfully. 5. Navigate to your Docker Hub to see your GitHub Actions generated Docker image. Tagging a Pushing an Image \u00b6 When updating your own Docker Images, you may want to tag the images with a version number. In this example, we update our Alpine Docker Image to a newer version and adding a package. Navigate to your Dockerfile and modify it by adding the following lines: FROM alpine:3.15 LABEL author=\"<name>\" LABEL email=\"<email>\" LABEL date_created=\"<date>\" RUN apk update && apk upgrade --available RUN apk add vim fortune ENV PATH=/usr/games:${PATH} ENV LC_ALL=C ENTRYPOINT fortune Save, and navigate to the github action ( .github/workflows/docker-image.yml ). The last line should be tags , edit the line to reflect the version you would like to add. tags: ${{ secrets.DOCKER_HUB_USERNAME }}/simplewhale:v0.2 Warning To correctoy add a tag, add a colon followed by a version number such as <name of container>:<version number> . If there is no tag, the version pushed will default to latest . Remember to update/change the tag line in future pushes. Add, commit, push your changes; You will see your Docker Image being built with the added tagged version. Setting up your own \u00b6 GitHub Actions Runner Self-hosted Actions Runner Docs GitLab Runner","title":"GitHub Actions and Docker"},{"location":"docker/actions/#github-actions-and-docker","text":"GitHub Actions is a feature that allows automation and execution of workflows invoved in the development of your software and code. Read more on GitHub Actions at the offical GitHub Docs page . Actions can also be used to create and manage your Docker Images. Here you will learn how to can create Actions which build and then push images from GitHub to a Docker Registry of your choice.","title":"GitHub Actions and Docker"},{"location":"docker/actions/#prerequisites","text":"A GitHub account A Docker account","title":"Prerequisites"},{"location":"docker/actions/#setting-up-a-git-repository-with-actions-for-docker","text":"CI/CD Terminology continuous integration builds, tests, and automatically accepts changes to code in a repository continuous delivery delivers code changes to production-ready environments continuous deployment does just that, it deploys changes to the code CI pipeline compiles code, tests it, and makes sure all of your changes work. It should run whenever there is a change (push) to the repository CD pipeline goes one step further and deploys new code into production. We are focusing on GitHub, but there are other platforms which you can explore for building and pushing containers to registries These include GitLab Runners Other types of Continuous Integration are used on code repositories to ensure that code stays functional.","title":"Setting up a Git Repository with Actions for Docker"},{"location":"docker/actions/#create-a-github-repository","text":"Navigate to your GitHub Account and select New ; Creating README, LICENSE, & .gitignore When you create your new git repository you are asked to create a README , a LICENCE , and a .gitignore file. Go ahead and create all three of these, as they are useful and fundamental to making your repository reproducible. README -- we want to use the README to help future-us when we revisit this repository. Make sure to include detailed instructions LICENSE -- pick a license which is useful for your specific software use case. .gitignore -- use a file which will keep docker project files isolated .gitignore example # Docker project generated files to ignore # if you want to ignore files created by your editor/tools, # please consider a global .gitignore https://help.github.com/articles/ignoring-files .vagrant* bin docker/docker .*.swp a.out *.orig build_src .flymake* .idea .DS_Store docs/_build docs/_static docs/_templates .gopath/ .dotcloud *.test bundles/ .hg/ .git/ vendor/pkg/ pyenv Vagrantfile In the repository, create two nested folders required for a Docker Actions project: A .github/workflows folder: containing required necessary yml files that build the containers through GitHub; In the workflows folder, we're going to add a .yml In the main repository, along with the README and LICENCE file, create another folder called /docker In the /docker folder we're going to put the Dockerfile file necessary to build the image.","title":"Create a GitHub Repository"},{"location":"docker/actions/#link-your-github-and-docker-accounts","text":"Ensure you can access Docker Hub from any workflows you create: Add your Docker ID as a secret to GitHub. Navigate to your GitHub repository and click Settings > Secrets > New secret . 2. Create a new secret with the name DOCKER_HUB_USERNAME and your Docker ID as value. 3. On DockerHub, create a new Personal Access Token (PAT). To create a new token, go to Docker Hub Settings and then click New Access Token . Name it, and copy the newly generated PAT. Tip Name your Docker Hub Access token the same name as your GitHub repository, it will help with keeping track which GitHub repository is related to which Docker image. On GitHub, return to your repository secrets, and add the PAT as a secret with the name DOCKER_HUB_ACCESS_TOKEN .","title":"Link your GitHub and Docker accounts"},{"location":"docker/actions/#setting-up-a-github-action-workflow","text":"Now that you have connected your GitHub repository with your Docker account, you are ready to add the necessary files to your repo. Note In this example, we will use the existing Docker Image Alpine. 1. In your GitHub repository, create a file and name in Dockerfile ; In the first line of your Dockerfile paste: FROM alpine:3.14 2. Click the Actions tab and in the search bar, search for docker . Select the docker image workflow (as shown in the image below) Note This will create the .github/workflows repository and necessary yml file required for the GitHub actions. 3. You will be prompted to the docker-image.yml file; paste the following code, and commit your changes. name: Docker Image Small Alpine on: push: branches: [ main ] pull_request: branches: [ main ] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Login to Docker Hub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKER_HUB_USERNAME }} password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }} - name: Build and push uses: docker/build-push-action@v2 with: context: . file: ./Dockerfile push: true tags: ${{ secrets.DOCKER_HUB_USERNAME }}/simplewhale 4. Upon committing and pushing your changes, you can check your Workflows under the Actions tab on GitHub. Note Github will show you when a workflow is building: - An orange dot next to your commit count means that the workflow is running; - A crossed red circle means that your workflow has failed; - A green check means your workflow ran successfully. 5. Navigate to your Docker Hub to see your GitHub Actions generated Docker image.","title":"Setting up a GitHub Action Workflow"},{"location":"docker/actions/#tagging-a-pushing-an-image","text":"When updating your own Docker Images, you may want to tag the images with a version number. In this example, we update our Alpine Docker Image to a newer version and adding a package. Navigate to your Dockerfile and modify it by adding the following lines: FROM alpine:3.15 LABEL author=\"<name>\" LABEL email=\"<email>\" LABEL date_created=\"<date>\" RUN apk update && apk upgrade --available RUN apk add vim fortune ENV PATH=/usr/games:${PATH} ENV LC_ALL=C ENTRYPOINT fortune Save, and navigate to the github action ( .github/workflows/docker-image.yml ). The last line should be tags , edit the line to reflect the version you would like to add. tags: ${{ secrets.DOCKER_HUB_USERNAME }}/simplewhale:v0.2 Warning To correctoy add a tag, add a colon followed by a version number such as <name of container>:<version number> . If there is no tag, the version pushed will default to latest . Remember to update/change the tag line in future pushes. Add, commit, push your changes; You will see your Docker Image being built with the added tagged version.","title":"Tagging a Pushing an Image"},{"location":"docker/actions/#setting-up-your-own","text":"GitHub Actions Runner Self-hosted Actions Runner Docs GitLab Runner","title":"Setting up your own"},{"location":"docker/advanced/","text":"Advanced Docker Techniques \u00b6 Volumes \u00b6 When you run a container, you can bring a directory from the host system into the container, and give it a new name and location using the -v or --volume flag. $ mkdir -p ~/local-data-folder $ echo \"some data\" >> ~/local-data-folder/data.txt $ docker run -v ${ HOME } /local-data-folder:/data $YOUR_DOCKERHUB_USERNAME /mynotebook:latest cat /data/data.txt In the example above, you can mount a folder from your localhost, in your home user directory into the container as a new directory named /data . Create and manage volumes \u00b6 Unlike a bind mount, you can create and manage volumes outside the scope of any container. A given volume can be mounted into multiple containers simultaneously. When no running container is using a volume, the volume is still available to Docker and is not removed automatically. You can remove unused volumes using docker volume prune command. When you create a Docker volume, it is stored within a directory on the Docker Linux host ( /var/lib/docker/ ). Note File location on Mac OS X is a bit different: https://timonweb.com/posts/getting-path-and-accessing-persistent-volumes-in-docker-for-mac/ Let's create a volume $ docker volume create my-vol List volumes: $ docker volume ls local my-vol Inspect a volume by looking at the Mount section in the docker volume inspect $ docker volume inspect my-vol [ { \"Driver\" : \"local\" , \"Labels\" : {} , \"Mountpoint\" : \"/var/lib/docker/volumes/my-vol/_data\" , \"Name\" : \"my-vol\" , \"Options\" : {} , \"Scope\" : \"local\" } ] Remove a volume $ docker volume rm my-vol $ docker volume ls Populate a volume using a container \u00b6 This example starts an alpine container and populates the new volume output-vol with the some output created by the container. docker volume create output-vol docker run --name = data-app --mount source = output-vol,target = /data alpine sh -c 'env >> /data/container-env.txt' Use docker inspect output-vol to see where the volume data lives on your host, and then use cat to confirm that it contains the output created by the container. docker volume inspect output-vol sudo cat /var/lib/docker/volumes/output-vol/_data/container-env.txt You should see something like: HOSTNAME=790e13bba28a SHLVL=1 HOME=/root PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/ After running either of these examples, run the following commands to clean up the container and volume. docker rm data-app docker volume rm output-vol Bind mounts \u00b6 Bind mounts: When you use a bind mount, a file or directory on the host machine is mounted into a container. Tip If you are developing new Docker applications, consider using named volumes instead. You can't use Docker CLI commands to directly manage bind mounts. Warning One side effect of using bind mounts, for better or for worse, is that you can change the host filesystem via processes running in a container, including creating, modifying, or deleting important system files or directories. This is a powerful ability which can have security implications, including impacting non-Docker processes on the host system. If you use --mount to bind-mount a file or directory that does not yet exist on the Docker host, Docker does not automatically create it for you, but generates an error. Start a container with a bind mount \u00b6 Create a bind-data directory in your home directory. cd ~ mkdir -p ~/bind-data Run a container, mounting this directory inside the container, and the container should create some data in there. docker run --mount type = bind,source = \" $( pwd ) \" /bind-data,target = /data alpine sh -c 'env >> /data/container-env.txt' Check that the output looks right. cat ~/bind-data/container-env.txt Use a read-only bind mount \u00b6 For some development applications, the container needs to write into the bind mount, so changes are propagated back to the Docker host. At other times, the container only needs read access. This example modifies the one above but mounts the directory as a read-only bind mount, by adding ro to the (empty by default) list of options, after the mount point within the container. Where multiple options are present, separate them by commas. docker run --mount type = bind,source = \" $( pwd ) \" /bind-data,target = /data,readonly alpine sh -c 'ls -al /data/ && env >> /data/container-env.txt' You should see an error message about not being able to write to a read-only file system. sh: can ' t create /data/container-env.txt: Read-only file system","title":"Advanced use of Docker"},{"location":"docker/advanced/#advanced-docker-techniques","text":"","title":"Advanced Docker Techniques"},{"location":"docker/advanced/#volumes","text":"When you run a container, you can bring a directory from the host system into the container, and give it a new name and location using the -v or --volume flag. $ mkdir -p ~/local-data-folder $ echo \"some data\" >> ~/local-data-folder/data.txt $ docker run -v ${ HOME } /local-data-folder:/data $YOUR_DOCKERHUB_USERNAME /mynotebook:latest cat /data/data.txt In the example above, you can mount a folder from your localhost, in your home user directory into the container as a new directory named /data .","title":"Volumes"},{"location":"docker/advanced/#create-and-manage-volumes","text":"Unlike a bind mount, you can create and manage volumes outside the scope of any container. A given volume can be mounted into multiple containers simultaneously. When no running container is using a volume, the volume is still available to Docker and is not removed automatically. You can remove unused volumes using docker volume prune command. When you create a Docker volume, it is stored within a directory on the Docker Linux host ( /var/lib/docker/ ). Note File location on Mac OS X is a bit different: https://timonweb.com/posts/getting-path-and-accessing-persistent-volumes-in-docker-for-mac/ Let's create a volume $ docker volume create my-vol List volumes: $ docker volume ls local my-vol Inspect a volume by looking at the Mount section in the docker volume inspect $ docker volume inspect my-vol [ { \"Driver\" : \"local\" , \"Labels\" : {} , \"Mountpoint\" : \"/var/lib/docker/volumes/my-vol/_data\" , \"Name\" : \"my-vol\" , \"Options\" : {} , \"Scope\" : \"local\" } ] Remove a volume $ docker volume rm my-vol $ docker volume ls","title":"Create and manage volumes"},{"location":"docker/advanced/#populate-a-volume-using-a-container","text":"This example starts an alpine container and populates the new volume output-vol with the some output created by the container. docker volume create output-vol docker run --name = data-app --mount source = output-vol,target = /data alpine sh -c 'env >> /data/container-env.txt' Use docker inspect output-vol to see where the volume data lives on your host, and then use cat to confirm that it contains the output created by the container. docker volume inspect output-vol sudo cat /var/lib/docker/volumes/output-vol/_data/container-env.txt You should see something like: HOSTNAME=790e13bba28a SHLVL=1 HOME=/root PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/ After running either of these examples, run the following commands to clean up the container and volume. docker rm data-app docker volume rm output-vol","title":"Populate a volume using a container"},{"location":"docker/advanced/#bind-mounts","text":"Bind mounts: When you use a bind mount, a file or directory on the host machine is mounted into a container. Tip If you are developing new Docker applications, consider using named volumes instead. You can't use Docker CLI commands to directly manage bind mounts. Warning One side effect of using bind mounts, for better or for worse, is that you can change the host filesystem via processes running in a container, including creating, modifying, or deleting important system files or directories. This is a powerful ability which can have security implications, including impacting non-Docker processes on the host system. If you use --mount to bind-mount a file or directory that does not yet exist on the Docker host, Docker does not automatically create it for you, but generates an error.","title":"Bind mounts"},{"location":"docker/advanced/#start-a-container-with-a-bind-mount","text":"Create a bind-data directory in your home directory. cd ~ mkdir -p ~/bind-data Run a container, mounting this directory inside the container, and the container should create some data in there. docker run --mount type = bind,source = \" $( pwd ) \" /bind-data,target = /data alpine sh -c 'env >> /data/container-env.txt' Check that the output looks right. cat ~/bind-data/container-env.txt","title":"Start a container with a bind mount"},{"location":"docker/advanced/#use-a-read-only-bind-mount","text":"For some development applications, the container needs to write into the bind mount, so changes are propagated back to the Docker host. At other times, the container only needs read access. This example modifies the one above but mounts the directory as a read-only bind mount, by adding ro to the (empty by default) list of options, after the mount point within the container. Where multiple options are present, separate them by commas. docker run --mount type = bind,source = \" $( pwd ) \" /bind-data,target = /data,readonly alpine sh -c 'ls -al /data/ && env >> /data/container-env.txt' You should see an error message about not being able to write to a read-only file system. sh: can ' t create /data/container-env.txt: Read-only file system","title":"Use a read-only bind mount"},{"location":"docker/build/","text":"Building Docker Images \u00b6 Now that we are relatively comfortable with running Docker, we can look at some advanced Docker topics, such as: Building our own Docker images from the Dockerfile Modify an existing Dockerfile and create a new image Push an image to a Registry Requirements \u00b6 Clone our example repository with pre-written Dockerfiles From your CodeSpace, we are going to copy a second GitHub repository onto our VM. If you are working locally, make sure that you change directories away from any other Git repository that you may have been working in. $ cd /workspaces $ git clone https://github.com/cyverse-education/intro2docker $ cd intro2docker/ Writing a Dockerfile \u00b6 Important Dockerfile must be capitalized. It does not have a file extension. Create a file called Dockerfile , and add content to it as described below, e.g. $ touch Dockerfile Formatting in the Dockerfile We use a code line escape character \\ to allow single line scripts to be written on multiple lines in the Dockerfile. We also use the double characters && which essentially mean \u201cif true, then do this\u201d while executing the code. The && can come at the beginning of a line or the end when used with \\ . The Dockerfile contains Instructions : a series of commands that Docker executes during the creation and execution of a container. ARG \u00b6 The only command that can come before a FROM statement is ARG ARG can be used to set arguments for later in the build, e.g., ARG VERSION=latest FROM ubuntu:$VERSION FROM \u00b6 A valid Dockerfile must start with a FROM statement which initializes a new build stage and sets the base image for subsequent layers. We\u2019ll start by specifying our base image, using the FROM statement FROM ubuntu:latest If you are building on an arm64 or Windows system you can also give the optional --platform flag, e.g., FROM --platform=linux/amd64 ubuntu:latest When to use a multi-stage build pattern? Docker has the ability to build container images from one image, and run that \"builder\" image from a second \"base\" image, in what is called a \"builder pattern\". Build patterns are useful if you're compiling code from (proprietary) source code and only want to feature the binary code as an executed function in the container at run time. Build patterns can greatly reduce the size of your container. You can use multiple FROM commands as build stages. The AS statement follows the image:tag as a psuedo argument. # build stage FROM golang:latest AS build-env WORKDIR /go/src/app ADD . /go/src/app RUN go mod init RUN cd /go/src/app && go build -o hello # final stage FROM alpine:latest WORKDIR /app COPY --from=build-env /go/src/app /app/ ENTRYPOINT ./hello LABEL \u00b6 You can create labels which are then tagged as JSON metadata to the image LABEL author=\"your-name\" LABEL email=\"your@email-address\" LABEL version=\"v1.0\" LABEL description=\"This is your first Dockerfile\" LABEL date_created=\"2022-05-13\" You can also add labels to a container when it is run: $ docker run --label description=\"this label came later\" ubuntu:latest $ docker ps -a $ docker inspect ### RUN \u00b6 Different than the docker run command is the RUN build function. RUN is used to create new layers atop the \"base image\" Here, we are going to install some games and programs into our base image: RUN apt-get update && apt-get install -y fortune cowsay lolcat Here we've installed fortune cowsay and lolcat as new programs into our base image. Best practices for building new layers Ever time you use the RUN command it is a good idea to use the apt-get update or apt update command to make sure your layer is up-to-date. This can become a problem though if you have a very large container with a large number of RUN layers. ENV \u00b6 In our new container, we need to change and update some of the environment flags. We can do this using the ENV command ENV PATH=/usr/games:${PATH} ENV LC_ALL=C Here we are adding the /usr/games directory to the PATH so that when we run the new container it will find our newly installed game commands We are also updating the \" locales \" to set the language of the container. COPY \u00b6 The COPY command will copy files from the directory where Dockerfile is kept into the new image. You must specify where to copy the files or directories COPY . /app When to use COPY vs ADD COPY is more basic and is good for files ADD has some extra features like .tar extraction and URL support CMD \u00b6 The CMD command is used to run software in your image. In general use the [\"command\"] syntax: CMD [\"executable\", \"parameter1\", \"parameter2\"] ENTRYPOINT \u00b6 ENTRYPOINT works similarly to CMD but is designed to allow you to run your container as an executable. ENTRYPOINT fortune | cowsay | lolcat The default ENTRYPOINT of most images is /bin/sh -c which executes a shell command. ENTRYPOINT supports both the ENTRYPOINT [\"command\"] syntax and the ENTRYPOINT command syntax What is the difference in the ENTRYPOINT and CMD The CMD instruction is used to define what is execute when the container is run. The ENTRYPOINT instruction cannot be overridden, instead it is appended to when a new command is given to the docker run container:tag new-cmd statement the executable is defined with ENTRYPOINT, while CMD specifies the default parameter USER \u00b6 Most containers are run as root meaning that they have super-user privileges within themselves Typically, a new user is necessary in a container that is used interactively or may be run on a remote system. During the build of the container, you can create a new user with the adduser command and set up a /home/ directory for them. This new user would have something like 1000:1000 uid:gid permissions without sudo privileges. As a last step, the container is run as the new USER , e.g., ARG VERSION=18.04 FROM ubuntu:$VERSION RUN useradd ubuntu && \\ chown -R ubuntu:ubuntu /home/ubuntu USER ubuntu EXPOSE \u00b6 You can open ports using the EXPOSE command. EXPOSE 8888 The above command will expose port 8888. Note Running multiple containers using the same port is not trivial and would require the usage of a web server such as NGINX . However, you can have multiple containers interact with each other using Docker Compose . Summary of Instructions \u00b6 Instruction Command Description ARG Sets environmental variables during image building FROM Instructs to use a specific Docker image LABEL Adds metadata to the image RUN Executes a specific command ENV Sets environmental variables COPY Copies a file from a specified location to the image CMD Sets a command to be executed when running a container ENTRYPOINT Configures and run a container as an executable USER Used to set User specific information EXPOSE exposes a specific port","title":"Building Docker Containers"},{"location":"docker/build/#building-docker-images","text":"Now that we are relatively comfortable with running Docker, we can look at some advanced Docker topics, such as: Building our own Docker images from the Dockerfile Modify an existing Dockerfile and create a new image Push an image to a Registry","title":"Building Docker Images"},{"location":"docker/build/#requirements","text":"Clone our example repository with pre-written Dockerfiles From your CodeSpace, we are going to copy a second GitHub repository onto our VM. If you are working locally, make sure that you change directories away from any other Git repository that you may have been working in. $ cd /workspaces $ git clone https://github.com/cyverse-education/intro2docker $ cd intro2docker/","title":"Requirements"},{"location":"docker/build/#writing-a-dockerfile","text":"Important Dockerfile must be capitalized. It does not have a file extension. Create a file called Dockerfile , and add content to it as described below, e.g. $ touch Dockerfile Formatting in the Dockerfile We use a code line escape character \\ to allow single line scripts to be written on multiple lines in the Dockerfile. We also use the double characters && which essentially mean \u201cif true, then do this\u201d while executing the code. The && can come at the beginning of a line or the end when used with \\ . The Dockerfile contains Instructions : a series of commands that Docker executes during the creation and execution of a container.","title":"Writing a Dockerfile"},{"location":"docker/build/#arg","text":"The only command that can come before a FROM statement is ARG ARG can be used to set arguments for later in the build, e.g., ARG VERSION=latest FROM ubuntu:$VERSION","title":"ARG"},{"location":"docker/build/#from","text":"A valid Dockerfile must start with a FROM statement which initializes a new build stage and sets the base image for subsequent layers. We\u2019ll start by specifying our base image, using the FROM statement FROM ubuntu:latest If you are building on an arm64 or Windows system you can also give the optional --platform flag, e.g., FROM --platform=linux/amd64 ubuntu:latest When to use a multi-stage build pattern? Docker has the ability to build container images from one image, and run that \"builder\" image from a second \"base\" image, in what is called a \"builder pattern\". Build patterns are useful if you're compiling code from (proprietary) source code and only want to feature the binary code as an executed function in the container at run time. Build patterns can greatly reduce the size of your container. You can use multiple FROM commands as build stages. The AS statement follows the image:tag as a psuedo argument. # build stage FROM golang:latest AS build-env WORKDIR /go/src/app ADD . /go/src/app RUN go mod init RUN cd /go/src/app && go build -o hello # final stage FROM alpine:latest WORKDIR /app COPY --from=build-env /go/src/app /app/ ENTRYPOINT ./hello","title":"FROM"},{"location":"docker/build/#label","text":"You can create labels which are then tagged as JSON metadata to the image LABEL author=\"your-name\" LABEL email=\"your@email-address\" LABEL version=\"v1.0\" LABEL description=\"This is your first Dockerfile\" LABEL date_created=\"2022-05-13\" You can also add labels to a container when it is run: $ docker run --label description=\"this label came later\" ubuntu:latest $ docker ps -a $ docker inspect ###","title":"LABEL"},{"location":"docker/build/#run","text":"Different than the docker run command is the RUN build function. RUN is used to create new layers atop the \"base image\" Here, we are going to install some games and programs into our base image: RUN apt-get update && apt-get install -y fortune cowsay lolcat Here we've installed fortune cowsay and lolcat as new programs into our base image. Best practices for building new layers Ever time you use the RUN command it is a good idea to use the apt-get update or apt update command to make sure your layer is up-to-date. This can become a problem though if you have a very large container with a large number of RUN layers.","title":"RUN"},{"location":"docker/build/#env","text":"In our new container, we need to change and update some of the environment flags. We can do this using the ENV command ENV PATH=/usr/games:${PATH} ENV LC_ALL=C Here we are adding the /usr/games directory to the PATH so that when we run the new container it will find our newly installed game commands We are also updating the \" locales \" to set the language of the container.","title":"ENV"},{"location":"docker/build/#copy","text":"The COPY command will copy files from the directory where Dockerfile is kept into the new image. You must specify where to copy the files or directories COPY . /app When to use COPY vs ADD COPY is more basic and is good for files ADD has some extra features like .tar extraction and URL support","title":"COPY"},{"location":"docker/build/#cmd","text":"The CMD command is used to run software in your image. In general use the [\"command\"] syntax: CMD [\"executable\", \"parameter1\", \"parameter2\"]","title":"CMD"},{"location":"docker/build/#entrypoint","text":"ENTRYPOINT works similarly to CMD but is designed to allow you to run your container as an executable. ENTRYPOINT fortune | cowsay | lolcat The default ENTRYPOINT of most images is /bin/sh -c which executes a shell command. ENTRYPOINT supports both the ENTRYPOINT [\"command\"] syntax and the ENTRYPOINT command syntax What is the difference in the ENTRYPOINT and CMD The CMD instruction is used to define what is execute when the container is run. The ENTRYPOINT instruction cannot be overridden, instead it is appended to when a new command is given to the docker run container:tag new-cmd statement the executable is defined with ENTRYPOINT, while CMD specifies the default parameter","title":"ENTRYPOINT"},{"location":"docker/build/#user","text":"Most containers are run as root meaning that they have super-user privileges within themselves Typically, a new user is necessary in a container that is used interactively or may be run on a remote system. During the build of the container, you can create a new user with the adduser command and set up a /home/ directory for them. This new user would have something like 1000:1000 uid:gid permissions without sudo privileges. As a last step, the container is run as the new USER , e.g., ARG VERSION=18.04 FROM ubuntu:$VERSION RUN useradd ubuntu && \\ chown -R ubuntu:ubuntu /home/ubuntu USER ubuntu","title":"USER"},{"location":"docker/build/#expose","text":"You can open ports using the EXPOSE command. EXPOSE 8888 The above command will expose port 8888. Note Running multiple containers using the same port is not trivial and would require the usage of a web server such as NGINX . However, you can have multiple containers interact with each other using Docker Compose .","title":"EXPOSE"},{"location":"docker/build/#summary-of-instructions","text":"Instruction Command Description ARG Sets environmental variables during image building FROM Instructs to use a specific Docker image LABEL Adds metadata to the image RUN Executes a specific command ENV Sets environmental variables COPY Copies a file from a specified location to the image CMD Sets a command to be executed when running a container ENTRYPOINT Configures and run a container as an executable USER Used to set User specific information EXPOSE exposes a specific port","title":"Summary of Instructions"},{"location":"docker/compose/","text":"Running multi-container Applications with Docker Compose \u00b6 Docker Compose is an extension of Docker which allows you to run multiple containers synchronously and in communication with one another. Compose allows you to define and run a multi-container service using a Dockerfile and a docker-compose.yml . Note Docker for Mac and Docker Toolbox already include Compose along with other Docker apps, so Mac users do not need to install Compose separately. Docker for Windows and Docker Toolbox already include Compose along with other Docker apps, so most Windows users do not need to install Compose separately. For Linux users sudo curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose- ` uname -s ` - ` uname -m ` -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose Main advantages of Docker compose include: Your applications can be defined in a YAML file where all the same options required in docker run are now defined (reproducibility). It allows you to manage your application(s) as a single entity rather than dealing with starting individual containers (simplicity). Note For the sake of this example, either create your own Dockerfile or use the same Jupyter SciPy Notebook as in the Advanced Section Creating a docker-compose.yml \u00b6 Let's now create a Docker Compose .yml that calls Jupyter Notebook and RStudio 1. Create a folder shared_data in your current directory: mkdir shared_data 2. Create an empty docker-compose.yml file (e.g., touch docker-compose.yml) and paste the following lines version: \"3\" # All available services services: # Computation jupyter: container_name: \"jupyter_notebook\" image: \"jupyter/minimal-notebook\" restart: \"always\" environment: - JUPYTER_TOKEN=mytoken user: root volumes: - ./data:/home/jovyan/work/ ports: - 8888:8888 rstudio: container_name: \"rstudio\" image: \"rocker/rstudio\" restart: \"always\" environment: - DISABLE_AUTH=true volumes: - ./data:/home/rstudio ports: - 8787:8787 3. Run both Jupyter Lab and RStudio using docker-compose up instead of docker run . Note Handling containers with Docker Compose is fairly simple docker-compose up attaches the volumes, opens ports, and starts the container docker-compose down destroys the container A brief explanation of docker-compose.yml is as below: The web service builds from the Dockerfile in the current directory. Forwards the container's exposed port to port 8888 on the host. Mounts the project directory on the host to /work or /rstudio inside the container (allowing you to modify code without having to rebuild the image). restart: always means that it will restart whenever it fails. Running, shutting down, restarting docker-compose \u00b6 Run the containers with $ docker-compose up -d To stop running a running docker-compose session, either press CTRL + C or use the command: docker-compose down The above command removes containers, networks, volumes and images created by docker-container up . To restart a container, use the command docker-compose restart restart will restart the docker-compose service without taking into account changes one may have made to the yml or environment. Example using Docker-Compose: WebODM \u00b6 Warning For the purpose of these following examples it is not suggested to use GitHub Codespaces. Web Open Drone Map (WebODM) OpenDroneMap is an open source photogrammetry toolkit to process aerial imagery into maps and 3D models running on command line. WebODM (Web OpenDroneMap) is an extension of ODM running on multiple Docker Containers provinding a user friendly web interface for easy visualization. To use WebODM: Prerequisites WebODM requires docker , docker-compose to function. Additionally, if you are on Windows, users will be required to have the Docker Windows Application installed as well as having the WSL2 (Windows Subsystem for Linux) operational. Ensure your machine is up to date: sudo apt-get update Clone the WebODM repository: git clone https://github.com/OpenDroneMap/WebODM --config core.autocrlf=input --depth 1 Move into the WebODM folder: cd WebODM Run WebODM: sudo ./webodm.sh start The necessary docker images will be downloaded (~2 minutes) and WebODM will be accessible through http://localhost:8000/ Note You will be asked to create an account as a formality. Add any username and a password and select Create Account . 6. Download example data: clone https://github.com/OpenDroneMap/odm_data_aukerman.git . This git repository contains 77 HD images usable for WebODM. For other examples refer to ODMData . 7. In the WebODM portal, click on Select Images and GCP , navigate to odm_data_aukerman/images and select between 20-50 images (16 is the absolute minimum, whilst 32 is the suggested minimum). 8. WebODM will process the uploaded images (~5-10 minutes); upon completion, click View Map . 9. A map will open; you can click on 3D (bottom right) to see the 3D rendered model generated.","title":"Running multi-container Applications"},{"location":"docker/compose/#running-multi-container-applications-with-docker-compose","text":"Docker Compose is an extension of Docker which allows you to run multiple containers synchronously and in communication with one another. Compose allows you to define and run a multi-container service using a Dockerfile and a docker-compose.yml . Note Docker for Mac and Docker Toolbox already include Compose along with other Docker apps, so Mac users do not need to install Compose separately. Docker for Windows and Docker Toolbox already include Compose along with other Docker apps, so most Windows users do not need to install Compose separately. For Linux users sudo curl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose- ` uname -s ` - ` uname -m ` -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose Main advantages of Docker compose include: Your applications can be defined in a YAML file where all the same options required in docker run are now defined (reproducibility). It allows you to manage your application(s) as a single entity rather than dealing with starting individual containers (simplicity). Note For the sake of this example, either create your own Dockerfile or use the same Jupyter SciPy Notebook as in the Advanced Section","title":"Running multi-container Applications with Docker Compose"},{"location":"docker/compose/#creating-a-docker-composeyml","text":"Let's now create a Docker Compose .yml that calls Jupyter Notebook and RStudio 1. Create a folder shared_data in your current directory: mkdir shared_data 2. Create an empty docker-compose.yml file (e.g., touch docker-compose.yml) and paste the following lines version: \"3\" # All available services services: # Computation jupyter: container_name: \"jupyter_notebook\" image: \"jupyter/minimal-notebook\" restart: \"always\" environment: - JUPYTER_TOKEN=mytoken user: root volumes: - ./data:/home/jovyan/work/ ports: - 8888:8888 rstudio: container_name: \"rstudio\" image: \"rocker/rstudio\" restart: \"always\" environment: - DISABLE_AUTH=true volumes: - ./data:/home/rstudio ports: - 8787:8787 3. Run both Jupyter Lab and RStudio using docker-compose up instead of docker run . Note Handling containers with Docker Compose is fairly simple docker-compose up attaches the volumes, opens ports, and starts the container docker-compose down destroys the container A brief explanation of docker-compose.yml is as below: The web service builds from the Dockerfile in the current directory. Forwards the container's exposed port to port 8888 on the host. Mounts the project directory on the host to /work or /rstudio inside the container (allowing you to modify code without having to rebuild the image). restart: always means that it will restart whenever it fails.","title":"Creating a docker-compose.yml"},{"location":"docker/compose/#running-shutting-down-restarting-docker-compose","text":"Run the containers with $ docker-compose up -d To stop running a running docker-compose session, either press CTRL + C or use the command: docker-compose down The above command removes containers, networks, volumes and images created by docker-container up . To restart a container, use the command docker-compose restart restart will restart the docker-compose service without taking into account changes one may have made to the yml or environment.","title":"Running, shutting down, restarting docker-compose"},{"location":"docker/compose/#example-using-docker-compose-webodm","text":"Warning For the purpose of these following examples it is not suggested to use GitHub Codespaces. Web Open Drone Map (WebODM) OpenDroneMap is an open source photogrammetry toolkit to process aerial imagery into maps and 3D models running on command line. WebODM (Web OpenDroneMap) is an extension of ODM running on multiple Docker Containers provinding a user friendly web interface for easy visualization. To use WebODM: Prerequisites WebODM requires docker , docker-compose to function. Additionally, if you are on Windows, users will be required to have the Docker Windows Application installed as well as having the WSL2 (Windows Subsystem for Linux) operational. Ensure your machine is up to date: sudo apt-get update Clone the WebODM repository: git clone https://github.com/OpenDroneMap/WebODM --config core.autocrlf=input --depth 1 Move into the WebODM folder: cd WebODM Run WebODM: sudo ./webodm.sh start The necessary docker images will be downloaded (~2 minutes) and WebODM will be accessible through http://localhost:8000/ Note You will be asked to create an account as a formality. Add any username and a password and select Create Account . 6. Download example data: clone https://github.com/OpenDroneMap/odm_data_aukerman.git . This git repository contains 77 HD images usable for WebODM. For other examples refer to ODMData . 7. In the WebODM portal, click on Select Images and GCP , navigate to odm_data_aukerman/images and select between 20-50 images (16 is the absolute minimum, whilst 32 is the suggested minimum). 8. WebODM will process the uploaded images (~5-10 minutes); upon completion, click View Map . 9. A map will open; you can click on 3D (bottom right) to see the 3D rendered model generated.","title":"Example using Docker-Compose: WebODM"},{"location":"docker/intro/","text":"Introduction to Docker \u00b6 Prerequisites \u00b6 In order to complete these exercises we STRONGLY recommend that you set up a personal GitHub and DockerHub account (account creation for both services is free). There are no specific skills needed for this tutorial beyond elementary command line ability and using a text editor. We are going to be using GitHub CodeSpaces for the hands on portion of the workshop, which features VS Code as a fully enabled development environment with Docker already installed. CodeSpaces is a featured product from GitHub and requires a paid subscription or Academic account for access. Your account will temporarily be integrated with the course GitHub Organization for the next steps in the workshop. Our instructions on starting a new CodeSpace are here . Installing Docker on your personal computer We are going to be using virtual machines on the cloud for this course, and we will explain why this is a good thing, but there may be a time when you want to run Docker on your own computer. Installing Docker takes a little time but it is reasonably straight forward and it is a one-time setup. Installation instructions from Docker Official Docs for common OS and chip architectures: Mac OS X Windows Ubuntu Linux Never used a terminal before? That is OK! (This person never used a terminal until after their terminal degree, and now they actually PREFER to work in it for writing code) Don't be afraid or ashamed, but be ready to learn some new skills -- we promise it will be worth your while and even FUN! Before venturing much further, you should review the Software Carpentry lessons on \"The Unix Shell\" and \"Version Control with Git\" -- these are great introductory lessons related to the skills we're teaching here. You've given up on ever using a terminal? No problem, Docker can be used from graphic interfaces, like Docker Desktop , or platforms like Portainer . We suggest you read through their documentation on how to use Docker. Fundamental Docker Commands \u00b6 Docker commands in the terminal use the prefix docker . Note For every command listed, the correct execution of the commands through the command line is by using docker in front of the command: for example docker help or docker search . Thus, every = docker . help \u00b6 Like many other command line applications the most helpful flag is the help command which can be used with the Management Commands: $ docker $ docker --help search \u00b6 We talk about the concept of Docker Registries in the next section, but you can search the public list of registeries by using the docker search command to find public containers on the Official Docker Hub Registry : $ docker search pull \u00b6 Go to the Docker Hub and type hello-world in the search bar at the top of the page. Click on the 'tag' tab to see all the available 'hello-world' images. Click the 'copy' icon at the right to copy the docker pull command, or type it into your terminal: $ docker pull hello-world Note If you leave off the : and the tag name, it will by default pull the latest image $ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world 2db29710123e: Pull complete Digest: sha256:bfea6278a0a267fad2634554f4f0c6f31981eea41c553fdf5a83e95a41d40c38 Status: Downloaded newer image for hello-world:latest docker.io/library/hello-world:latest Now try to list the files in your current working directory: $ ls -l Where is the image you just pulled? Docker saves container images to the Docker directory (where Docker is installed). You won't ever see them in your working directory. Use 'docker images' to see all the images on your computer: $ docker images adding yourself to the Docker group on Linux Depending on how and where you've installed Docker, you may see a permission denied error after running $ docker run helo-world command. If you're on Linux, you may need to prefix your Docker commands with sudo . Alternatively to run docker command without sudo , you need to add your user name (who has root privileges) to the docker \"group\". Create the docker group: $ sudo groupadd docker Add your user to the docker group:: $ sudo usermod -aG docker $USER Log out or close terminal and log back in and your group membership will be initiated run \u00b6 The single most common command that you'll use with Docker is docker run ( see official help manual for more details). docker run starts a container and executes the default \"entrypoint\", or any other \"command\" that follows run and any optional flags. What is an entrypoint ? An entrypoint is the initial command(s) executed upon starting the Docker container. It is listed in the Dockerfile as ENTRYPOINT and can take 2 forms: as commands followed by parameters ( ENTRYPOINT command param1 param2 ) or as an executable ( ENTRYPOINT [\u201cexecutable\u201d, \u201cparam1\u201d, \u201cparam2\u201d] ) $ docker run hello-world:latest In the demo above, you used the docker pull command to download the hello-world:latest image. What about if you run a container that you haven't downloaded? $ docker run alpine:latest ls -l When you executed the command docker run alpine:latest , Docker first looked for the cached image locally, but did not find it, it then ran a docker pull behind the scenes to download the alpine:latest image and then execute your command. When you ran docker run alpine:latest , you provided a command ls -l , so Docker started the command specified and you saw the listing of the Alpine file system (not your host system, this was insice the container!). images \u00b6 You can now use the docker images command to see a list of all the cached images on your system: $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE alpine latest c51f86c28340 4 weeks ago 1.109 MB hello-world latest 690ed74de00f 5 months ago 960 B Inspecting your containers To find out more about a Docker images, run docker inspect hello-world:latest ps \u00b6 Now it's time to see the docker ps command which shows you all containers that are currently running on your machine. docker ps Since no containers are running, you see a blank line. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Let's try a more useful variant: docker ps --all $ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a5eab9243a15 hello-world \"/hello\" 5 seconds ago Exited (0) 3 seconds ago loving_mcnulty 3bb4e26d2e0c alpine:latest \"/bin/sh\" 17 seconds ago Exited (0) 16 seconds ago objective_meninsky 192ffdf0cbae opensearchproject/opensearch-dashboards:latest \"./opensearch-dashbo\u2026\" 3 days ago Exited (0) 3 days ago opensearch-dashboards a10d47d3b6de opensearchproject/opensearch:latest \"./opensearch-docker\u2026\" 3 days ago Exited (0) 3 days ago opensearch-node1 What you see above is a list of all containers that you have run. Notice that the STATUS column shows the current condition of the container: running, or as shown in the example, when the container was exited. stop \u00b6 The stop command is used for containers that are actively running, either as a foreground process or as a detached background one. You can find a running container using the docker ps command. rm \u00b6 You can remove individual stopped containers by using the rm command. Use the ps command to see all your stopped contiainers: @user \u279c /workspaces $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 03542eaac9dc hello-world \"/hello\" About a minute ago Exited (0) About a minute ago unruffled_nobel Use the first few unique alphanumerics in the CONTAINER ID to remove the stopped container: @user \u279c /workspaces (mkdocs \u2717) $ docker rm 0354 0354 Check to see that the container is gone using ps -a a second time ( -a is shorthand for --all ; the full command is docker ps -a or docker ps --all ). rmi \u00b6 The rmi command is similar to rm but it will remove the cached images. Used in combination with docker images or docker system df you can clean up a full cache docker rmi @user \u279c /workspaces/ (mkdocs \u2717) $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE opendronemap/webodm_webapp latest e075d13aaf35 21 hours ago 1.62GB redis latest a10f849e1540 5 days ago 117MB opendronemap/nodeodm latest b4c50165f838 6 days ago 1.77GB hello-world latest feb5d9fea6a5 7 months ago 13.3kB opendronemap/webodm_db latest e40c0f274bba 8 months ago 695MB @user \u279c /workspaces (mkdocs \u2717) $ docker rmi hello-world Untagged: hello-world:latest Untagged: hello-world@sha256:10d7d58d5ebd2a652f4d93fdd86da8f265f5318c6a73cc5b6a9798ff6d2b2e67 Deleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412 Deleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359 @user \u279c /workspaces (mkdocs \u2717) $ system \u00b6 The system command can be used to view information about containers on your cache, you can view your total disk usage, view events or info. You can also use it to prune unused data and image layers. To remove all cached layers, images, and data you can use the -af flag for all and force docker system prune -af tag \u00b6 By default an image will recieve the tag latest when it is not specified during the docker build Image names and tags can be created or changed using the docker tag command. docker tag imagename:oldtag imagename:newtag You can also change the registry name used in the tag: docker tag docker.io/username/imagename:oldtag harbor.cyverse.org/project/imagename:newtag The cached image laters will not change their sha256 and both image tags will still be present after the new tag name is generated. push \u00b6 By default docker push will upload your local container image to the Docker Hub We will cover push in more detail at the end of Day 2, but the essential functionality is the same as pull. Also, make sure that your container has the appropriate tag First, make sure to log into the Docker Hub, this will allow you to download private limages, to upload private/public images: docker login Alternately, you can link GitHub / GitLab accounts to the Docker Hub. To push the image to the Docker Hub: docker push username/imagename:tag or docker push docker.io/username/imagename:tag or, to a private registry, here we push to CyVerse private harbor.cyverse.org registry which uses \"project\" sub folders: docker push harbor.cyverse.org/project/imagename:newtag Commands & Entrypoints \u00b6 We will cover the differences in CMD and ENTRYPOINT on Day 2 when we build our own images, but it is important to understand that a container can have a command appended to the docker run function. When a image has no commands or entrypoints specified in its Dockerfile, it will default to running a /bin/sh syntax. In those cases, you can add a command when the congtainer is run: $ docker run alpine echo \"Hello world\" the Docker client dutifully ran the echo command in our alpine container and then exited. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast! Interactive Commands with Containers \u00b6 Lets try another command, this time to access the container as a shell: $ docker run alpine:latest sh Wait, nothing happened, right? Is that a bug? Well, no. The container will exit after running any scripted commands such as sh , unless they are run in an \"interactive\" terminal (TTY) - so for this example to not exit, you need to add the -i for interactive and -t for TTY. You can run them both in a single flag as -it , which is the more common way of adding the flag: $ docker run -it alpine:latest sh The prompt should change to something more like / # . You are now running a shell inside the container! Try out a few commands like ls -l , uname -a and others. Exit out of the container by giving the exit command. / # exit Making sure you've exited the container If you type exit your container will exit and is no longer active. To check that, try the following: $ docker ps --latest CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES de4bbc3eeaec alpine \"/bin/sh\" 3 minutes ago Exited (0) About a minute ago pensive_leavitt If you want to keep the container active, then you can use keys ctrl +p ctrl +q . To make sure that it is not exited run the same docker ps --latest command again: $ docker ps --latest CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0db38ea51a48 alpine \"sh\" 3 minutes ago Up 3 minutes elastic_lewin Now if you want to get back into that container, then you can type docker attach <container id> . This way you can save your container: $ docker attach 0db38ea51a48 House Keeping and Cleaning Up Exited Containers \u00b6 Managing Docker Images \u00b6 In the previous example, you pulled the alpine image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the docker images command. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu bionic 47b19964fb50 4 weeks ago 88.1MB alpine latest caf27325b298 4 weeks ago 5.53MB hello-world latest fce289e99eb9 2 months ago 1.84kB Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image. For simplicity, you can think of an image akin to a Git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to latest. Clutter and Cache \u00b6 Docker images are cached on your machine in the location where Docker was installed. These image files are not visible in the same directory where you might have used docker pull <imagename> . Some Docker images can be large. Especially data science images with many scientific programming libraries and packages pre-installed. Checking your system cache Pulling many images from the Docker Registries may fill up your hard disk! To inspect your system and disk use: $ docker system info $ docker system df To find out how many images are on your machine, type: $ docker images To remove images that you no longer need, type: $ docker system prune This is where it becomes important to differentiate between images , containers , and volumes (which we'll get to more in a bit). You can take care of all of the dangling images and containers on your system. Note, that prune will not remove your cached images $ docker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache Are you sure you want to continue? [y/N] If you added the -af flag it will remove \"all\" -a dangling images, empty containers, AND ALL CACHED IMAGES with \"force\" -f . Managing Data in Docker \u00b6 It is possible to store data within the writable layer of a container, but there are some limitations: The data doesn\u2019t persist when that container is no longer running, and it can be difficult to get the data out of the container if another process needs it. A container\u2019s writable layer is tightly coupled to the host machine where the container is running. You can\u2019t easily move the data somewhere else. Its better to put your data into the container AFTER it is built - this keeps the container size smaller and easier to move across networks. Docker offers three different ways to mount data into a container from the Docker host: Volumes tmpfs mounts Bind mounts When in doubt, volumes are almost always the right choice. Volumes \u00b6 Volumes are often a better choice than persisting data in a container\u2019s writable layer, because using a volume does not increase the size of containers using it, and the volume\u2019s contents exist outside the lifecycle of a given container. While bind mounts (which we will see in the Advanced portion of the Camp) are dependent on the directory structure of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts: Volumes are easier to back up or migrate than bind mounts. You can manage volumes using Docker CLI commands or the Docker API. Volumes work on both UNIX and Windows containers. Volumes can be more safely shared among multiple containers. A new volume\u2019s contents can be pre-populated by a container. When Should I Use the Temporary File System mount? If your container generates non-persistent state data, consider using a tmpfs mount to avoid storing the data anywhere permanently, and to increase the container\u2019s performance by avoiding writing into the container\u2019s writable layer. The data is written to the host's memory instead of a volume; When the container stops, the tmpfs mount is removed, and files written there will not be kept. Choose the -v flag for mounting volumes -v or --volume : Consists of three fields, separated by colon characters (:). The fields must be in the correct order, and the meaning of each field is not immediately obvious. The first field is the path on your local machine that where the data are. The second field is the path where the file or directory are mounted in the container . The third field is optional, and is a comma-separated list of options, such as ro (read only). -v /home/username/your_data_folder:/container_folder $ docker run -v /home/$USER/read_cleanup:/work alpine:latest ls -l /work So what if we wanted to work interactively inside the container? $ docker run -it -v /home/$USER/read_cleanup:/work alpine:latest sh $ ls -l $ ls -l work Once you're in the container, you will see that the /work directory is mounted in the working directory. Any data that you add to that folder outside the container will appear INSIDE the container. And any work you do inside the container saved in that folder will be saved OUTSIDE the container as well. Working with Interactive Containers \u00b6 Let's go ahead and run some Integrated Development Environment images from \"trusted\" organizations on the Docker Hub Registry. Jupyter Lab or RStudio-Server IDE \u00b6 In this section, let's find a Docker image which can run a Jupyter Notebook Search for official images on Docker Hub which contain the string 'jupyter' $ docker search jupyter It should return something like: NAME DESCRIPTION STARS OFFICIAL AUTOMATED jupyter/datascience-notebook Jupyter Notebook Data Science Stack from htt\u2026 912 jupyter/all-spark-notebook Jupyter Notebook Python, Scala, R, Spark, Me\u2026 374 jupyter/scipy-notebook Jupyter Notebook Scientific Python Stack fro\u2026 337 jupyterhub/jupyterhub JupyterHub: multi-user Jupyter notebook serv\u2026 307 [OK] jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ \u2026 298 jupyter/pyspark-notebook Jupyter Notebook Python, Spark, Mesos Stack \u2026 224 jupyter/base-notebook Small base image for Jupyter Notebook stacks\u2026 168 jupyter/minimal-notebook Minimal Jupyter Notebook Stack from https://\u2026 150 jupyter/r-notebook Jupyter Notebook R Stack from https://github\u2026 44 jupyterhub/singleuser single-user docker images for use with Jupyt\u2026 43 [OK] jupyter/nbviewer Jupyter Notebook Viewer 27 [OK] Search for images on Docker Hub which contain the string 'rstudio' $ docker search rstudio NAME DESCRIPTION STARS OFFICIAL AUTOMATED rocker/rstudio RStudio Server image 389 [OK] rstudio/r-base Docker Images for R 24 rocker/rstudio-stable Build RStudio based on a debian:stable (debi\u2026 16 [OK] rstudio/rstudio-server-pro Deprecated Docker images for RStudio Server \u2026 10 rstudio/r-session-complete Images for sessions and jobs in RStudio Serv\u2026 10 rstudio/plumber 6 rstudio/rstudio-connect Default Docker image for RStudio Connect 4 rstudio/r-builder-images-win 3 rstudio/rstudio-workbench Docker Image for RStudio Workbench (formerly\u2026 2 saagie/rstudio RStudio with sparklyr, Saagie's addin and ab\u2026 2 [OK] ibmcom/rstudio-ppc64le Integrated development environment (IDE) for\u2026 2 rstudio/checkrs-tew Test Environment: Web 1 [OK] rstudio/rstudio-package-manager Default Docker image for RStudio Package Man\u2026 1 rstudio/shinyapps-package-dependencies Docker images used to test the install scrip\u2026 1 rstudio/rstudio-workbench-preview 1 Untrusted community images An important thing to note: None of these Jupyter or RStudio images are 'official' Docker images, meaning they could be trojans for spyware, malware, or other nasty warez. Understanding PORTS \u00b6 When we want to run a container that runs on the open internet, we need to add a TCP or UDP port number from which we can access the application in a browser using the machine's IP (Internet Protocol) address or DNS (Domain Name Service) location. To do this, we need to access the container over a separate port address on the machine we're working on. Docker uses the flag --port or -p for short followed by two sets of port numbers. Exposing Ports Docker can in fact expose all ports to a container using the capital -P flag For security purposes, it is generally NEVER a good idea to expose all ports. Typically these numbers can be the same, but in some cases your machine may already be running another program (or container) on that open port. The port has two sides left:right separated by a colon. The left side port number is the INTERNAL port that the container software thinks its using. The right side number is the EXTERNAL port that you can access on your computer (or virtual machine). Here are some examples to run basic RStudio and Jupyter Lab: $ docker run --rm -p 8787:8787 -e PASSWORD=cc2022 rocker/rstudio note: on CodeSpaces, the reverse proxy for the DNS requires you to turn off authentication $ docker run --rm -p 8787:8787 -e DISABLE_AUTH=true rocker/rstudio $ docker run --rm -p 8888:8888 jupyter/base-notebook Preempting stale containers from your cache We've added the --rm flag, which means the container will automatically removed from the cache when the container is exited. When you start an IDE in a terminal, the terminal connection must stay active to keep the container alive. Detaching your container while it is running \u00b6 If we want to keep our window in the foreground we can use the -d - the detached flag will run the container as a background process, rather than in the foreground. When you run a container with this flag, it will start, run, telling you the container ID: $ docker run --rm -d -p 8888:8888 jupyter/base-notebook Note, that your terminal is still active and you can use it to launch more containers. To view the running container, use the docker ps command. Docker Commands \u00b6 Here is a compiled list of fundamental Docker Commands: Command Usage Example pull Downloads an image from Docker Hub docker pull hello-world:latest run runs a container with entrypoint docker run -it user/image:tag build Builds a docker image from a Dockerfile in current working directory docker build -t user/image:tag . images List all images on the local machine docker images list tag Adds a different tag name to an image docker tag hello-world:latest hello-world:new-tag-name login Authenticate to the Docker Hub (requires username and password) docker login push Upload your new image to the Docker Hub docker push user/image:tag inspect Provide detailed information on constructs controlled by Docker docker inspect containerID ps List all containers on your system docker ps -a rm Delete a stopped or running container docker rm -f <container ID> rmi Delete an image from your cache docker rmi hello-world:latest stop Stop a running container docker stop alpine:latest system View system details, remove old images and containers with prune docker system prune push Uploads an image to the Docker Hub (or other private registry) docker push username/image:tag","title":"Introduction to Docker"},{"location":"docker/intro/#introduction-to-docker","text":"","title":"Introduction to Docker "},{"location":"docker/intro/#prerequisites","text":"In order to complete these exercises we STRONGLY recommend that you set up a personal GitHub and DockerHub account (account creation for both services is free). There are no specific skills needed for this tutorial beyond elementary command line ability and using a text editor. We are going to be using GitHub CodeSpaces for the hands on portion of the workshop, which features VS Code as a fully enabled development environment with Docker already installed. CodeSpaces is a featured product from GitHub and requires a paid subscription or Academic account for access. Your account will temporarily be integrated with the course GitHub Organization for the next steps in the workshop. Our instructions on starting a new CodeSpace are here . Installing Docker on your personal computer We are going to be using virtual machines on the cloud for this course, and we will explain why this is a good thing, but there may be a time when you want to run Docker on your own computer. Installing Docker takes a little time but it is reasonably straight forward and it is a one-time setup. Installation instructions from Docker Official Docs for common OS and chip architectures: Mac OS X Windows Ubuntu Linux Never used a terminal before? That is OK! (This person never used a terminal until after their terminal degree, and now they actually PREFER to work in it for writing code) Don't be afraid or ashamed, but be ready to learn some new skills -- we promise it will be worth your while and even FUN! Before venturing much further, you should review the Software Carpentry lessons on \"The Unix Shell\" and \"Version Control with Git\" -- these are great introductory lessons related to the skills we're teaching here. You've given up on ever using a terminal? No problem, Docker can be used from graphic interfaces, like Docker Desktop , or platforms like Portainer . We suggest you read through their documentation on how to use Docker.","title":"Prerequisites"},{"location":"docker/intro/#fundamental-docker-commands","text":"Docker commands in the terminal use the prefix docker . Note For every command listed, the correct execution of the commands through the command line is by using docker in front of the command: for example docker help or docker search . Thus, every = docker .","title":"Fundamental Docker Commands "},{"location":"docker/intro/#help","text":"Like many other command line applications the most helpful flag is the help command which can be used with the Management Commands: $ docker $ docker --help","title":" help"},{"location":"docker/intro/#search","text":"We talk about the concept of Docker Registries in the next section, but you can search the public list of registeries by using the docker search command to find public containers on the Official Docker Hub Registry : $ docker search","title":" search"},{"location":"docker/intro/#pull","text":"Go to the Docker Hub and type hello-world in the search bar at the top of the page. Click on the 'tag' tab to see all the available 'hello-world' images. Click the 'copy' icon at the right to copy the docker pull command, or type it into your terminal: $ docker pull hello-world Note If you leave off the : and the tag name, it will by default pull the latest image $ docker pull hello-world Using default tag: latest latest: Pulling from library/hello-world 2db29710123e: Pull complete Digest: sha256:bfea6278a0a267fad2634554f4f0c6f31981eea41c553fdf5a83e95a41d40c38 Status: Downloaded newer image for hello-world:latest docker.io/library/hello-world:latest Now try to list the files in your current working directory: $ ls -l Where is the image you just pulled? Docker saves container images to the Docker directory (where Docker is installed). You won't ever see them in your working directory. Use 'docker images' to see all the images on your computer: $ docker images adding yourself to the Docker group on Linux Depending on how and where you've installed Docker, you may see a permission denied error after running $ docker run helo-world command. If you're on Linux, you may need to prefix your Docker commands with sudo . Alternatively to run docker command without sudo , you need to add your user name (who has root privileges) to the docker \"group\". Create the docker group: $ sudo groupadd docker Add your user to the docker group:: $ sudo usermod -aG docker $USER Log out or close terminal and log back in and your group membership will be initiated","title":" pull"},{"location":"docker/intro/#run","text":"The single most common command that you'll use with Docker is docker run ( see official help manual for more details). docker run starts a container and executes the default \"entrypoint\", or any other \"command\" that follows run and any optional flags. What is an entrypoint ? An entrypoint is the initial command(s) executed upon starting the Docker container. It is listed in the Dockerfile as ENTRYPOINT and can take 2 forms: as commands followed by parameters ( ENTRYPOINT command param1 param2 ) or as an executable ( ENTRYPOINT [\u201cexecutable\u201d, \u201cparam1\u201d, \u201cparam2\u201d] ) $ docker run hello-world:latest In the demo above, you used the docker pull command to download the hello-world:latest image. What about if you run a container that you haven't downloaded? $ docker run alpine:latest ls -l When you executed the command docker run alpine:latest , Docker first looked for the cached image locally, but did not find it, it then ran a docker pull behind the scenes to download the alpine:latest image and then execute your command. When you ran docker run alpine:latest , you provided a command ls -l , so Docker started the command specified and you saw the listing of the Alpine file system (not your host system, this was insice the container!).","title":" run"},{"location":"docker/intro/#images","text":"You can now use the docker images command to see a list of all the cached images on your system: $ docker images REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE alpine latest c51f86c28340 4 weeks ago 1.109 MB hello-world latest 690ed74de00f 5 months ago 960 B Inspecting your containers To find out more about a Docker images, run docker inspect hello-world:latest","title":" images"},{"location":"docker/intro/#ps","text":"Now it's time to see the docker ps command which shows you all containers that are currently running on your machine. docker ps Since no containers are running, you see a blank line. $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Let's try a more useful variant: docker ps --all $ docker ps --all CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a5eab9243a15 hello-world \"/hello\" 5 seconds ago Exited (0) 3 seconds ago loving_mcnulty 3bb4e26d2e0c alpine:latest \"/bin/sh\" 17 seconds ago Exited (0) 16 seconds ago objective_meninsky 192ffdf0cbae opensearchproject/opensearch-dashboards:latest \"./opensearch-dashbo\u2026\" 3 days ago Exited (0) 3 days ago opensearch-dashboards a10d47d3b6de opensearchproject/opensearch:latest \"./opensearch-docker\u2026\" 3 days ago Exited (0) 3 days ago opensearch-node1 What you see above is a list of all containers that you have run. Notice that the STATUS column shows the current condition of the container: running, or as shown in the example, when the container was exited.","title":" ps"},{"location":"docker/intro/#stop","text":"The stop command is used for containers that are actively running, either as a foreground process or as a detached background one. You can find a running container using the docker ps command.","title":" stop"},{"location":"docker/intro/#rm","text":"You can remove individual stopped containers by using the rm command. Use the ps command to see all your stopped contiainers: @user \u279c /workspaces $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 03542eaac9dc hello-world \"/hello\" About a minute ago Exited (0) About a minute ago unruffled_nobel Use the first few unique alphanumerics in the CONTAINER ID to remove the stopped container: @user \u279c /workspaces (mkdocs \u2717) $ docker rm 0354 0354 Check to see that the container is gone using ps -a a second time ( -a is shorthand for --all ; the full command is docker ps -a or docker ps --all ).","title":" rm"},{"location":"docker/intro/#rmi","text":"The rmi command is similar to rm but it will remove the cached images. Used in combination with docker images or docker system df you can clean up a full cache docker rmi @user \u279c /workspaces/ (mkdocs \u2717) $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE opendronemap/webodm_webapp latest e075d13aaf35 21 hours ago 1.62GB redis latest a10f849e1540 5 days ago 117MB opendronemap/nodeodm latest b4c50165f838 6 days ago 1.77GB hello-world latest feb5d9fea6a5 7 months ago 13.3kB opendronemap/webodm_db latest e40c0f274bba 8 months ago 695MB @user \u279c /workspaces (mkdocs \u2717) $ docker rmi hello-world Untagged: hello-world:latest Untagged: hello-world@sha256:10d7d58d5ebd2a652f4d93fdd86da8f265f5318c6a73cc5b6a9798ff6d2b2e67 Deleted: sha256:feb5d9fea6a5e9606aa995e879d862b825965ba48de054caab5ef356dc6b3412 Deleted: sha256:e07ee1baac5fae6a26f30cabfe54a36d3402f96afda318fe0a96cec4ca393359 @user \u279c /workspaces (mkdocs \u2717) $","title":" rmi"},{"location":"docker/intro/#system","text":"The system command can be used to view information about containers on your cache, you can view your total disk usage, view events or info. You can also use it to prune unused data and image layers. To remove all cached layers, images, and data you can use the -af flag for all and force docker system prune -af","title":" system"},{"location":"docker/intro/#tag","text":"By default an image will recieve the tag latest when it is not specified during the docker build Image names and tags can be created or changed using the docker tag command. docker tag imagename:oldtag imagename:newtag You can also change the registry name used in the tag: docker tag docker.io/username/imagename:oldtag harbor.cyverse.org/project/imagename:newtag The cached image laters will not change their sha256 and both image tags will still be present after the new tag name is generated.","title":" tag"},{"location":"docker/intro/#push","text":"By default docker push will upload your local container image to the Docker Hub We will cover push in more detail at the end of Day 2, but the essential functionality is the same as pull. Also, make sure that your container has the appropriate tag First, make sure to log into the Docker Hub, this will allow you to download private limages, to upload private/public images: docker login Alternately, you can link GitHub / GitLab accounts to the Docker Hub. To push the image to the Docker Hub: docker push username/imagename:tag or docker push docker.io/username/imagename:tag or, to a private registry, here we push to CyVerse private harbor.cyverse.org registry which uses \"project\" sub folders: docker push harbor.cyverse.org/project/imagename:newtag","title":" push"},{"location":"docker/intro/#commands-entrypoints","text":"We will cover the differences in CMD and ENTRYPOINT on Day 2 when we build our own images, but it is important to understand that a container can have a command appended to the docker run function. When a image has no commands or entrypoints specified in its Dockerfile, it will default to running a /bin/sh syntax. In those cases, you can add a command when the congtainer is run: $ docker run alpine echo \"Hello world\" the Docker client dutifully ran the echo command in our alpine container and then exited. If you've noticed, all of that happened pretty quickly. Imagine booting up a virtual machine, running a command and then killing it. Now you know why they say containers are fast!","title":"Commands &amp; Entrypoints"},{"location":"docker/intro/#interactive-commands-with-containers","text":"Lets try another command, this time to access the container as a shell: $ docker run alpine:latest sh Wait, nothing happened, right? Is that a bug? Well, no. The container will exit after running any scripted commands such as sh , unless they are run in an \"interactive\" terminal (TTY) - so for this example to not exit, you need to add the -i for interactive and -t for TTY. You can run them both in a single flag as -it , which is the more common way of adding the flag: $ docker run -it alpine:latest sh The prompt should change to something more like / # . You are now running a shell inside the container! Try out a few commands like ls -l , uname -a and others. Exit out of the container by giving the exit command. / # exit Making sure you've exited the container If you type exit your container will exit and is no longer active. To check that, try the following: $ docker ps --latest CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES de4bbc3eeaec alpine \"/bin/sh\" 3 minutes ago Exited (0) About a minute ago pensive_leavitt If you want to keep the container active, then you can use keys ctrl +p ctrl +q . To make sure that it is not exited run the same docker ps --latest command again: $ docker ps --latest CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0db38ea51a48 alpine \"sh\" 3 minutes ago Up 3 minutes elastic_lewin Now if you want to get back into that container, then you can type docker attach <container id> . This way you can save your container: $ docker attach 0db38ea51a48","title":"Interactive Commands with Containers"},{"location":"docker/intro/#house-keeping-and-cleaning-up-exited-containers","text":"","title":" House Keeping and  Cleaning Up Exited Containers"},{"location":"docker/intro/#managing-docker-images","text":"In the previous example, you pulled the alpine image from the registry and asked the Docker client to run a container based on that image. To see the list of images that are available locally on your system, run the docker images command. $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu bionic 47b19964fb50 4 weeks ago 88.1MB alpine latest caf27325b298 4 weeks ago 5.53MB hello-world latest fce289e99eb9 2 months ago 1.84kB Above is a list of images that I've pulled from the registry and those I've created myself (we'll shortly see how). You will have a different list of images on your machine. The TAG refers to a particular snapshot of the image and the ID is the corresponding unique identifier for that image. For simplicity, you can think of an image akin to a Git repository - images can be committed with changes and have multiple versions. When you do not provide a specific version number, the client defaults to latest.","title":"Managing Docker Images"},{"location":"docker/intro/#clutter-and-cache","text":"Docker images are cached on your machine in the location where Docker was installed. These image files are not visible in the same directory where you might have used docker pull <imagename> . Some Docker images can be large. Especially data science images with many scientific programming libraries and packages pre-installed. Checking your system cache Pulling many images from the Docker Registries may fill up your hard disk! To inspect your system and disk use: $ docker system info $ docker system df To find out how many images are on your machine, type: $ docker images To remove images that you no longer need, type: $ docker system prune This is where it becomes important to differentiate between images , containers , and volumes (which we'll get to more in a bit). You can take care of all of the dangling images and containers on your system. Note, that prune will not remove your cached images $ docker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache Are you sure you want to continue? [y/N] If you added the -af flag it will remove \"all\" -a dangling images, empty containers, AND ALL CACHED IMAGES with \"force\" -f .","title":"Clutter and Cache"},{"location":"docker/intro/#managing-data-in-docker","text":"It is possible to store data within the writable layer of a container, but there are some limitations: The data doesn\u2019t persist when that container is no longer running, and it can be difficult to get the data out of the container if another process needs it. A container\u2019s writable layer is tightly coupled to the host machine where the container is running. You can\u2019t easily move the data somewhere else. Its better to put your data into the container AFTER it is built - this keeps the container size smaller and easier to move across networks. Docker offers three different ways to mount data into a container from the Docker host: Volumes tmpfs mounts Bind mounts When in doubt, volumes are almost always the right choice.","title":"Managing Data in Docker"},{"location":"docker/intro/#volumes","text":"Volumes are often a better choice than persisting data in a container\u2019s writable layer, because using a volume does not increase the size of containers using it, and the volume\u2019s contents exist outside the lifecycle of a given container. While bind mounts (which we will see in the Advanced portion of the Camp) are dependent on the directory structure of the host machine, volumes are completely managed by Docker. Volumes have several advantages over bind mounts: Volumes are easier to back up or migrate than bind mounts. You can manage volumes using Docker CLI commands or the Docker API. Volumes work on both UNIX and Windows containers. Volumes can be more safely shared among multiple containers. A new volume\u2019s contents can be pre-populated by a container. When Should I Use the Temporary File System mount? If your container generates non-persistent state data, consider using a tmpfs mount to avoid storing the data anywhere permanently, and to increase the container\u2019s performance by avoiding writing into the container\u2019s writable layer. The data is written to the host's memory instead of a volume; When the container stops, the tmpfs mount is removed, and files written there will not be kept. Choose the -v flag for mounting volumes -v or --volume : Consists of three fields, separated by colon characters (:). The fields must be in the correct order, and the meaning of each field is not immediately obvious. The first field is the path on your local machine that where the data are. The second field is the path where the file or directory are mounted in the container . The third field is optional, and is a comma-separated list of options, such as ro (read only). -v /home/username/your_data_folder:/container_folder $ docker run -v /home/$USER/read_cleanup:/work alpine:latest ls -l /work So what if we wanted to work interactively inside the container? $ docker run -it -v /home/$USER/read_cleanup:/work alpine:latest sh $ ls -l $ ls -l work Once you're in the container, you will see that the /work directory is mounted in the working directory. Any data that you add to that folder outside the container will appear INSIDE the container. And any work you do inside the container saved in that folder will be saved OUTSIDE the container as well.","title":"Volumes"},{"location":"docker/intro/#working-with-interactive-containers","text":"Let's go ahead and run some Integrated Development Environment images from \"trusted\" organizations on the Docker Hub Registry.","title":"Working with Interactive Containers"},{"location":"docker/intro/#jupyter-lab-or-rstudio-server-ide","text":"In this section, let's find a Docker image which can run a Jupyter Notebook Search for official images on Docker Hub which contain the string 'jupyter' $ docker search jupyter It should return something like: NAME DESCRIPTION STARS OFFICIAL AUTOMATED jupyter/datascience-notebook Jupyter Notebook Data Science Stack from htt\u2026 912 jupyter/all-spark-notebook Jupyter Notebook Python, Scala, R, Spark, Me\u2026 374 jupyter/scipy-notebook Jupyter Notebook Scientific Python Stack fro\u2026 337 jupyterhub/jupyterhub JupyterHub: multi-user Jupyter notebook serv\u2026 307 [OK] jupyter/tensorflow-notebook Jupyter Notebook Scientific Python Stack w/ \u2026 298 jupyter/pyspark-notebook Jupyter Notebook Python, Spark, Mesos Stack \u2026 224 jupyter/base-notebook Small base image for Jupyter Notebook stacks\u2026 168 jupyter/minimal-notebook Minimal Jupyter Notebook Stack from https://\u2026 150 jupyter/r-notebook Jupyter Notebook R Stack from https://github\u2026 44 jupyterhub/singleuser single-user docker images for use with Jupyt\u2026 43 [OK] jupyter/nbviewer Jupyter Notebook Viewer 27 [OK] Search for images on Docker Hub which contain the string 'rstudio' $ docker search rstudio NAME DESCRIPTION STARS OFFICIAL AUTOMATED rocker/rstudio RStudio Server image 389 [OK] rstudio/r-base Docker Images for R 24 rocker/rstudio-stable Build RStudio based on a debian:stable (debi\u2026 16 [OK] rstudio/rstudio-server-pro Deprecated Docker images for RStudio Server \u2026 10 rstudio/r-session-complete Images for sessions and jobs in RStudio Serv\u2026 10 rstudio/plumber 6 rstudio/rstudio-connect Default Docker image for RStudio Connect 4 rstudio/r-builder-images-win 3 rstudio/rstudio-workbench Docker Image for RStudio Workbench (formerly\u2026 2 saagie/rstudio RStudio with sparklyr, Saagie's addin and ab\u2026 2 [OK] ibmcom/rstudio-ppc64le Integrated development environment (IDE) for\u2026 2 rstudio/checkrs-tew Test Environment: Web 1 [OK] rstudio/rstudio-package-manager Default Docker image for RStudio Package Man\u2026 1 rstudio/shinyapps-package-dependencies Docker images used to test the install scrip\u2026 1 rstudio/rstudio-workbench-preview 1 Untrusted community images An important thing to note: None of these Jupyter or RStudio images are 'official' Docker images, meaning they could be trojans for spyware, malware, or other nasty warez.","title":"Jupyter Lab  or RStudio-Server  IDE"},{"location":"docker/intro/#understanding-ports","text":"When we want to run a container that runs on the open internet, we need to add a TCP or UDP port number from which we can access the application in a browser using the machine's IP (Internet Protocol) address or DNS (Domain Name Service) location. To do this, we need to access the container over a separate port address on the machine we're working on. Docker uses the flag --port or -p for short followed by two sets of port numbers. Exposing Ports Docker can in fact expose all ports to a container using the capital -P flag For security purposes, it is generally NEVER a good idea to expose all ports. Typically these numbers can be the same, but in some cases your machine may already be running another program (or container) on that open port. The port has two sides left:right separated by a colon. The left side port number is the INTERNAL port that the container software thinks its using. The right side number is the EXTERNAL port that you can access on your computer (or virtual machine). Here are some examples to run basic RStudio and Jupyter Lab: $ docker run --rm -p 8787:8787 -e PASSWORD=cc2022 rocker/rstudio note: on CodeSpaces, the reverse proxy for the DNS requires you to turn off authentication $ docker run --rm -p 8787:8787 -e DISABLE_AUTH=true rocker/rstudio $ docker run --rm -p 8888:8888 jupyter/base-notebook Preempting stale containers from your cache We've added the --rm flag, which means the container will automatically removed from the cache when the container is exited. When you start an IDE in a terminal, the terminal connection must stay active to keep the container alive.","title":"Understanding PORTS"},{"location":"docker/intro/#detaching-your-container-while-it-is-running","text":"If we want to keep our window in the foreground we can use the -d - the detached flag will run the container as a background process, rather than in the foreground. When you run a container with this flag, it will start, run, telling you the container ID: $ docker run --rm -d -p 8888:8888 jupyter/base-notebook Note, that your terminal is still active and you can use it to launch more containers. To view the running container, use the docker ps command.","title":"Detaching your container while it is running"},{"location":"docker/intro/#docker-commands","text":"Here is a compiled list of fundamental Docker Commands: Command Usage Example pull Downloads an image from Docker Hub docker pull hello-world:latest run runs a container with entrypoint docker run -it user/image:tag build Builds a docker image from a Dockerfile in current working directory docker build -t user/image:tag . images List all images on the local machine docker images list tag Adds a different tag name to an image docker tag hello-world:latest hello-world:new-tag-name login Authenticate to the Docker Hub (requires username and password) docker login push Upload your new image to the Docker Hub docker push user/image:tag inspect Provide detailed information on constructs controlled by Docker docker inspect containerID ps List all containers on your system docker ps -a rm Delete a stopped or running container docker rm -f <container ID> rmi Delete an image from your cache docker rmi hello-world:latest stop Stop a running container docker stop alpine:latest system View system details, remove old images and containers with prune docker system prune push Uploads an image to the Docker Hub (or other private registry) docker push username/image:tag","title":"Docker Commands"},{"location":"docker/registry/","text":"There is a high likelihood that the Docker image you need already exists for the application you use in your research. Rather than starting from scratch and creating your own image from a Dockerfile , you should first go searching for one that already exists. This will save you time writing and compiling your own image. First, you will need to know where to look for existing images. Docker images are hosted across the internet on libraries that are called Registries . Container Terminology Container : when an image is run it becomes the container ; A container shares its kernel with other containers using the same image and each runs as an isolated process on the host. Registry : an online library of container images Name : the name of the image Tag : identifies exact version of the image, following a : in the name. If no tag name is given, by default Docker will assign the latest tag name to an image. Layer : an intermediate image, the result of a single set of build commands. An image is built upon layers starting with a base operating system. Dockerfile : a text file that contains a list of commands that the Docker daemon calls while creating the layers of an image. Image : images are compressed files in a cache on a host and can be built locally or downloaded from a registry Base image : has no parent layers, usually base images are a basic Linux operating system like alpine , debian , or ubuntu . Child image : any image built from a base image that has added layers. Official images : verified ( ) images hosted on a public container registry. Safe to use, built by the professionals who know them best. Publisher image : certified images that also include support and guarantee compatibility. User image : images created and shared by users like you. Their contents may be a mystery and therefore are not to be trusted. Docker Registries \u00b6 Docker uses the concept of Registries , online libraries where container images are cached for public utilization. What EXACTLY is a Container Registry? A Registry is a storage and distribution system for named Docker images Organized by owners into repositories with compiled images that users can download and run Things you can do with registries: Search for public images; Pull official images; Host and share private images; Push your images. You must have an account on each registry in order to create repositories and to host your own images. You can create multiple repositories; You can create multiple tagged images in a repository; You can set up your own private registry using a Docker Trusted Registry . Search Image Registries \u00b6 Warning Only use images from trusted sources or images for which you can see the Dockerfile . Any image from an untrusted source could contain something other than what's indicated. If you can see the Dockerfile you can see exactly what is in the image. Docker looks into the Docker Hub public registry by default. Examples of public/private registries to consider for your research needs: Registry Name Container Types Docker Hub Official Images for Docker Amazon Elastic Container Registry run containers on AWS Google Container Registry run containers on Google Cloud Azure Container Registry run containers on Azure NVIDIA GPU Cloud containers for GPU computing GitHub Container Registry managed containers on GitHub GitLab Container Registry managed containers on GitLab RedHat Quay.io containers managed by RedHat BioContainers Registry bioinformatics containers DockerHub \u00b6 Docker Hub is a service provided by Docker for finding and sharing container images with your team. Docker Hub is the most well-known and popular image registry for Docker containers. Info Registry : a storage and distribution system for named Docker images Repository : collection of \"images\" with individual \"tags\". Teams & Organizations: Manages access to private repositories. Builds: Automatically build container images from GitHub or Bitbucket on the Docker Hub. Webhooks: Trigger actions after a successful push to a repository to integrate Docker Hub with other services. BioContainers \u00b6 BioContainers is a community-driven project that provides the infrastructure and basic guidelines to create, manage and distribute bioinformatics containers with special focus in proteomics, genomics, transcriptomics and metabolomics . BioContainers is based on the popular frameworks of Docker. Although anyone can create a BioContainer, the majority of BioContainers are created by the Bioconda project. Every Bioconda package has a corresponding BioContainer available at Quay.io. RedHat Quay.io \u00b6 Quay is another general image registry. It works the same way as Docker Hub. However, Quay is home to all BioContainers made by the Bioconda project. Now we will find a BioContainer image at Quay, pull that image and run it on cloud virtual machine. NVIDIA GPU Cloud \u00b6 NVIDIA is one of the leading makers of graphic processing units (GPU). GPU were established as a means of handling graphics processing operations for video cards, but have been greatly expanded for use in generalized computing applications, Machine Learning, image processing, and matrix-based linear algebras. NVIDIA have created their own set of Docker containers and Registries for running on CPU-GPU enabled systems. NVIDIA-Docker runs atop the NVIDIA graphics drivers on the host system, the NVIDIA drivers are imported to the container at runtime. NVIDIA Docker Hub hosts numerous NVIDIA Docker containers, from which you can build your own images. NVIDIA GPU Cloud hosts numerous containers for HPC and Cloud applications. You must register an account with them (free) to access these. NVIDIA GPU Cloud hosts three registry spaces nvcr.io/nvidia - catalog of fully integrated and optimized deep learning framework containers. nvcr.io/nvidia-hpcvis - catalog of HPC visualization containers (beta). nvcr.io/hpc - popular third-party GPU ready HPC application containers. NVIDIA Docker can be used as a base-image to create containers running graphical applications remotely. High resolution 3D screens are piped to a remote desktop platform.","title":"Finding the right container"},{"location":"docker/registry/#docker-registries","text":"Docker uses the concept of Registries , online libraries where container images are cached for public utilization. What EXACTLY is a Container Registry? A Registry is a storage and distribution system for named Docker images Organized by owners into repositories with compiled images that users can download and run Things you can do with registries: Search for public images; Pull official images; Host and share private images; Push your images. You must have an account on each registry in order to create repositories and to host your own images. You can create multiple repositories; You can create multiple tagged images in a repository; You can set up your own private registry using a Docker Trusted Registry .","title":"Docker Registries"},{"location":"docker/registry/#search-image-registries","text":"Warning Only use images from trusted sources or images for which you can see the Dockerfile . Any image from an untrusted source could contain something other than what's indicated. If you can see the Dockerfile you can see exactly what is in the image. Docker looks into the Docker Hub public registry by default. Examples of public/private registries to consider for your research needs: Registry Name Container Types Docker Hub Official Images for Docker Amazon Elastic Container Registry run containers on AWS Google Container Registry run containers on Google Cloud Azure Container Registry run containers on Azure NVIDIA GPU Cloud containers for GPU computing GitHub Container Registry managed containers on GitHub GitLab Container Registry managed containers on GitLab RedHat Quay.io containers managed by RedHat BioContainers Registry bioinformatics containers","title":"Search Image Registries"},{"location":"docker/registry/#dockerhub","text":"Docker Hub is a service provided by Docker for finding and sharing container images with your team. Docker Hub is the most well-known and popular image registry for Docker containers. Info Registry : a storage and distribution system for named Docker images Repository : collection of \"images\" with individual \"tags\". Teams & Organizations: Manages access to private repositories. Builds: Automatically build container images from GitHub or Bitbucket on the Docker Hub. Webhooks: Trigger actions after a successful push to a repository to integrate Docker Hub with other services.","title":" DockerHub"},{"location":"docker/registry/#biocontainers","text":"BioContainers is a community-driven project that provides the infrastructure and basic guidelines to create, manage and distribute bioinformatics containers with special focus in proteomics, genomics, transcriptomics and metabolomics . BioContainers is based on the popular frameworks of Docker. Although anyone can create a BioContainer, the majority of BioContainers are created by the Bioconda project. Every Bioconda package has a corresponding BioContainer available at Quay.io.","title":" BioContainers"},{"location":"docker/registry/#redhat-quayio","text":"Quay is another general image registry. It works the same way as Docker Hub. However, Quay is home to all BioContainers made by the Bioconda project. Now we will find a BioContainer image at Quay, pull that image and run it on cloud virtual machine.","title":" RedHat Quay.io"},{"location":"docker/registry/#nvidia-gpu-cloud","text":"NVIDIA is one of the leading makers of graphic processing units (GPU). GPU were established as a means of handling graphics processing operations for video cards, but have been greatly expanded for use in generalized computing applications, Machine Learning, image processing, and matrix-based linear algebras. NVIDIA have created their own set of Docker containers and Registries for running on CPU-GPU enabled systems. NVIDIA-Docker runs atop the NVIDIA graphics drivers on the host system, the NVIDIA drivers are imported to the container at runtime. NVIDIA Docker Hub hosts numerous NVIDIA Docker containers, from which you can build your own images. NVIDIA GPU Cloud hosts numerous containers for HPC and Cloud applications. You must register an account with them (free) to access these. NVIDIA GPU Cloud hosts three registry spaces nvcr.io/nvidia - catalog of fully integrated and optimized deep learning framework containers. nvcr.io/nvidia-hpcvis - catalog of HPC visualization containers (beta). nvcr.io/hpc - popular third-party GPU ready HPC application containers. NVIDIA Docker can be used as a base-image to create containers running graphical applications remotely. High resolution 3D screens are piped to a remote desktop platform.","title":" NVIDIA GPU Cloud"},{"location":"getting_started/code_conduct/","text":"Code of Conduct \u00b6 All attendees, speakers, staff and volunteers at Container Camp are required to follow our code of conduct. CyVerse expects and appreciates cooperation from all participants to help ensure a safe, collaborative environment for everyone. Harrassment by any individual will not be tolerated and may result in the individual being removed from the Camp. Harassment includes: offensive verbal comments related to gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion, technology choices, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants who are asked to stop any harassing behavior are expected to comply immediately. Workshop staff are also subject to the anti-harassment policy. In particular, staff should not use sexualised images, activities, or other material. If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund. If you are being harassed, or notice that someone else is being harassed, or have any other concerns, please contact a member of the workshop staff immediately. Staff can be identified as they'll be wearing badges or nametags. Workshop staff will be happy to help participants contact local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance. We expect participants to follow these rules at conference and workshop venues and conference-related social events. See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.","title":"Code of Conduct"},{"location":"getting_started/code_conduct/#code-of-conduct","text":"All attendees, speakers, staff and volunteers at Container Camp are required to follow our code of conduct. CyVerse expects and appreciates cooperation from all participants to help ensure a safe, collaborative environment for everyone. Harrassment by any individual will not be tolerated and may result in the individual being removed from the Camp. Harassment includes: offensive verbal comments related to gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion, technology choices, sexual images in public spaces, deliberate intimidation, stalking, following, harassing photography or recording, sustained disruption of talks or other events, inappropriate physical contact, and unwelcome sexual attention. Participants who are asked to stop any harassing behavior are expected to comply immediately. Workshop staff are also subject to the anti-harassment policy. In particular, staff should not use sexualised images, activities, or other material. If a participant engages in harassing behavior, the workshop organisers may take any action they deem appropriate, including warning the offender or expulsion from the workshop with no refund. If you are being harassed, or notice that someone else is being harassed, or have any other concerns, please contact a member of the workshop staff immediately. Staff can be identified as they'll be wearing badges or nametags. Workshop staff will be happy to help participants contact local law enforcement, provide escorts, or otherwise assist those experiencing harassment to feel safe for the duration of the workshop. We value your attendance. We expect participants to follow these rules at conference and workshop venues and conference-related social events. See http://www.ashedryden.com/blog/codes-of-conduct-101-faq for more information on codes of conduct.","title":" Code of Conduct"},{"location":"getting_started/logistics/","text":"Logistics \u00b6 Location \u00b6 Basics Camp will be held virtually, a Zoom URL will be sent to registered participants. Advanced Camp will be hosted in the Health Sciences Innovation Building (HSIB) on the University of Arizona Campus: Room # 444 Transportation \u00b6 Visitor Parking Interactive Map -- Parking is available around campus in designated lots: https://parking.arizona.edu/parking/ Cat Tran Shuttle service drops off near the HSIB. Sun Link StreetCar service drops off near HSIB and goes through Downtown. Tucson International Airport is located eight miles south of campus and downtown. Local ride-sharing services and public transit are available between campus and local hotels. Hotels \u00b6 Campus hotels with transportation: Aloft Tucson University The Graduate Tucson Tucson Marriot University Park Meals \u00b6 Tucson is a UNESCO World City of Gastronomy . There is no shortage of amazing local restaurants to explore near the University. Advanced Container Campers will be able to access local food trucks and cafes in the immediate vicinity of the workshop classroom. Coffee, water, and snacks will be provided. Emergency Medicine \u00b6 HSIB is adjacent the Banner University Medical Center . In the event of a medical emergency, attendees may be transported to Banner, or to the nearest urgent care facility .","title":":material-directions-fork: Logistics"},{"location":"getting_started/logistics/#logistics","text":"","title":" Logistics"},{"location":"getting_started/logistics/#location","text":"Basics Camp will be held virtually, a Zoom URL will be sent to registered participants. Advanced Camp will be hosted in the Health Sciences Innovation Building (HSIB) on the University of Arizona Campus: Room # 444","title":" Location"},{"location":"getting_started/logistics/#transportation","text":"Visitor Parking Interactive Map -- Parking is available around campus in designated lots: https://parking.arizona.edu/parking/ Cat Tran Shuttle service drops off near the HSIB. Sun Link StreetCar service drops off near HSIB and goes through Downtown. Tucson International Airport is located eight miles south of campus and downtown. Local ride-sharing services and public transit are available between campus and local hotels.","title":" Transportation"},{"location":"getting_started/logistics/#hotels","text":"Campus hotels with transportation: Aloft Tucson University The Graduate Tucson Tucson Marriot University Park","title":" Hotels"},{"location":"getting_started/logistics/#meals","text":"Tucson is a UNESCO World City of Gastronomy . There is no shortage of amazing local restaurants to explore near the University. Advanced Container Campers will be able to access local food trucks and cafes in the immediate vicinity of the workshop classroom. Coffee, water, and snacks will be provided.","title":" Meals"},{"location":"getting_started/logistics/#emergency-medicine","text":"HSIB is adjacent the Banner University Medical Center . In the event of a medical emergency, attendees may be transported to Banner, or to the nearest urgent care facility .","title":" Emergency Medicine"},{"location":"getting_started/schedule/","text":"Container Camp Basics \u00b6 Dates : Mar 6 th -8 th 2023 Time : 09:00AM\u201313:00PM US Mountain Standard Time (8am-12pm PST; 9am-1pm MST/AZ; 10am-2pm CST; 11am-3pm EST) Location : Virtual Zoom Day 1 - Introduction to Docker \u00b6 Content Introduction to Docker and its uses in reproducible science Launching development environments on CodeSpaces for container testing Using Docker on the commandline. Goals Introduction to containers & where to find them Command line containers with CodeSpaces (optional: run locally) Find and use official Docker images Activities Time (MST/AZ) Activity Instructor Outcome 09:00 Welcome All 09:05 Overview of Website Michele Code of Conduct 09:10 Why use Containers? Carlos What containers are used for in science 09:25 Start CodeSpace Tyson Using Dev Environments to create containers 09:30 Docker Commands and Execution Carlos Basic command line use of Docker 09:55 Break 10:00 Docker Commands and Execution Carlos & Tyson Basic command line use of Docker 10:55 Break 11:00 Managing Docker and Data Michele Volumes and Interactive Use inside containers 11:55 Break 12:00 Managing Docker and Data Michele & Tyson 12:55 Conclude for the day All push changes to your GitHub Optional Homework Test other Docker container images on CodeSpaces or locally Day 2 - Building Docker Containers \u00b6 Content Use GitHub to browse for public Dockerfiles Build Dockerfiles and push them to public registry Use Version Control to set up automated container builds with GitHub Actions Goals Introduction to what Dockerfiles are and what you use them for Start thinking about how to modify them for your own applications Activities Time (MST/AZ) Activity Instructor Notes 09:00 Welcome back All 09:05 Discuss previous day, answer questions 09:15 (re)start Dev Environment Tyson 09:20 Finding the right container Tyson 09:55 Break 10:00 Building Docker Images Carlos 10:55 Break 11:00 Using Docker Compose Michele 11:55 Break 12:00 Integrating your Containers into CyVerse Tyson 12:55 Conclude Day 3 - Singularity, Orchestration, and Containers on the HPC \u00b6 Content Introduction to Singularity Introduction to Kubernetes Goals Being able to execute containers on the HPC Create a small Kubernetes cluster Activities Time (MST/AZ) Activity Instructor Notes 09:00 Welcome back All 09:05 Discuss previous day, answer questions 09:15 (re(re))start Dev Environment 09:20 Introduction to Singularity 09:30 Obtaining Singularity and the Singularity CLI 09:55 Break 10:00 Singularity Commands & Interacting with Singularity Images 10:55 Break 11:00 Singularity Images Continued * 11:55 Break 12:00 Integrating your Containers into CyVerse , questions, inquiries 12:55 Conclude** If we still have time, we will also discuss using Singularity on the HPC . ** For additional questions and inquiries, we will make time for one-on-ones on either Thursday or Friday (flexible schedule).","title":"Schedule"},{"location":"getting_started/schedule/#container-camp-basics","text":"Dates : Mar 6 th -8 th 2023 Time : 09:00AM\u201313:00PM US Mountain Standard Time (8am-12pm PST; 9am-1pm MST/AZ; 10am-2pm CST; 11am-3pm EST) Location : Virtual Zoom","title":" Container Camp Basics"},{"location":"getting_started/schedule/#day-1-introduction-to-docker","text":"Content Introduction to Docker and its uses in reproducible science Launching development environments on CodeSpaces for container testing Using Docker on the commandline. Goals Introduction to containers & where to find them Command line containers with CodeSpaces (optional: run locally) Find and use official Docker images Activities Time (MST/AZ) Activity Instructor Outcome 09:00 Welcome All 09:05 Overview of Website Michele Code of Conduct 09:10 Why use Containers? Carlos What containers are used for in science 09:25 Start CodeSpace Tyson Using Dev Environments to create containers 09:30 Docker Commands and Execution Carlos Basic command line use of Docker 09:55 Break 10:00 Docker Commands and Execution Carlos & Tyson Basic command line use of Docker 10:55 Break 11:00 Managing Docker and Data Michele Volumes and Interactive Use inside containers 11:55 Break 12:00 Managing Docker and Data Michele & Tyson 12:55 Conclude for the day All push changes to your GitHub Optional Homework Test other Docker container images on CodeSpaces or locally","title":"Day 1 - Introduction to Docker"},{"location":"getting_started/schedule/#day-2-building-docker-containers","text":"Content Use GitHub to browse for public Dockerfiles Build Dockerfiles and push them to public registry Use Version Control to set up automated container builds with GitHub Actions Goals Introduction to what Dockerfiles are and what you use them for Start thinking about how to modify them for your own applications Activities Time (MST/AZ) Activity Instructor Notes 09:00 Welcome back All 09:05 Discuss previous day, answer questions 09:15 (re)start Dev Environment Tyson 09:20 Finding the right container Tyson 09:55 Break 10:00 Building Docker Images Carlos 10:55 Break 11:00 Using Docker Compose Michele 11:55 Break 12:00 Integrating your Containers into CyVerse Tyson 12:55 Conclude","title":"Day 2 - Building Docker Containers"},{"location":"getting_started/schedule/#day-3-singularity-orchestration-and-containers-on-the-hpc","text":"Content Introduction to Singularity Introduction to Kubernetes Goals Being able to execute containers on the HPC Create a small Kubernetes cluster Activities Time (MST/AZ) Activity Instructor Notes 09:00 Welcome back All 09:05 Discuss previous day, answer questions 09:15 (re(re))start Dev Environment 09:20 Introduction to Singularity 09:30 Obtaining Singularity and the Singularity CLI 09:55 Break 10:00 Singularity Commands & Interacting with Singularity Images 10:55 Break 11:00 Singularity Images Continued * 11:55 Break 12:00 Integrating your Containers into CyVerse , questions, inquiries 12:55 Conclude** If we still have time, we will also discuss using Singularity on the HPC . ** For additional questions and inquiries, we will make time for one-on-ones on either Thursday or Friday (flexible schedule).","title":"Day 3 - Singularity, Orchestration, and Containers on the HPC"},{"location":"getting_started/setup/","text":"Prerequisites \u00b6 Before attending Basics Container Camp please do the following: Create a GitHub Account Create a Docker Hub Account Create a CyVerse Account Before attending Advanced Container Camp please do the following: (all of the above) Create a XSEDE profile Create a Texas Advanced Computing Center (TACC) profile","title":"Pre-Camp Setup"},{"location":"getting_started/setup/#prerequisites","text":"Before attending Basics Container Camp please do the following: Create a GitHub Account Create a Docker Hub Account Create a CyVerse Account Before attending Advanced Container Camp please do the following: (all of the above) Create a XSEDE profile Create a Texas Advanced Computing Center (TACC) profile","title":" Prerequisites"},{"location":"orchestration/advk8s/","text":"Deploying your own Kubernetes clusters \u00b6 We are going to be using Jetstream-2 for this section and will rely on their documentation for Kubernetes deployment Helm \u00b6 Helm is the package manager for Kubernetes. Zero to JupyterHub \u00b6 JupyterHub for Kubernetes Project Jupyter maintains a lesson on deploying K8s and Helm with minik8s across a variety of commercial cloud solutions. The closest example that you can attempt is the Bare Metal example on JS-2 Miniaturized versions of Kubernetes \u00b6 Why use mini versions of Kubernetes? There are multiple projects developing \"light-weight\" Kubernetes. The justification for these projects being that full Kubernetes deployments with all of its functionality increases the cognitive load and the number of configurations and parameters that you must work with when running containers and clusters, particularly at the edge. Projects that are working on miniaturized versions of K8s: name functionality use cases minikube microK8s runs fast, self-healing, and highly available Kubernetes clusters K3s runs production-level Kubernetes workloads on low resourced and remotely located IoT and Edge devices K3d lightweight wrapper that runs K3s in a docker container Install K3s \u00b6 K3s Install minikube \u00b6 Minikube is useful for running K8s on a single node or locally -- its primary use is to teach you how to use K8s. curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube sudo minikube config set vm-driver none Installing microK8s \u00b6","title":"Advanced use of Kubernetes"},{"location":"orchestration/advk8s/#deploying-your-own-kubernetes-clusters","text":"We are going to be using Jetstream-2 for this section and will rely on their documentation for Kubernetes deployment","title":"Deploying your own Kubernetes clusters"},{"location":"orchestration/advk8s/#helm","text":"Helm is the package manager for Kubernetes.","title":" Helm"},{"location":"orchestration/advk8s/#zero-to-jupyterhub","text":"JupyterHub for Kubernetes Project Jupyter maintains a lesson on deploying K8s and Helm with minik8s across a variety of commercial cloud solutions. The closest example that you can attempt is the Bare Metal example on JS-2","title":"Zero to JupyterHub"},{"location":"orchestration/advk8s/#miniaturized-versions-of-kubernetes","text":"Why use mini versions of Kubernetes? There are multiple projects developing \"light-weight\" Kubernetes. The justification for these projects being that full Kubernetes deployments with all of its functionality increases the cognitive load and the number of configurations and parameters that you must work with when running containers and clusters, particularly at the edge. Projects that are working on miniaturized versions of K8s: name functionality use cases minikube microK8s runs fast, self-healing, and highly available Kubernetes clusters K3s runs production-level Kubernetes workloads on low resourced and remotely located IoT and Edge devices K3d lightweight wrapper that runs K3s in a docker container","title":"Miniaturized versions of Kubernetes"},{"location":"orchestration/advk8s/#install-k3s","text":"K3s","title":"Install K3s"},{"location":"orchestration/advk8s/#install-minikube","text":"Minikube is useful for running K8s on a single node or locally -- its primary use is to teach you how to use K8s. curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 sudo install minikube-linux-amd64 /usr/local/bin/minikube sudo minikube config set vm-driver none","title":"Install minikube"},{"location":"orchestration/advk8s/#installing-microk8s","text":"","title":"Installing microK8s"},{"location":"orchestration/advterra/","text":"Creating Terraform Templates on OpenStack \u00b6 Overview \u00b6 This advanced tutorial will guide you through setting up a Terraform project and deploying virtual machines (VMs) with Docker & lightweight Kubernetes ( Rancher K3s ). Goals Understand how to install software and provision multiple VMs Understand how Terraform is used to orchestrate VMs together Ability to create a Terraform deployment for a multi-node JupyterHub Ability to re-provisioning an already deployed cluster on OpenStack through Terraform Things we won't cover Basic management of Kubernetes clusters (that is a different lesson) All of Terraform's features Prerequisites \u00b6 Basic understanding of OpenStack and Terraform as was covered in the prior lesson Access to an OpenStack cloud (we will use Jetstream2 ) Terraform installed on your workstation or local machine Outcomes By the end of this tutorial, you will have launched a lightweight Kubernetes (K3s) cluster know how to modify the size of cluster to scale up and down with more or less resources started and stopped a JupyterHub using a public Docker image Updating Terraform Configurations \u00b6 We will re-use our configurations from the Terraform basics lesson We will update and append the following files: variables.tf - we are going to be adding new variables to include specific features related to our K3s and JupyterHub instances.tf - we are going to be launching multiple VM flavors to support our cluster. We want the server VM to be large enough to manage all of the JupyterHub worker nodes. security.tf - we need to specify all of the new security and networking settings for K3s and JupyterHub We will create new configuration files for: k3s.tf - we will keep our Kubernetes (Rancher) configuration files here docker.tf - we will specify the JupyterHub Docker image here jupyterhub.tf - we will configure our JupyterHub here instances.tf \u00b6 Our new cluster is going to be variably sized, meaning we can re-size or re-scale the number of VMs running in our cluster to account for demand by our users. k3s.tf \u00b6 docker.tf \u00b6 We are going to be using the featured Datascience Latest image from the Project Jupyter team jupyterhub.tf \u00b6 variables.tf \u00b6 Start the Deployment \u00b6 Re-sizing the VMs while the Deployment is running \u00b6 Adding / Removing VMs from the cluster while it is running \u00b6 Managing the K3s configuration \u00b6 Restarting the JupyterHub \u00b6","title":"Advanced use of Terraform"},{"location":"orchestration/advterra/#creating-terraform-templates-on-openstack","text":"","title":"Creating Terraform Templates on OpenStack"},{"location":"orchestration/advterra/#overview","text":"This advanced tutorial will guide you through setting up a Terraform project and deploying virtual machines (VMs) with Docker & lightweight Kubernetes ( Rancher K3s ). Goals Understand how to install software and provision multiple VMs Understand how Terraform is used to orchestrate VMs together Ability to create a Terraform deployment for a multi-node JupyterHub Ability to re-provisioning an already deployed cluster on OpenStack through Terraform Things we won't cover Basic management of Kubernetes clusters (that is a different lesson) All of Terraform's features","title":"Overview"},{"location":"orchestration/advterra/#prerequisites","text":"Basic understanding of OpenStack and Terraform as was covered in the prior lesson Access to an OpenStack cloud (we will use Jetstream2 ) Terraform installed on your workstation or local machine Outcomes By the end of this tutorial, you will have launched a lightweight Kubernetes (K3s) cluster know how to modify the size of cluster to scale up and down with more or less resources started and stopped a JupyterHub using a public Docker image","title":"Prerequisites"},{"location":"orchestration/advterra/#updating-terraform-configurations","text":"We will re-use our configurations from the Terraform basics lesson We will update and append the following files: variables.tf - we are going to be adding new variables to include specific features related to our K3s and JupyterHub instances.tf - we are going to be launching multiple VM flavors to support our cluster. We want the server VM to be large enough to manage all of the JupyterHub worker nodes. security.tf - we need to specify all of the new security and networking settings for K3s and JupyterHub We will create new configuration files for: k3s.tf - we will keep our Kubernetes (Rancher) configuration files here docker.tf - we will specify the JupyterHub Docker image here jupyterhub.tf - we will configure our JupyterHub here","title":"Updating Terraform Configurations"},{"location":"orchestration/advterra/#instancestf","text":"Our new cluster is going to be variably sized, meaning we can re-size or re-scale the number of VMs running in our cluster to account for demand by our users.","title":" instances.tf"},{"location":"orchestration/advterra/#k3stf","text":"","title":" k3s.tf"},{"location":"orchestration/advterra/#dockertf","text":"We are going to be using the featured Datascience Latest image from the Project Jupyter team","title":" docker.tf"},{"location":"orchestration/advterra/#jupyterhubtf","text":"","title":" jupyterhub.tf"},{"location":"orchestration/advterra/#variablestf","text":"","title":" variables.tf"},{"location":"orchestration/advterra/#start-the-deployment","text":"","title":"Start the Deployment"},{"location":"orchestration/advterra/#re-sizing-the-vms-while-the-deployment-is-running","text":"","title":"Re-sizing the VMs while the Deployment is running"},{"location":"orchestration/advterra/#adding-removing-vms-from-the-cluster-while-it-is-running","text":"","title":"Adding / Removing VMs from the cluster while it is running"},{"location":"orchestration/advterra/#managing-the-k3s-configuration","text":"","title":"Managing the K3s configuration"},{"location":"orchestration/advterra/#restarting-the-jupyterhub","text":"","title":"Restarting the JupyterHub"},{"location":"orchestration/cacao/","text":"Log into CACAO (JS2) \u00b6 To start, you will need an Xsede user account and have access to a project. Go to Jetstream-2 CACAO to log in with your Xsede account. You will be prompted to allow Jetstream 2 to authenticate with Globus. Add your Credential \u00b6 Next, you will need to add cloud computing credentials. For now, the only option is Jetstream 2, but in the future, other providers like Google and AWS will be supported. Click on Credentials on the lefthand menu bar. From the Credentials page, click the \"+ Add Credential\" button and select \"Cloud Credential\". Select Jetstream 2 and the project you would like to add. These correspond to Jetstream 2 projects you have access to. Once your credential is added, it should show up on the Credentials page. Start a Deployment \u00b6 Next, we will start a deployment onto Jetstream 2. Click on the Deployments tab on the lefthand menu bar. You will see your cloud providers and projects, and if you have multiple providers or projects, you can select them here. You should have Jetstream 2 and a project selected. Next, click the \"+ Add Deployment\" button. You will see several options for default templates to launch VMs, containers, or whole clusters. We will use the first template to launch a single VM. You will then name the deployment, select the number of instances and size of instances. For now, stick with the default Featured-AlmaLinux8 image and 1 instance of m3.tiny. You can name it whatever you want. Access Deployment \u00b6 Once you have submitted your deployment, you can see it on the Deployments page. It will be in the \"Starting\" status for a few minutes. Once it is running, you can click on it to see details about it and to access it. From here, you can click on the icons on the right to access a Web Shell or a Web Desktop (for certain images). You can also pause, shelve, or delete the deployment from here. Try opening the Web Shell, which will bring you to a command line inside your running VM.","title":"CyVerse CACAO Interface"},{"location":"orchestration/cacao/#log-into-cacao-js2","text":"To start, you will need an Xsede user account and have access to a project. Go to Jetstream-2 CACAO to log in with your Xsede account. You will be prompted to allow Jetstream 2 to authenticate with Globus.","title":"Log into CACAO (JS2)"},{"location":"orchestration/cacao/#add-your-credential","text":"Next, you will need to add cloud computing credentials. For now, the only option is Jetstream 2, but in the future, other providers like Google and AWS will be supported. Click on Credentials on the lefthand menu bar. From the Credentials page, click the \"+ Add Credential\" button and select \"Cloud Credential\". Select Jetstream 2 and the project you would like to add. These correspond to Jetstream 2 projects you have access to. Once your credential is added, it should show up on the Credentials page.","title":"Add your Credential"},{"location":"orchestration/cacao/#start-a-deployment","text":"Next, we will start a deployment onto Jetstream 2. Click on the Deployments tab on the lefthand menu bar. You will see your cloud providers and projects, and if you have multiple providers or projects, you can select them here. You should have Jetstream 2 and a project selected. Next, click the \"+ Add Deployment\" button. You will see several options for default templates to launch VMs, containers, or whole clusters. We will use the first template to launch a single VM. You will then name the deployment, select the number of instances and size of instances. For now, stick with the default Featured-AlmaLinux8 image and 1 instance of m3.tiny. You can name it whatever you want.","title":"Start a Deployment"},{"location":"orchestration/cacao/#access-deployment","text":"Once you have submitted your deployment, you can see it on the Deployments page. It will be in the \"Starting\" status for a few minutes. Once it is running, you can click on it to see details about it and to access it. From here, you can click on the icons on the right to access a Web Shell or a Web Desktop (for certain images). You can also pause, shelve, or delete the deployment from here. Try opening the Web Shell, which will bring you to a command line inside your running VM.","title":"Access Deployment"},{"location":"orchestration/cacao_terra/","text":"CACAO Terraform templates on the CLI \u00b6 Overview \u00b6 This tutorial will guide you through the process of deploying Zero to JupyterHub with Kubernetes on an OpenStack VM using Terraform. We will be using a Terraform template for CACAO which has additional configurations. By the end of this tutorial, you will have a running JupyterHub instance that can be accessed by your users. Prerequisites \u00b6 Basic understanding of Terraform, Kubernetes, and JupyterHub. An OpenStack environment with access to the API. Terraform installed on your local machine or VM server. kubectl and helm command-line tools installed on your local machine. An SSH key pair to access the VM server running Terraform. Installation \u00b6 Terraform needs a dedicated resource to manage its deployments. Terraform - install Terraform on your server Ansible - Optional: VS Code Terraform Extension - add VS Code extension Mac OS X Installation Instructions for Mac OS X installation If you're on OS X, you can use brew to install with the following commands: # install terraform -- taken from https://learn.hashicorp.com/tutorials/terraform/install-cli brew tap hashicorp/tap && brew install hashicorp/tap/terraform # install ansible and jq (for processing terraform's statefile into an ansible inventory) brew install ansible jq Linux Installation Instructions for Ubuntu 22.04 installation wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update && sudo apt install terraform Install Ansible & J Query sudo apt-add-repository ppa:ansible/ansible sudo apt update & sudo apt install -y ansible jq CyVerse CACAO Browser UI \u00b6 CyVerse's CACAO (Cloud Automation & Continuous Analysis Orchestration) CyVerse CACAO CLI \u00b6 CACAO file structure \u00b6 terraform-project / \u251c\u2500\u2500 main . tf \u251c\u2500\u2500 inputs . tf \u251c\u2500\u2500 outputs . tf \u251c\u2500\u2500 output-ansible-inventory . tf \u251c\u2500\u2500 metadata . json \u251c\u2500\u2500 terraform . tfvars . example \u2514\u2500\u2500 ansible / \u251c\u2500\u2500 ansible . cfg \u251c\u2500\u2500 find_connection . yaml \u251c\u2500\u2500 playbook . yaml \u2514\u2500\u2500 requirements . yaml Main Configuration File ( main.tf ): - contains the primary infrastructure resources and configurations for virtual machines, networks, and storage. Inputs File ( inputs.tf ): - defines all the input variables used in the configuration. Declare variables with default values or leave them empty for required user input. Include descriptions for each variable to provide context. Outputs File ( outputs.tf ): - defines the outputs Terraform displays after applying the Main and Variables configuration. Includes: IP addresses, DNS names, or information for resources. Provider Configuration File ( provider.tf ): - Modules and Reusable Configurations: - create separate .tf files for reusable modules and configurations. Reuse across multiple projects or within the same project on multiple VMs. File Examples \u00b6 main.tf \u00b6 Start-up the Terraform Server \u00b6 We need to run terraform from somewhere, this should be a permanent instance which is not pre-emptable (suggest a tiny VM on Jetstream2) Before we start terraform , we need to create an SSH key to add to our newly deployed VMs. Create SSH keys To create an SSH key on an Ubuntu 22.04 terminal, you can follow these steps: Step 1: Open your terminal and type the following command to generate a new SSH key pair: ssh-keygen -t rsa -b 4096 Step 2: When prompted, press \"Enter\" to save the key in the default location, or enter a different filename and location to save the key. Enter a passphrase to secure your key. This passphrase will be required to use the key later. Once the key is generated, you can add it to your SSH agent by running the following command: eval \" $( ssh-agent -s ) \" ssh-add ~/.ssh/id_rsa Step 3: Copy the public key to your remote server by running the following command, replacing \"user\" and \"server\" with your username and server address: ssh-copy-id <user>@<server-ip> create_ssh_script.sh: #!/bin/bash echo \"Generating SSH key pair...\" ssh-keygen -t rsa -b 4096 echo \"Adding key to SSH agent...\" eval \" $( ssh-agent -s ) \" ssh-add ~/.ssh/id_rsa read -p \"Enter your remote server username: \" username read -p \"Enter your remote server IP address: \" server echo \"Ready to copy your new public key to a remote server:\" ssh-copy-id username@server echo \"SSH key setup complete!\" Save the script to a file, make it executable with the following command: chmod +x create_ssh_script.sh run it with the following command: ./create_ssh_script.sh Running Terraform \u00b6 Step 2 Clone the CACAO z2jh repository git clone https://gitlab.com/stack0/cacao-tf-jupyterhub.git ** Step 3** Initialize Terraform in the repository Change directory to the repo: cd cacao-tf-jupyterhub Run terraform init Step 3 Source the OpenRC shell script source *-openrc.sh Step 4 Start Terraform terraform apply -auto-approve Step 5 Suspend Terraform Update your terraform.tfvars file by changing the power_state from \"active\" to \"shelved_offloaded\" ### Set Power Status ### power_state = \"active\" # power_state = \"shelved_offloaded\" re-run terraform apply -auto-approve to update the deployment Step 6 Destroying a Terraform deployment when done, with your cluster, run terraform destroy -auto-approve to take the cluster back down. Editing Terraform Variables file (.tfvars) \u00b6 Zero to JupyterHub with Kubernetes Deployment Example Terraform variables (.tfvars) file for deploying Zero to JupyterHub with K3s (Rancher) Based on the Zero to JupyterHub with Kubernetes ### Set the floating_ip address for DNS redirections ### # jupyterhub_floating_ip=\"149.165.152.223\" ### Set up the Security Groups (re=used from Horizon) ### security_groups = [\"default\",\"cacao-default\",\"cacao-k8s\"] ### Give the Deployment a Name (shorter is better) ### instance_name = \"jh-prod\" ### Horizon Account User ID and ACCESS-CI Project Account ### username = \"${USER}\" project = \"BIO220085\" # Horizon SSH key-pair keypair = \"${USER}-default\" ### Set the Number of Instances (Master + Workers) ### instance_count = 6 # Storage Volume Size in GiB jh_storage_size=1000 # Storage Volume directory path jh_storage_mount_dir=\"/home/shared\" # OS type image_name = \"Featured-Ubuntu22\" # Worker VM Flavor flavor = \"m3.medium\" # Master VM Flavor (m3.medium or larger is recommended) flavor_master = \"m3.medium\" ### Run JupyterHub on Master Node ### do_jh_singleuser_exclude_master=\"true\" # Jetstream2 Region region = \"IU\" # VM image (root disk) sizes root_storage_size = 100 root_storage_type = \"volume\" # Enable GPUs (true/false) - use \"true\" with \"g3\" flavors do_enable_gpu=false ### Zero 2 JupyterHub with K3s Rancher Configuration ### jupyterhub_deploy_strategy=\"z2jh_k3s\" ### Ansible-based Setup for JupyterHub ### do_ansible_execution=true ### CPU usage per user utilization 10000m = 100% of 1 core ### jh_resources_request_cpu=\"3000m\" ### Memory RAM per-user requirement in Gigibytes ### jh_resources_request_memory=\"10Gi\" ### Set Power Status ### power_state = \"active\" # power_state = \"shelved_offloaded\" ### Pre-Cache images? ### do_jh_precache_images=false user_data = \"\" ### JupyterHub Configurations ### # set the type of authentication, comment out Dummy or OAuth # OAuth-type (GitHub Authentication) # jupyterhub_oauth2_clientid=\"02b0a98c5d1547101514\" # jupyterhub_oauth2_secret=\"ac658055d848e787c31803afd973a05fbdb98f64\" # Dummy-type Authentication jupyterhub_authentication=\"dummy\" # set the dummy username password jupyterhub_dummy_password=\"password\" # Specify the JupyterHub admins jupyterhub_admins=\"${USER}\" # add JupyterHub student account usernames jupyterhub_allowed_users=\"${USER}, student1, student2, student3, student4\" ### Select a JupyterHub-ready image ### #jupyterhub_singleuser_image=\"harbor.cyverse.org/vice/jupyter/pytorch\" jupyterhub_singleuser_image=\"jupyter/datascience-notebook\" jupyterhub_singleuser_image_tag=\"latest\"","title":"CyVerse CACAO Terraform CLI"},{"location":"orchestration/cacao_terra/#cacao-terraform-templates-on-the-cli","text":"","title":"CACAO Terraform templates on the CLI"},{"location":"orchestration/cacao_terra/#overview","text":"This tutorial will guide you through the process of deploying Zero to JupyterHub with Kubernetes on an OpenStack VM using Terraform. We will be using a Terraform template for CACAO which has additional configurations. By the end of this tutorial, you will have a running JupyterHub instance that can be accessed by your users.","title":"Overview"},{"location":"orchestration/cacao_terra/#prerequisites","text":"Basic understanding of Terraform, Kubernetes, and JupyterHub. An OpenStack environment with access to the API. Terraform installed on your local machine or VM server. kubectl and helm command-line tools installed on your local machine. An SSH key pair to access the VM server running Terraform.","title":"Prerequisites"},{"location":"orchestration/cacao_terra/#installation","text":"Terraform needs a dedicated resource to manage its deployments. Terraform - install Terraform on your server Ansible - Optional: VS Code Terraform Extension - add VS Code extension Mac OS X Installation Instructions for Mac OS X installation If you're on OS X, you can use brew to install with the following commands: # install terraform -- taken from https://learn.hashicorp.com/tutorials/terraform/install-cli brew tap hashicorp/tap && brew install hashicorp/tap/terraform # install ansible and jq (for processing terraform's statefile into an ansible inventory) brew install ansible jq Linux Installation Instructions for Ubuntu 22.04 installation wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update && sudo apt install terraform Install Ansible & J Query sudo apt-add-repository ppa:ansible/ansible sudo apt update & sudo apt install -y ansible jq","title":"Installation"},{"location":"orchestration/cacao_terra/#cyverse-cacao-browser-ui","text":"CyVerse's CACAO (Cloud Automation & Continuous Analysis Orchestration)","title":"CyVerse CACAO Browser UI"},{"location":"orchestration/cacao_terra/#cyverse-cacao-cli","text":"","title":"CyVerse CACAO CLI"},{"location":"orchestration/cacao_terra/#cacao-file-structure","text":"terraform-project / \u251c\u2500\u2500 main . tf \u251c\u2500\u2500 inputs . tf \u251c\u2500\u2500 outputs . tf \u251c\u2500\u2500 output-ansible-inventory . tf \u251c\u2500\u2500 metadata . json \u251c\u2500\u2500 terraform . tfvars . example \u2514\u2500\u2500 ansible / \u251c\u2500\u2500 ansible . cfg \u251c\u2500\u2500 find_connection . yaml \u251c\u2500\u2500 playbook . yaml \u2514\u2500\u2500 requirements . yaml Main Configuration File ( main.tf ): - contains the primary infrastructure resources and configurations for virtual machines, networks, and storage. Inputs File ( inputs.tf ): - defines all the input variables used in the configuration. Declare variables with default values or leave them empty for required user input. Include descriptions for each variable to provide context. Outputs File ( outputs.tf ): - defines the outputs Terraform displays after applying the Main and Variables configuration. Includes: IP addresses, DNS names, or information for resources. Provider Configuration File ( provider.tf ): - Modules and Reusable Configurations: - create separate .tf files for reusable modules and configurations. Reuse across multiple projects or within the same project on multiple VMs.","title":"CACAO file structure"},{"location":"orchestration/cacao_terra/#file-examples","text":"","title":"File Examples"},{"location":"orchestration/cacao_terra/#maintf","text":"","title":" main.tf"},{"location":"orchestration/cacao_terra/#start-up-the-terraform-server","text":"We need to run terraform from somewhere, this should be a permanent instance which is not pre-emptable (suggest a tiny VM on Jetstream2) Before we start terraform , we need to create an SSH key to add to our newly deployed VMs. Create SSH keys To create an SSH key on an Ubuntu 22.04 terminal, you can follow these steps: Step 1: Open your terminal and type the following command to generate a new SSH key pair: ssh-keygen -t rsa -b 4096 Step 2: When prompted, press \"Enter\" to save the key in the default location, or enter a different filename and location to save the key. Enter a passphrase to secure your key. This passphrase will be required to use the key later. Once the key is generated, you can add it to your SSH agent by running the following command: eval \" $( ssh-agent -s ) \" ssh-add ~/.ssh/id_rsa Step 3: Copy the public key to your remote server by running the following command, replacing \"user\" and \"server\" with your username and server address: ssh-copy-id <user>@<server-ip> create_ssh_script.sh: #!/bin/bash echo \"Generating SSH key pair...\" ssh-keygen -t rsa -b 4096 echo \"Adding key to SSH agent...\" eval \" $( ssh-agent -s ) \" ssh-add ~/.ssh/id_rsa read -p \"Enter your remote server username: \" username read -p \"Enter your remote server IP address: \" server echo \"Ready to copy your new public key to a remote server:\" ssh-copy-id username@server echo \"SSH key setup complete!\" Save the script to a file, make it executable with the following command: chmod +x create_ssh_script.sh run it with the following command: ./create_ssh_script.sh","title":"Start-up the Terraform Server"},{"location":"orchestration/cacao_terra/#running-terraform","text":"Step 2 Clone the CACAO z2jh repository git clone https://gitlab.com/stack0/cacao-tf-jupyterhub.git ** Step 3** Initialize Terraform in the repository Change directory to the repo: cd cacao-tf-jupyterhub Run terraform init Step 3 Source the OpenRC shell script source *-openrc.sh Step 4 Start Terraform terraform apply -auto-approve Step 5 Suspend Terraform Update your terraform.tfvars file by changing the power_state from \"active\" to \"shelved_offloaded\" ### Set Power Status ### power_state = \"active\" # power_state = \"shelved_offloaded\" re-run terraform apply -auto-approve to update the deployment Step 6 Destroying a Terraform deployment when done, with your cluster, run terraform destroy -auto-approve to take the cluster back down.","title":"Running Terraform"},{"location":"orchestration/cacao_terra/#editing-terraform-variables-file-tfvars","text":"Zero to JupyterHub with Kubernetes Deployment Example Terraform variables (.tfvars) file for deploying Zero to JupyterHub with K3s (Rancher) Based on the Zero to JupyterHub with Kubernetes ### Set the floating_ip address for DNS redirections ### # jupyterhub_floating_ip=\"149.165.152.223\" ### Set up the Security Groups (re=used from Horizon) ### security_groups = [\"default\",\"cacao-default\",\"cacao-k8s\"] ### Give the Deployment a Name (shorter is better) ### instance_name = \"jh-prod\" ### Horizon Account User ID and ACCESS-CI Project Account ### username = \"${USER}\" project = \"BIO220085\" # Horizon SSH key-pair keypair = \"${USER}-default\" ### Set the Number of Instances (Master + Workers) ### instance_count = 6 # Storage Volume Size in GiB jh_storage_size=1000 # Storage Volume directory path jh_storage_mount_dir=\"/home/shared\" # OS type image_name = \"Featured-Ubuntu22\" # Worker VM Flavor flavor = \"m3.medium\" # Master VM Flavor (m3.medium or larger is recommended) flavor_master = \"m3.medium\" ### Run JupyterHub on Master Node ### do_jh_singleuser_exclude_master=\"true\" # Jetstream2 Region region = \"IU\" # VM image (root disk) sizes root_storage_size = 100 root_storage_type = \"volume\" # Enable GPUs (true/false) - use \"true\" with \"g3\" flavors do_enable_gpu=false ### Zero 2 JupyterHub with K3s Rancher Configuration ### jupyterhub_deploy_strategy=\"z2jh_k3s\" ### Ansible-based Setup for JupyterHub ### do_ansible_execution=true ### CPU usage per user utilization 10000m = 100% of 1 core ### jh_resources_request_cpu=\"3000m\" ### Memory RAM per-user requirement in Gigibytes ### jh_resources_request_memory=\"10Gi\" ### Set Power Status ### power_state = \"active\" # power_state = \"shelved_offloaded\" ### Pre-Cache images? ### do_jh_precache_images=false user_data = \"\" ### JupyterHub Configurations ### # set the type of authentication, comment out Dummy or OAuth # OAuth-type (GitHub Authentication) # jupyterhub_oauth2_clientid=\"02b0a98c5d1547101514\" # jupyterhub_oauth2_secret=\"ac658055d848e787c31803afd973a05fbdb98f64\" # Dummy-type Authentication jupyterhub_authentication=\"dummy\" # set the dummy username password jupyterhub_dummy_password=\"password\" # Specify the JupyterHub admins jupyterhub_admins=\"${USER}\" # add JupyterHub student account usernames jupyterhub_allowed_users=\"${USER}, student1, student2, student3, student4\" ### Select a JupyterHub-ready image ### #jupyterhub_singleuser_image=\"harbor.cyverse.org/vice/jupyter/pytorch\" jupyterhub_singleuser_image=\"jupyter/datascience-notebook\" jupyterhub_singleuser_image_tag=\"latest\"","title":"Editing Terraform Variables file (.tfvars)"},{"location":"orchestration/k8s/","text":"Kubernetes , often abbreviated as K8s , is an open-source system for automating deployment, scaling, and management of containerized applications. K8s is the most widely used platform for managing containerized applications at scale in Cloud computing. Leveraging a K8s Cluster is likely if you adopt containers into your research portfolio. In these introductory lessons we will be discussing how to leverage existing Kubernetes Clusters using kubectl Why build your own K8s Cluster? Learning about containerized applications and container orchestration is why you're here afterall. We want to at least expose you to the existence of these technologies and to help you understand how they fit within the roadmap of Cloud Native Computing. With that said, Designing, deploying and running your own K8s cluster is very likely a bad idea Partly because there are so many other existing platforms out there, i.e., other managed K8s services, but also because the engineering and technological understanding of managing K8s requires a dedicated DevOps or software engineering team to keep your platform running. Still not deterred? Reasons why you may need to use K8s for your research: (1) Your applications consist of multiple services. K8s API automates the tasks of managing many containers and provision resources. (2) Your work scales dynamically - if you need more or less quantities of computing depending on workloads, K8s might be useful. The ability to use containers to scale your applications is easier than launching VMs by hand. (3) You have too many containers to keep track of - K8s is good at what it was made for, managing and keeping containers up and running. (4) Its all going to the cloud anyway. If your science domain is moving towards being cloud-native, you need to build your workflows for the time of migration. (5) Its consistent. K8s declarative state describes exactly how everything is managed. Kubernetes Terminology Kubernetes Cluster is a set of nodes that run containerized applications. kubectl is the command line interface for connecting to K8s clusters Pod is the smallest deployable unit of Kubernetes, can be one or more containers that talk to one another. Pods are run on Nodes Node : typically run on a virtual machine (VM), a Nodes components include kubelet , kube-proxy , and Container run-time Kubelet is the Node agent that manages the Pods on each VM Kube-proxy is used by a Service to run a network proxy on each Node Container runtime is the software responsible for running containers in the Pods . Example containers that K8s users are: Docker , containerd (native), and CRI-O . Service sets up a network address for an application, including a persistent IP address and a local DNS entry within the cluster . Service load balances across the Pods and can add or remove them. ReplicaSet is a convenient way to build multiple pods at once. Deployment provides declarative updates for Pods and ReplicaSets (rolling updates) Control Pane is a node that controls/manages worker nodes. The components of the Control Pane are the API server , the cluster store , the controller manager , and the scheduler . Introduction to K8s CLI with kubectl \u00b6 The Kubernetes API uses a command-line tool called kubectl Using K8s does not require you to own or maintain your own cluster. You can use the kubectl tool to connect to running clusters and start your containers. Install kubectl \u00b6 Official Documentation kubectl is already installed in CodeSpaces. To install on Linux: curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x kubectl sudo mv ./kubectl /usr/local/bin/kubectl kubeconfig \u00b6 K8s uses YAML files for configuring a cluster. The config file is required to make the handshake between the K8s Cluster and external requests, like the one you're making from your local computer or CodeSpace. config yaml \u00b6 To connect to a running K8s cluster, you need to create a config yaml and place it in the ~/.kube/ folder. example config file Example of the ~/.kube/config file which is used for K3s Copy the ~/.kube/config file from your cluster or use our example here over to your localhost or VM and put it into a temporary directory. Make a second copy and put it into your own ~/.kube/config folder By default, the config file is in the ~/.kube/ directory, but it can be put anywhere or given any name, use the --kubeconfig flag to tell kubectl where to get the config: kubectl --kubeconfig /custom/path/kube.config get pods Setting up the namespace \u00b6 kubectl needs to know what the namespace is of of the Cluster config you're working with. In the first example today, we're going to be working with a cluster with a pre-set namespace To set the context and namespace of the cluster on your localhost: kubectl config set-context cc2022 --namespace=containercamp2022 Once the config is set, you should be ready to launch \"pods\" i.e. containers on the cluster pods \u00b6 We are going to launch pods (w/ containers) on the same K3s cluster which is already running in JS-2. Create a new file in your VM or localhost called pod1.yaml apiVersion: v1 kind: Pod metadata: name: pod-<change-this-to-your-username> spec: containers: - name: mypod image: alpine:3.14 resources: limits: memory: 100Mi cpu: 100m requests: memory: 100Mi cpu: 100m command: [\"sh\", \"-c\", \"sleep infinity\"] Launching a Pod \u00b6 Try launching a pod on the cluster kubectl create -f pod1.yaml Check to see that it is running kubectl get pods Do you see any other pods? If the pod hasn't started yet, you can check the timestamp to see if it was created: kubectl get events --sort-by=.metadata.creationTimestamp Try to connect to your running pod (container) kubectl exec -it pod-<yourusername> -- /bin/sh Pod networking \u00b6 Let's check the networking inside the pod # uncomment if ifconfig is not installed # apk add net-tools ifconfig -a Exit the pod ( ctrl ^ D ) Check the IP with kubectl kubectl get pod -o wide pod-<yourname> Taking down a Pod \u00b6 Once you've exited the pod, delete it kubectl delete -f pod1.yaml double check that its gone kubectl get pods Create a Deployment \u00b6 While we can create and delete pods on our own, what we realy want is to make our containers have \"high availability\". High availiability means that when a node dies or is restarted, the pod will \"come back up\" on its own. Run a Dashboard \u00b6 UpCloud instructions Once the cluster is up and running, you may choose to create a Dashboard for it The Dashboard can only run from a localhost so we have to do an ssh tunnel to connect to it ssh -L localhost:8001:127.0.0.1:8001 <user>@<master_public_IP> start the dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml check to see the dashboard pods are running kubectl get pods -A create an admin dashboard user \u00b6 mkdir ~/dashboard && cd ~/dashboard create a dashboard-admin.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard deploy kubectl apply -f dashboard-admin.yaml print the admin token so you can log into the dashboard kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode create a read-only dashboard-read-only.yaml user \u00b6 apiVersion: v1 kind: ServiceAccount metadata: name: read-only-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: name: read-only-clusterrole namespace: default rules: - apiGroups: - \"\" resources: [\"*\"] verbs: - get - list - watch - apiGroups: - extensions resources: [\"*\"] verbs: - get - list - watch - apiGroups: - apps resources: [\"*\"] verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-only-binding roleRef: kind: ClusterRole name: read-only-clusterrole apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: read-only-user namespace: kubernetes-dashboard deploy it kubectl apply -f dashboard-read-only.yaml print read-only token kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode Start the dashboard \u00b6 kubectl proxy http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ Copy/Paste your admin or read-only token delete the dashboard \u00b6 kubectl delete -f dashboard-admin.yaml kubectl delete -f dashboard-read-only.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml","title":"Introduction to Kubernetes"},{"location":"orchestration/k8s/#introduction-to-k8s-cli-with-kubectl","text":"The Kubernetes API uses a command-line tool called kubectl Using K8s does not require you to own or maintain your own cluster. You can use the kubectl tool to connect to running clusters and start your containers.","title":"Introduction to K8s CLI with kubectl"},{"location":"orchestration/k8s/#install-kubectl","text":"Official Documentation kubectl is already installed in CodeSpaces. To install on Linux: curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl chmod +x kubectl sudo mv ./kubectl /usr/local/bin/kubectl","title":"Install kubectl"},{"location":"orchestration/k8s/#kubeconfig","text":"K8s uses YAML files for configuring a cluster. The config file is required to make the handshake between the K8s Cluster and external requests, like the one you're making from your local computer or CodeSpace.","title":" kubeconfig"},{"location":"orchestration/k8s/#config-yaml","text":"To connect to a running K8s cluster, you need to create a config yaml and place it in the ~/.kube/ folder. example config file Example of the ~/.kube/config file which is used for K3s Copy the ~/.kube/config file from your cluster or use our example here over to your localhost or VM and put it into a temporary directory. Make a second copy and put it into your own ~/.kube/config folder By default, the config file is in the ~/.kube/ directory, but it can be put anywhere or given any name, use the --kubeconfig flag to tell kubectl where to get the config: kubectl --kubeconfig /custom/path/kube.config get pods","title":"config yaml"},{"location":"orchestration/k8s/#setting-up-the-namespace","text":"kubectl needs to know what the namespace is of of the Cluster config you're working with. In the first example today, we're going to be working with a cluster with a pre-set namespace To set the context and namespace of the cluster on your localhost: kubectl config set-context cc2022 --namespace=containercamp2022 Once the config is set, you should be ready to launch \"pods\" i.e. containers on the cluster","title":"Setting up the namespace"},{"location":"orchestration/k8s/#pods","text":"We are going to launch pods (w/ containers) on the same K3s cluster which is already running in JS-2. Create a new file in your VM or localhost called pod1.yaml apiVersion: v1 kind: Pod metadata: name: pod-<change-this-to-your-username> spec: containers: - name: mypod image: alpine:3.14 resources: limits: memory: 100Mi cpu: 100m requests: memory: 100Mi cpu: 100m command: [\"sh\", \"-c\", \"sleep infinity\"]","title":" pods"},{"location":"orchestration/k8s/#launching-a-pod","text":"Try launching a pod on the cluster kubectl create -f pod1.yaml Check to see that it is running kubectl get pods Do you see any other pods? If the pod hasn't started yet, you can check the timestamp to see if it was created: kubectl get events --sort-by=.metadata.creationTimestamp Try to connect to your running pod (container) kubectl exec -it pod-<yourusername> -- /bin/sh","title":"Launching a Pod"},{"location":"orchestration/k8s/#pod-networking","text":"Let's check the networking inside the pod # uncomment if ifconfig is not installed # apk add net-tools ifconfig -a Exit the pod ( ctrl ^ D ) Check the IP with kubectl kubectl get pod -o wide pod-<yourname>","title":"Pod networking"},{"location":"orchestration/k8s/#taking-down-a-pod","text":"Once you've exited the pod, delete it kubectl delete -f pod1.yaml double check that its gone kubectl get pods","title":"Taking down a Pod"},{"location":"orchestration/k8s/#create-a-deployment","text":"While we can create and delete pods on our own, what we realy want is to make our containers have \"high availability\". High availiability means that when a node dies or is restarted, the pod will \"come back up\" on its own.","title":"Create a Deployment"},{"location":"orchestration/k8s/#run-a-dashboard","text":"UpCloud instructions Once the cluster is up and running, you may choose to create a Dashboard for it The Dashboard can only run from a localhost so we have to do an ssh tunnel to connect to it ssh -L localhost:8001:127.0.0.1:8001 <user>@<master_public_IP> start the dashboard kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml check to see the dashboard pods are running kubectl get pods -A","title":"Run a Dashboard"},{"location":"orchestration/k8s/#create-an-admin-dashboard-user","text":"mkdir ~/dashboard && cd ~/dashboard create a dashboard-admin.yaml apiVersion: v1 kind: ServiceAccount metadata: name: admin-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: admin-user roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: admin-user namespace: kubernetes-dashboard deploy kubectl apply -f dashboard-admin.yaml print the admin token so you can log into the dashboard kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode","title":"create an admin dashboard user"},{"location":"orchestration/k8s/#create-a-read-only-dashboard-read-onlyyaml-user","text":"apiVersion: v1 kind: ServiceAccount metadata: name: read-only-user namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: name: read-only-clusterrole namespace: default rules: - apiGroups: - \"\" resources: [\"*\"] verbs: - get - list - watch - apiGroups: - extensions resources: [\"*\"] verbs: - get - list - watch - apiGroups: - apps resources: [\"*\"] verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: read-only-binding roleRef: kind: ClusterRole name: read-only-clusterrole apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: read-only-user namespace: kubernetes-dashboard deploy it kubectl apply -f dashboard-read-only.yaml print read-only token kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount read-only-user -n kubernetes-dashboard -o jsonpath=\"{.secrets[0].name}\") -o jsonpath=\"{.data.token}\" | base64 --decode","title":"create a read-only dashboard-read-only.yaml user"},{"location":"orchestration/k8s/#start-the-dashboard","text":"kubectl proxy http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/ Copy/Paste your admin or read-only token","title":"Start the dashboard"},{"location":"orchestration/k8s/#delete-the-dashboard","text":"kubectl delete -f dashboard-admin.yaml kubectl delete -f dashboard-read-only.yaml kubectl delete -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml","title":"delete the dashboard"},{"location":"orchestration/registry/","text":"If you maintain your own cyberinfrastructure and are pulling many containers per day, you may consider hosting your own registry. Docker Registry Server \u00b6 Setting up your own Docker Registry Server Registry deployment documentation Harbor \u00b6 Harbor is for managing container registries with Kubernetes CyVerse manages their own public/private Harbor server for their Discovery Environment. You can authenticate to it using your CyVerse user account. https://harbor.cyverse.org CyVerse featured Docker containers in the Discovery Environment are cached on our Harbor, using a combination of GitHub Actions. https://github.com/cyverse-vice","title":"Manage private container registry"},{"location":"orchestration/registry/#docker-registry-server","text":"Setting up your own Docker Registry Server Registry deployment documentation","title":" Docker Registry Server"},{"location":"orchestration/registry/#harbor","text":"Harbor is for managing container registries with Kubernetes CyVerse manages their own public/private Harbor server for their Discovery Environment. You can authenticate to it using your CyVerse user account. https://harbor.cyverse.org CyVerse featured Docker containers in the Discovery Environment are cached on our Harbor, using a combination of GitHub Actions. https://github.com/cyverse-vice","title":" Harbor"},{"location":"orchestration/terra/","text":"Introduction to Terraform \u00b6 Terraform is an open-source infrastructure-as-code (IaC) software tool created by HashiCorp . What is Infrastructure-as-Code (IaC)? \"Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.\" - Wikipedia IaC tools allow you to manage infrastructure with configuration files rather than through a graphical user interface. IaC allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share. -- Terraform Documentation Overview \u00b6 This basic tutorial will guide you through setting up a Terraform project and deploying virtual machines (VMs) as infrastructure on OpenStack Cloud. Goals Understand orchestration for deployment to OpenStack cloud (Jetstream2) Understand the benefits of Terraform Ability to perform basic deployments on OpenStack using Terraform Ability to perform provisioning of deployed OpenStack resources through Terraform Things we won't cover OpenStack API All of Terraform's features Prerequisites \u00b6 Basic understanding of OpenStack and VMs Access to an OpenStack cloud (we will use Jetstream2 ) Terraform installed on your local machine Optional: request your own allocation to Jetstream2 on NSF ACCESS-CI Outcomes By the end of this tutorial, you will have created SSH keypair generated an *-openrc.sh file from OpenStack started, stopped, and destroyed a Terraform deployment on an OpenStack Cloud Terminology Ansible - is a suite of software tools that enables infrastructure as code Deploy - to create a cloud resource or software Infrastructure - is the collection of hardware and software elements such as computing power, networking, storage, and virtualization resources needed to enable cloud computing Orchestration - is the automated configuring, coordinating, and managing of computer systems and software Playbook - are a list of tasks that automatically execute against a host Provision - making changes to a VM including updating the operating system, installing software, adding configurations Terraform - is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently Getting onto OpenStack Cloud \u00b6 What is OpenStack? OpenStack is an open source cloud computing infrastructure software project and is one of the three most active open source projects in the world. OpenStack clouds are managed by individuals and institutions on their own bare-metal hardware. If you do not have an account, go to https://allocations.access-ci.org/ and begin the process by requesting an \"Explore\" start-up allocation. ACCESS is the NSF's management layer for their research computing network (formerly called TerraGrid and XSEDE) which includes high performance computing, high throughput computing, and research clouds like Jetstream2. Jetstream2 is a public research cloud which uses OpenStack as its management layer. CyVerse is developing a User Interface for Jetstream2 called CACAO (Cloud Automation & Continuous Analysis Orchestration) . Beneath its hood is Terraform. CACAO can also be used from the CLI (which we will show in a later lesson ). Terraform installation \u00b6 Official Documentation Windows Installation Download Terraform using the appropriate distribution for your OS Mac OS X Installation Instructions for Mac OS X installation If you're on OS X, you can use brew to install with the following commands: # install terraform -- taken from https://learn.hashicorp.com/tutorials/terraform/install-cli brew tap hashicorp/tap && brew install hashicorp/tap/terraform # install ansible and jq (for processing terraform's statefile into an ansible inventory) brew install ansible jq Linux Installation Instructions for Ubuntu 22.04 installation wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update && sudo apt install terraform Install Ansible & J Query sudo apt-add-repository ppa:ansible/ansible sudo apt update & sudo apt install -y ansible jq Confirm installation: terraform Should output: Usage: terraform [ global options ] <subcommand> [ args ] The available commands for execution are listed below. The primary workflow commands are given first, followed by less common or more advanced commands. Main commands: init Prepare your working directory for other commands validate Check whether the configuration is valid plan Show changes required by the current configuration apply Create or update infrastructure destroy Destroy previously-created infrastructure All other commands: console Try Terraform expressions at an interactive command prompt fmt Reformat your configuration in the standard style force-unlock Release a stuck lock on the current workspace get Install or upgrade remote Terraform modules graph Generate a Graphviz graph of the steps in an operation import Associate existing infrastructure with a Terraform resource login Obtain and save credentials for a remote host logout Remove locally-stored credentials for a remote host metadata Metadata related commands output Show output values from your root module providers Show the providers required for this configuration refresh Update the state to match remote systems show Show the current state or a saved plan state Advanced state management taint Mark a resource instance as not fully functional test Experimental support for module integration testing untaint Remove the 'tainted' state from a resource instance version Show the current Terraform version workspace Workspace management Global options ( use these before the subcommand, if any ) : -chdir = DIR Switch to a different working directory before executing the given subcommand. -help Show this help output, or the help for a specified subcommand. -version An alias for the \"version\" subcommand. Generate an OpenStack Credential for Terraform \u00b6 Log into OpenStack's Horizon Interface Step 1 Log into OpenStack's Horizon Interface and create application credentials Generate an openrc.sh file in Jetstream2 Horizon Interface ( https://js2.jetstream-cloud.org ), Select the \"Identity\" then \"Application Credentials\" option in the menu (left side) Select \"+ Create Application Credential\" button on right Give your new credentials a name and description, leave most of the fields blank Download the new crededential openrc.sh file to your local Important Do not close the Application Credentials window without copying the secret or downloading the openrc.sh file. Create an SSH keypair with OpenStack \u00b6 Creating a SSH key To create an SSH key on an Ubuntu 22.04 terminal, you can follow these steps: Step 1: Open your terminal and type the following command to generate a new SSH key pair: ssh-keygen -t rsa -b 4096 Step 2: When prompted, press \"Enter\" to save the key in the default location, or enter a different filename and location to save the key. Enter a passphrase to secure your key. This passphrase will be required to use the key later. Once the key is generated, you can add it to your SSH agent by running the following command: eval \" $( ssh-agent -s ) \" ssh-add ~/.ssh/id_rsa Step 3: Copy the public key to your remote server by running the following command, replacing \"user\" and \"server\" with your username and server address: ssh-copy-id <user>@<server-ip> create_ssh_script.sh: #!/bin/bash echo \"Generating SSH key pair...\" ssh-keygen -t rsa -b 4096 echo \"Adding key to SSH agent...\" eval \" $( ssh-agent -s ) \" ssh-add ~/.ssh/id_rsa read -p \"Enter your remote server username: \" username read -p \"Enter your remote server IP address: \" server echo \"Ready to copy your new public key to a remote server:\" ssh-copy-id username@server echo \"SSH key setup complete!\" Save the script to a file, make it executable with the following command: chmod +x create_ssh_script.sh run it with the following command: ./create_ssh_script.sh Check to make sure you have a public key in the ~/.ssh/ directory, it should have the extension .pub ls ~/.ssh/ Create the keypair to OpenStack openstack keypair create --public-key ~/.ssh/id_rsa.pub tswetnam-terraform-key You can now check in OpenStack for the new keypair here: https://js2.jetstream-cloud.org/project/key_pairs Initialize your Terraform project \u00b6 Create a new project folder for our configuration files. mkdir ~/terraform Copy the openrc.sh file you downloaded from OpenStack into the new folder. cp *-openrc.sh ~/terraform/ Change Directory into your new terraform/ folder and source the openrc.sh file to create its environmental variables locally. By sourcing this file, you avoid placing sensitive information about yourself into your code. source *-openrc.sh Configuration files \u00b6 Terraform code is written in HCL (Hashicorp Configuration Language), and its configuration files typically end in the .tf file extension. Configuration .tf files can either be split into multiple files or maintained in a single file. When using multiple files, it is up to your discretion what the file names are, or how many you decide to split it into. File Organization \u00b6 An example for file organization of a terraform project might involve: terraform-project / \u251c\u2500\u2500 main . tf \u251c\u2500\u2500 variables . tf \u251c\u2500\u2500 outputs . tf \u251c\u2500\u2500 instances . tf \u251c\u2500\u2500 security . tf \u2514\u2500\u2500 modules / \u251c\u2500\u2500 network / \u2502 \u251c\u2500\u2500 main . tf \u2502 \u251c\u2500\u2500 variables . tf \u2502 \u2514\u2500\u2500 outputs . tf \u2514\u2500\u2500 compute / \u251c\u2500\u2500 main . tf \u251c\u2500\u2500 variables . tf \u2514\u2500\u2500 outputs . tf Main Configuration File ( main.tf ): - contains the primary infrastructure resources and configurations for virtual machines, networks, and storage. Security File ( security.tf ): - contains optional security group details for instances. Variables File ( variables.tf ): - defines all the input variables used in the configuration. Declare variables with default values or leave them empty for required user input. Include descriptions for each variable to provide context. Instances File ( instances.tf ): - provisions the Instance flavor and IP networking of the VMs Outputs File ( outputs.tf ): - defines the outputs Terraform displays after applying the Main and Variables configuration. Includes: IP addresses, DNS names, or information for resources. Other optional files \u00b6 Provider Configuration File ( provider.tf ): - includes the provider(s) used in the configuration, such as OpenStack (on commerical cloud: Amazon Web Services (AWS), Azure, or Google Cloud Platform(GCP)) along with their authentication and regional settings. Modules and Reusable Configurations: - create separate .tf files for reusable modules and configurations. Reuse across multiple projects or within the same project on multiple VMs. Terraform configuration files. \u00b6 Create the main.tf file in the ~/terraform/ directory main.tf \u00b6 terraform { required_version = \">= 0.14.0\" required_providers { openstack = { source = \"terraform-provider-openstack/openstack\" version = \">=1.47.0\" } } } provider \"openstack\" { auth_url = \"https://js2.jetstream-cloud.org:5000/v3/\" region = \"IU\" } The main.tf file has just the basics for calling out to an OpenStack provider - we created the necessary configurations for this in the prior steps by sourcing the *-openrc.sh file and running terraform init . variables.tf \u00b6 Create variables.tf , it can also be called inputs.tf Here you need to go back to OpenStack and get a couple of additional variables: vm_number - defines the number of VMs you wish to launch public_key - you need the name of your paired SSH key that you generated in the prior step image_name - select the name of a featured or unique image you wish to launch. variable \"vm_number\" { # creates a single VM # replace with a larger number to launch more than one VM default = \"1\" } variable \"public_key\" { # replace this with the name of the public ssh key you uploaded to Jetstream 2 # https://docs.jetstream-cloud.org/ui/cli/managing-ssh-keys/ default = \"tswetnam-terraform-key\" } variable \"image_name\" { # replace this with the image name of the ubuntu iso you want to use # https://js2.jetstream-cloud.org/project/images default = \"Featured-Ubuntu20\" } variable \"network_name\" { # replace this with the id of the public interface on JS2 in Project / Network / Networks / public # https://js2.jetstream-cloud.org/project/networks/ default = \"auto_allocated_network\" } security.tf \u00b6 This file produces two distinct security groups, terraform_ssh_ping creates a group with ssh access and ping , terraform_tcp_1 creates a group which opens the HTTP (80, 8080), HTTPS (443) ports for connecting a browser based service. ################ #Security section ################ # Creating Compute Security group resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" { name = \"terraform_ssh_ping\" description = \"Security group with SSH and PING open to 0.0.0.0/0\" #ssh rule rule { ip_protocol = \"tcp\" from_port = \"22\" to_port = \"22\" cidr = \"0.0.0.0/0\" } rule { from_port = -1 to_port = -1 ip_protocol = \"icmp\" cidr = \"0.0.0.0/0\" } } # Create a Netowrking Security group resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" { name = \"terraform_tcp_1\" description = \"Security group with TCP open to 0.0.0.0/0\" } # Allow HTTP (port 80) traffic resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { direction = \"ingress\" ethertype = \"IPv4\" protocol = \"tcp\" port_range_min = 80 port_range_max = 80 remote_ip_prefix = \"0.0.0.0/0\" security_group_id = \" ${ openstack_networking_secgroup_v2 .terraform_tcp_1.id } \" } # Allow HTTPS (port 443) traffic resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" { direction = \"ingress\" ethertype = \"IPv4\" protocol = \"tcp\" port_range_min = 443 port_range_max = 443 remote_ip_prefix = \"0.0.0.0/0\" security_group_id = \" ${ openstack_networking_secgroup_v2 .terraform_tcp_1.id } \" } # Allow Service (port 8080) traffic resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" { direction = \"ingress\" ethertype = \"IPv4\" protocol = \"tcp\" port_range_min = 8080 port_range_max = 8080 remote_ip_prefix = \"0.0.0.0/0\" security_group_id = \" ${ openstack_networking_secgroup_v2 .terraform_tcp_1.id } \" } instances.tf \u00b6 ################ # Instance OS ################ # create each Ubuntu20 instance resource \"openstack_compute_instance_v2\" \"Ubuntu20\" { name = \"container_camp_Ubuntu20_ ${ count .index } \" # ID of Featured-Ubuntu20 image_name = var.image_name # flavor_id is the size of the VM # https://docs.jetstream-cloud.org/general/vmsizes/ flavor_name = \"m3.tiny\" # this public key is set above in security section key_pair = var.public_key security_groups = [ \"terraform_ssh_ping\" , \"default\" ] count = var.vm_number metadata = { terraform_controlled = \"yes\" } network { name = var.network_name } #depends_on = [openstack_networking_network_v2.terraform_network] } # creating floating ips from the public ip pool resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" { pool = \"public\" count = var.vm_number } # assign floating ip to each Ubuntu20 VM resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" { floating_ip = \" ${ openstack_networking_floatingip_v2 .terraform_floatip_ubuntu20[count.index].address } \" instance_id = \" ${ openstack_compute_instance_v2 .Ubuntu20[count.index].id } \" count = var.vm_number } output.tf \u00b6 ################ #Output ################ output \"floating_ip_ubuntu20\" { value = openstack_networking_floatingip_v2.terraform_floatip_ubuntu20.*.address description = \"Public IP for Ubuntu 20\" } terraform.tfvars \u00b6 A terraform.tfvars file is used to define the values of input variables. It can also be renamed *.auto.tfvars . It serves as a convenient way to store and manage variable values that you don't want to hardcode in your .tf files or provide via command-line arguments. By using a terraform.tfvars file, you can easily customize and update the variable values for different environments or scenarios. The file should contain key-value pairs, where the key is the variable name and the value is the corresponding variable value. The syntax for defining variables in the terraform.tfvars file can be either HCL or JSON. Add your OpenStack credentials and other required information to terraform.tfvars : In HCL: openstack_user_name = \"your-openstack-username\" openstack_password = \"your-openstack-password\" openstack_tenant_name = \"your-openstack-tenant-name\" openstack_auth_url = \"your-openstack-auth-url\" In JSON: { \"openstack_user_name\" : \"your-openstack-username\" , \"openstack_password\" : \"your-openstack-password\" , \"openstack_tenant_name\" : \"your-openstack-tenant-name\" , \"openstack_auth_url\" : \"your-openstack-auth-url\" } When you run terraform apply or terraform plan , Terraform will automatically load the values from the terraform.tfvars file if it exists in the working directory. You can also create multiple .tfvars files and specify which one to use by passing the -var-file flag when executing Terraform commands: terraform apply -var-file = \"custom.tfvars\" Intermediate directories and files \u00b6 When terraform plan and terraform apply are executed, Terraform generates some new project files, notably .terraform/ - this directory will contain the terraform-provider-openstack_version and a README.md , LICENCE , and CHANGELOG.md terraform.lock.hcl terraform.tfstate terraform.tfstate.backup Terraform Commands \u00b6 init \u00b6 Now, you are ready to initialize the Terraform project cd ~/terraform terraform init Expected Response Initializing the backend... Initializing provider plugins... - Reusing previous version of terraform-provider-openstack/openstack from the dependency lock file - Using previously-installed terraform-provider-openstack/openstack v1.51.1 Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. plan \u00b6 plan - generates the configuration based on the .tf files in the initialized directory terraform plan Expected Response Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0] will be created + resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" { + floating_ip = ( known after apply ) + id = ( known after apply ) + instance_id = ( known after apply ) + region = ( known after apply ) } # openstack_compute_instance_v2.Ubuntu20[0] will be created + resource \"openstack_compute_instance_v2\" \"Ubuntu20\" { + access_ip_v4 = ( known after apply ) + access_ip_v6 = ( known after apply ) + all_metadata = ( known after apply ) + all_tags = ( known after apply ) + availability_zone = ( known after apply ) + created = ( known after apply ) + flavor_id = ( known after apply ) + flavor_name = \"m3.tiny\" + force_delete = false + id = ( known after apply ) + image_id = ( known after apply ) + image_name = \"Featured-Ubuntu20\" + key_pair = \"tswetnam-terraform-key\" + metadata = { + \"terraform_controlled\" = \"yes\" } + name = \"container_camp_Ubuntu20_0\" + power_state = \"active\" + region = ( known after apply ) + security_groups = [ + \"default\" , + \"terraform_ssh_ping\" , ] + stop_before_destroy = false + updated = ( known after apply ) + network { + access_network = false + fixed_ip_v4 = ( known after apply ) + fixed_ip_v6 = ( known after apply ) + floating_ip = ( known after apply ) + mac = ( known after apply ) + name = \"auto_allocated_network\" + port = ( known after apply ) + uuid = ( known after apply ) } } # openstack_compute_secgroup_v2.terraform_ssh_ping will be created + resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" { + description = \"Security group with SSH and PING open to 0.0.0.0/0\" + id = ( known after apply ) + name = \"terraform_ssh_ping\" + region = ( known after apply ) + rule { + cidr = \"0.0.0.0/0\" + from_port = -1 + id = ( known after apply ) + ip_protocol = \"icmp\" + self = false + to_port = -1 } + rule { + cidr = \"0.0.0.0/0\" + from_port = 22 + id = ( known after apply ) + ip_protocol = \"tcp\" + self = false + to_port = 22 } } # openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0] will be created + resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" { + address = ( known after apply ) + all_tags = ( known after apply ) + dns_domain = ( known after apply ) + dns_name = ( known after apply ) + fixed_ip = ( known after apply ) + id = ( known after apply ) + pool = \"public\" + port_id = ( known after apply ) + region = ( known after apply ) + subnet_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.http_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 80 + port_range_min = 80 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.https_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 443 + port_range_min = 443 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.service_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 8080 + port_range_min = 8080 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_v2.terraform_tcp_1 will be created + resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" { + all_tags = ( known after apply ) + description = \"Security group with TCP open to 0.0.0.0/0\" + id = ( known after apply ) + name = \"terraform_tcp_1\" + region = ( known after apply ) + tenant_id = ( known after apply ) } Plan: 8 to add, 0 to change, 0 to destroy. Changes to Outputs: + floating_ip_ubuntu20 = [ + null, ] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn 't use the -out option to save this plan, so Terraform can' t guarantee to take exactly these actions if you run \"terraform apply\" now. apply \u00b6 terraform apply Expected response Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0] will be created + resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" { + floating_ip = ( known after apply ) + id = ( known after apply ) + instance_id = ( known after apply ) + region = ( known after apply ) } # openstack_compute_instance_v2.Ubuntu20[0] will be created + resource \"openstack_compute_instance_v2\" \"Ubuntu20\" { + access_ip_v4 = ( known after apply ) + access_ip_v6 = ( known after apply ) + all_metadata = ( known after apply ) + all_tags = ( known after apply ) + availability_zone = ( known after apply ) + created = ( known after apply ) + flavor_id = ( known after apply ) + flavor_name = \"m3.tiny\" + force_delete = false + id = ( known after apply ) + image_id = ( known after apply ) + image_name = \"Featured-Ubuntu20\" + key_pair = \"tswetnam-terraform-key\" + metadata = { + \"terraform_controlled\" = \"yes\" } + name = \"container_camp_Ubuntu20_0\" + power_state = \"active\" + region = ( known after apply ) + security_groups = [ + \"default\" , + \"terraform_ssh_ping\" , ] + stop_before_destroy = false + updated = ( known after apply ) + network { + access_network = false + fixed_ip_v4 = ( known after apply ) + fixed_ip_v6 = ( known after apply ) + floating_ip = ( known after apply ) + mac = ( known after apply ) + name = \"auto_allocated_network\" + port = ( known after apply ) + uuid = ( known after apply ) } } # openstack_compute_secgroup_v2.terraform_ssh_ping will be created + resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" { + description = \"Security group with SSH and PING open to 0.0.0.0/0\" + id = ( known after apply ) + name = \"terraform_ssh_ping\" + region = ( known after apply ) + rule { + cidr = \"0.0.0.0/0\" + from_port = -1 + id = ( known after apply ) + ip_protocol = \"icmp\" + self = false + to_port = -1 } + rule { + cidr = \"0.0.0.0/0\" + from_port = 22 + id = ( known after apply ) + ip_protocol = \"tcp\" + self = false + to_port = 22 } } # openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0] will be created + resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" { + address = ( known after apply ) + all_tags = ( known after apply ) + dns_domain = ( known after apply ) + dns_name = ( known after apply ) + fixed_ip = ( known after apply ) + id = ( known after apply ) + pool = \"public\" + port_id = ( known after apply ) + region = ( known after apply ) + subnet_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.http_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 80 + port_range_min = 80 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.https_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 443 + port_range_min = 443 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.service_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 8080 + port_range_min = 8080 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_v2.terraform_tcp_1 will be created + resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" { + all_tags = ( known after apply ) + description = \"Security group with TCP open to 0.0.0.0/0\" + id = ( known after apply ) + name = \"terraform_tcp_1\" + region = ( known after apply ) + tenant_id = ( known after apply ) } Plan: 8 to add, 0 to change, 0 to destroy. Changes to Outputs: + floating_ip_ubuntu20 = [ + null, ] Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. You should be prompted Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Expected Response after choosing 'yes' openstack_networking_secgroup_v2.terraform_tcp_1: Creating... openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Creating... openstack_compute_secgroup_v2.terraform_ssh_ping: Creating... openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Creating... openstack_networking_secgroup_v2.terraform_tcp_1: Creation complete after 1s [ id = 4f0ab1d5-ca29-4f60-9a65-c38d2380719c ] openstack_networking_secgroup_rule_v2.service_rule: Creating... openstack_networking_secgroup_rule_v2.https_rule: Creating... openstack_networking_secgroup_rule_v2.http_rule: Creating... openstack_networking_secgroup_rule_v2.http_rule: Creation complete after 1s [ id = 2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41 ] openstack_networking_secgroup_rule_v2.service_rule: Creation complete after 2s [ id = ead48696-4aae-436f-a12b-8bf3ceac253a ] openstack_networking_secgroup_rule_v2.https_rule: Creation complete after 2s [ id = 20225b99-fd2a-4a05-912c-61056fa21e3b ] openstack_compute_secgroup_v2.terraform_ssh_ping: Creation complete after 4s [ id = 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Creation complete after 7s [ id = 6af630bc-9c16-450c-b16f-0483294d8d75 ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Still creating... [ 10s elapsed ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Creation complete after 15s [ id = afc53214-d2bd-45d5-b8b5-b6886976df8c ] openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Creating... openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Creation complete after 2s [ id = 149 .165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/ ] Apply complete! Resources: 8 added, 0 changed, 0 destroyed. Outputs: floating_ip_ubuntu20 = [ \"149.165.168.217\" , ] Congratulations! You now have a running VM, you can check its status on Horizon, or try to connect to it over ssh The floating_ip_ubuntu20 should be the new VM's IP address (yours will be different than this example). ssh ubuntu@<IP-ADDRESS> Make sure that destroy \u00b6 When you are ready to complete the use of your VMs or your deployment, you can destroy the project. terraform destroy Expected Response openstack_compute_secgroup_v2.terraform_ssh_ping: Refreshing state... [ id = 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Refreshing state... [ id = afc53214-d2bd-45d5-b8b5-b6886976df8c ] openstack_networking_secgroup_v2.terraform_tcp_1: Refreshing state... [ id = 4f0ab1d5-ca29-4f60-9a65-c38d2380719c ] openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Refreshing state... [ id = 6af630bc-9c16-450c-b16f-0483294d8d75 ] openstack_networking_secgroup_rule_v2.service_rule: Refreshing state... [ id = ead48696-4aae-436f-a12b-8bf3ceac253a ] openstack_networking_secgroup_rule_v2.https_rule: Refreshing state... [ id = 20225b99-fd2a-4a05-912c-61056fa21e3b ] openstack_networking_secgroup_rule_v2.http_rule: Refreshing state... [ id = 2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41 ] openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Refreshing state... [ id = 149 .165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/ ] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0] will be destroyed - resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" { - floating_ip = \"149.165.168.217\" -> null - id = \"149.165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/\" -> null - instance_id = \"afc53214-d2bd-45d5-b8b5-b6886976df8c\" -> null - region = \"IU\" -> null } # openstack_compute_instance_v2.Ubuntu20[0] will be destroyed - resource \"openstack_compute_instance_v2\" \"Ubuntu20\" { - access_ip_v4 = \"10.0.24.205\" -> null - all_metadata = { - \"terraform_controlled\" = \"yes\" } -> null - all_tags = [] -> null - availability_zone = \"nova\" -> null - created = \"2023-03-23 23:09:42 +0000 UTC\" -> null - flavor_id = \"1\" -> null - flavor_name = \"m3.tiny\" -> null - force_delete = false -> null - id = \"afc53214-d2bd-45d5-b8b5-b6886976df8c\" -> null - image_id = \"27424df1-c3ea-4c4a-ad8b-6ea9a476f6f8\" -> null - image_name = \"Featured-Ubuntu20\" -> null - key_pair = \"tswetnam-terraform-key\" -> null - metadata = { - \"terraform_controlled\" = \"yes\" } -> null - name = \"container_camp_Ubuntu20_0\" -> null - power_state = \"active\" -> null - region = \"IU\" -> null - security_groups = [ - \"default\" , - \"terraform_ssh_ping\" , ] -> null - stop_before_destroy = false -> null - tags = [] -> null - updated = \"2023-03-23 23:09:52 +0000 UTC\" -> null - network { - access_network = false -> null - fixed_ip_v4 = \"10.0.24.205\" -> null - mac = \"fa:16:3e:16:6e:8f\" -> null - name = \"auto_allocated_network\" -> null - uuid = \"e4ff98ff-d29a-48f4-a95b-88ebd6b0662f\" -> null } } # openstack_compute_secgroup_v2.terraform_ssh_ping will be destroyed - resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" { - description = \"Security group with SSH and PING open to 0.0.0.0/0\" -> null - id = \"1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f\" -> null - name = \"terraform_ssh_ping\" -> null - region = \"IU\" -> null - rule { - cidr = \"0.0.0.0/0\" -> null - from_port = -1 -> null - id = \"611e1c0a-edc1-462d-8e1a-5f8a72c4d968\" -> null - ip_protocol = \"icmp\" -> null - self = false -> null - to_port = -1 -> null } - rule { - cidr = \"0.0.0.0/0\" -> null - from_port = 22 -> null - id = \"ce99a035-5df2-4131-ba90-268d045d0ff3\" -> null - ip_protocol = \"tcp\" -> null - self = false -> null - to_port = 22 -> null } } # openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0] will be destroyed - resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" { - address = \"149.165.168.217\" -> null - all_tags = [] -> null - fixed_ip = \"10.0.24.205\" -> null - id = \"6af630bc-9c16-450c-b16f-0483294d8d75\" -> null - pool = \"public\" -> null - port_id = \"71e9ae50-e281-4f98-afab-f1b6c1932806\" -> null - region = \"IU\" -> null - tags = [] -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } # openstack_networking_secgroup_rule_v2.http_rule will be destroyed - resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { - direction = \"ingress\" -> null - ethertype = \"IPv4\" -> null - id = \"2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41\" -> null - port_range_max = 80 -> null - port_range_min = 80 -> null - protocol = \"tcp\" -> null - region = \"IU\" -> null - remote_ip_prefix = \"0.0.0.0/0\" -> null - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } # openstack_networking_secgroup_rule_v2.https_rule will be destroyed - resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" { - direction = \"ingress\" -> null - ethertype = \"IPv4\" -> null - id = \"20225b99-fd2a-4a05-912c-61056fa21e3b\" -> null - port_range_max = 443 -> null - port_range_min = 443 -> null - protocol = \"tcp\" -> null - region = \"IU\" -> null - remote_ip_prefix = \"0.0.0.0/0\" -> null - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } # openstack_networking_secgroup_rule_v2.service_rule will be destroyed - resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" { - direction = \"ingress\" -> null - ethertype = \"IPv4\" -> null - id = \"ead48696-4aae-436f-a12b-8bf3ceac253a\" -> null - port_range_max = 8080 -> null - port_range_min = 8080 -> null - protocol = \"tcp\" -> null - region = \"IU\" -> null - remote_ip_prefix = \"0.0.0.0/0\" -> null - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } # openstack_networking_secgroup_v2.terraform_tcp_1 will be destroyed - resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" { - all_tags = [] -> null - description = \"Security group with TCP open to 0.0.0.0/0\" -> null - id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -> null - name = \"terraform_tcp_1\" -> null - region = \"IU\" -> null - tags = [] -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } Plan: 0 to add, 0 to change, 8 to destroy. Changes to Outputs: - floating_ip_ubuntu20 = [ - \"149.165.168.217\" , ] -> null Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yes Expected Response after choosing 'yes' openstack_networking_secgroup_rule_v2.service_rule: Destroying... [ id = ead48696-4aae-436f-a12b-8bf3ceac253a ] openstack_networking_secgroup_rule_v2.https_rule: Destroying... [ id = 20225b99-fd2a-4a05-912c-61056fa21e3b ] openstack_compute_secgroup_v2.terraform_ssh_ping: Destroying... [ id = 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] openstack_networking_secgroup_rule_v2.http_rule: Destroying... [ id = 2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41 ] openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Destroying... [ id = 149 .165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/ ] openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Destruction complete after 3s openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Destroying... [ id = 6af630bc-9c16-450c-b16f-0483294d8d75 ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Destroying... [ id = afc53214-d2bd-45d5-b8b5-b6886976df8c ] openstack_networking_secgroup_rule_v2.https_rule: Destruction complete after 6s openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Destruction complete after 6s openstack_networking_secgroup_rule_v2.service_rule: Still destroying... [ id = ead48696-4aae-436f-a12b-8bf3ceac253a, 10s elapsed ] openstack_networking_secgroup_rule_v2.http_rule: Still destroying... [ id = 2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41, 10s elapsed ] openstack_networking_secgroup_rule_v2.service_rule: Destruction complete after 11s openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Still destroying... [ id = afc53214-d2bd-45d5-b8b5-b6886976df8c, 10s elapsed ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Destruction complete after 10s openstack_networking_secgroup_rule_v2.http_rule: Destruction complete after 16s openstack_networking_secgroup_v2.terraform_tcp_1: Destroying... [ id = 4f0ab1d5-ca29-4f60-9a65-c38d2380719c ] openstack_networking_secgroup_v2.terraform_tcp_1: Destruction complete after 9s openstack_compute_secgroup_v2.terraform_ssh_ping: Destroying... [ id = 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] openstack_compute_secgroup_v2.terraform_ssh_ping: Destruction complete after 2s Destroy complete! Resources: 8 destroyed. Troubleshooting \u00b6 My deployment cannot authenticate to the provider Make sure that you run source *-openrc.sh to provide your OpenStack credentials. Also make sure that you are using the correct ~/.ssh/id_rsa.pub key and that it has been injected to OpenStack terraform destroy did not complete there was an error like: Error: Error deleting openstack_compute_secgroup_v2 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f: Bad request with: [ DELETE https://js2.jetstream-cloud.org:8774/v2.1/os-security-groups/1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] , error message: { \"badRequest\" : { \"code\" : 400 , \"message\" : \"Security Group 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f in use.\\nNeutron server returns request_ids: ['req-e3d1548c-9d4c-4445-9642-952977a44853']\" }} Try running terraform destroy one more time, occasionally Horizon times out while destroying deployments.","title":"Introduction to Terraform"},{"location":"orchestration/terra/#introduction-to-terraform","text":"Terraform is an open-source infrastructure-as-code (IaC) software tool created by HashiCorp . What is Infrastructure-as-Code (IaC)? \"Infrastructure as code (IaC) is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools.\" - Wikipedia IaC tools allow you to manage infrastructure with configuration files rather than through a graphical user interface. IaC allows you to build, change, and manage your infrastructure in a safe, consistent, and repeatable way by defining resource configurations that you can version, reuse, and share. -- Terraform Documentation","title":"Introduction to Terraform"},{"location":"orchestration/terra/#overview","text":"This basic tutorial will guide you through setting up a Terraform project and deploying virtual machines (VMs) as infrastructure on OpenStack Cloud. Goals Understand orchestration for deployment to OpenStack cloud (Jetstream2) Understand the benefits of Terraform Ability to perform basic deployments on OpenStack using Terraform Ability to perform provisioning of deployed OpenStack resources through Terraform Things we won't cover OpenStack API All of Terraform's features","title":"Overview"},{"location":"orchestration/terra/#prerequisites","text":"Basic understanding of OpenStack and VMs Access to an OpenStack cloud (we will use Jetstream2 ) Terraform installed on your local machine Optional: request your own allocation to Jetstream2 on NSF ACCESS-CI Outcomes By the end of this tutorial, you will have created SSH keypair generated an *-openrc.sh file from OpenStack started, stopped, and destroyed a Terraform deployment on an OpenStack Cloud Terminology Ansible - is a suite of software tools that enables infrastructure as code Deploy - to create a cloud resource or software Infrastructure - is the collection of hardware and software elements such as computing power, networking, storage, and virtualization resources needed to enable cloud computing Orchestration - is the automated configuring, coordinating, and managing of computer systems and software Playbook - are a list of tasks that automatically execute against a host Provision - making changes to a VM including updating the operating system, installing software, adding configurations Terraform - is an infrastructure as code tool that lets you build, change, and version cloud and on-prem resources safely and efficiently","title":"Prerequisites"},{"location":"orchestration/terra/#getting-onto-openstack-cloud","text":"What is OpenStack? OpenStack is an open source cloud computing infrastructure software project and is one of the three most active open source projects in the world. OpenStack clouds are managed by individuals and institutions on their own bare-metal hardware. If you do not have an account, go to https://allocations.access-ci.org/ and begin the process by requesting an \"Explore\" start-up allocation. ACCESS is the NSF's management layer for their research computing network (formerly called TerraGrid and XSEDE) which includes high performance computing, high throughput computing, and research clouds like Jetstream2. Jetstream2 is a public research cloud which uses OpenStack as its management layer. CyVerse is developing a User Interface for Jetstream2 called CACAO (Cloud Automation & Continuous Analysis Orchestration) . Beneath its hood is Terraform. CACAO can also be used from the CLI (which we will show in a later lesson ).","title":"Getting onto  OpenStack Cloud"},{"location":"orchestration/terra/#terraform-installation","text":"Official Documentation Windows Installation Download Terraform using the appropriate distribution for your OS Mac OS X Installation Instructions for Mac OS X installation If you're on OS X, you can use brew to install with the following commands: # install terraform -- taken from https://learn.hashicorp.com/tutorials/terraform/install-cli brew tap hashicorp/tap && brew install hashicorp/tap/terraform # install ansible and jq (for processing terraform's statefile into an ansible inventory) brew install ansible jq Linux Installation Instructions for Ubuntu 22.04 installation wget -O- https://apt.releases.hashicorp.com/gpg | gpg --dearmor | sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $( lsb_release -cs ) main\" | sudo tee /etc/apt/sources.list.d/hashicorp.list sudo apt update && sudo apt install terraform Install Ansible & J Query sudo apt-add-repository ppa:ansible/ansible sudo apt update & sudo apt install -y ansible jq Confirm installation: terraform Should output: Usage: terraform [ global options ] <subcommand> [ args ] The available commands for execution are listed below. The primary workflow commands are given first, followed by less common or more advanced commands. Main commands: init Prepare your working directory for other commands validate Check whether the configuration is valid plan Show changes required by the current configuration apply Create or update infrastructure destroy Destroy previously-created infrastructure All other commands: console Try Terraform expressions at an interactive command prompt fmt Reformat your configuration in the standard style force-unlock Release a stuck lock on the current workspace get Install or upgrade remote Terraform modules graph Generate a Graphviz graph of the steps in an operation import Associate existing infrastructure with a Terraform resource login Obtain and save credentials for a remote host logout Remove locally-stored credentials for a remote host metadata Metadata related commands output Show output values from your root module providers Show the providers required for this configuration refresh Update the state to match remote systems show Show the current state or a saved plan state Advanced state management taint Mark a resource instance as not fully functional test Experimental support for module integration testing untaint Remove the 'tainted' state from a resource instance version Show the current Terraform version workspace Workspace management Global options ( use these before the subcommand, if any ) : -chdir = DIR Switch to a different working directory before executing the given subcommand. -help Show this help output, or the help for a specified subcommand. -version An alias for the \"version\" subcommand.","title":" Terraform installation"},{"location":"orchestration/terra/#generate-an-openstack-credential-for-terraform","text":"Log into OpenStack's Horizon Interface Step 1 Log into OpenStack's Horizon Interface and create application credentials Generate an openrc.sh file in Jetstream2 Horizon Interface ( https://js2.jetstream-cloud.org ), Select the \"Identity\" then \"Application Credentials\" option in the menu (left side) Select \"+ Create Application Credential\" button on right Give your new credentials a name and description, leave most of the fields blank Download the new crededential openrc.sh file to your local Important Do not close the Application Credentials window without copying the secret or downloading the openrc.sh file.","title":"Generate an OpenStack Credential for Terraform"},{"location":"orchestration/terra/#create-an-ssh-keypair-with-openstack","text":"Creating a SSH key To create an SSH key on an Ubuntu 22.04 terminal, you can follow these steps: Step 1: Open your terminal and type the following command to generate a new SSH key pair: ssh-keygen -t rsa -b 4096 Step 2: When prompted, press \"Enter\" to save the key in the default location, or enter a different filename and location to save the key. Enter a passphrase to secure your key. This passphrase will be required to use the key later. Once the key is generated, you can add it to your SSH agent by running the following command: eval \" $( ssh-agent -s ) \" ssh-add ~/.ssh/id_rsa Step 3: Copy the public key to your remote server by running the following command, replacing \"user\" and \"server\" with your username and server address: ssh-copy-id <user>@<server-ip> create_ssh_script.sh: #!/bin/bash echo \"Generating SSH key pair...\" ssh-keygen -t rsa -b 4096 echo \"Adding key to SSH agent...\" eval \" $( ssh-agent -s ) \" ssh-add ~/.ssh/id_rsa read -p \"Enter your remote server username: \" username read -p \"Enter your remote server IP address: \" server echo \"Ready to copy your new public key to a remote server:\" ssh-copy-id username@server echo \"SSH key setup complete!\" Save the script to a file, make it executable with the following command: chmod +x create_ssh_script.sh run it with the following command: ./create_ssh_script.sh Check to make sure you have a public key in the ~/.ssh/ directory, it should have the extension .pub ls ~/.ssh/ Create the keypair to OpenStack openstack keypair create --public-key ~/.ssh/id_rsa.pub tswetnam-terraform-key You can now check in OpenStack for the new keypair here: https://js2.jetstream-cloud.org/project/key_pairs","title":"Create an SSH keypair with OpenStack"},{"location":"orchestration/terra/#initialize-your-terraform-project","text":"Create a new project folder for our configuration files. mkdir ~/terraform Copy the openrc.sh file you downloaded from OpenStack into the new folder. cp *-openrc.sh ~/terraform/ Change Directory into your new terraform/ folder and source the openrc.sh file to create its environmental variables locally. By sourcing this file, you avoid placing sensitive information about yourself into your code. source *-openrc.sh","title":"Initialize your Terraform project"},{"location":"orchestration/terra/#configuration-files","text":"Terraform code is written in HCL (Hashicorp Configuration Language), and its configuration files typically end in the .tf file extension. Configuration .tf files can either be split into multiple files or maintained in a single file. When using multiple files, it is up to your discretion what the file names are, or how many you decide to split it into.","title":"Configuration files"},{"location":"orchestration/terra/#file-organization","text":"An example for file organization of a terraform project might involve: terraform-project / \u251c\u2500\u2500 main . tf \u251c\u2500\u2500 variables . tf \u251c\u2500\u2500 outputs . tf \u251c\u2500\u2500 instances . tf \u251c\u2500\u2500 security . tf \u2514\u2500\u2500 modules / \u251c\u2500\u2500 network / \u2502 \u251c\u2500\u2500 main . tf \u2502 \u251c\u2500\u2500 variables . tf \u2502 \u2514\u2500\u2500 outputs . tf \u2514\u2500\u2500 compute / \u251c\u2500\u2500 main . tf \u251c\u2500\u2500 variables . tf \u2514\u2500\u2500 outputs . tf Main Configuration File ( main.tf ): - contains the primary infrastructure resources and configurations for virtual machines, networks, and storage. Security File ( security.tf ): - contains optional security group details for instances. Variables File ( variables.tf ): - defines all the input variables used in the configuration. Declare variables with default values or leave them empty for required user input. Include descriptions for each variable to provide context. Instances File ( instances.tf ): - provisions the Instance flavor and IP networking of the VMs Outputs File ( outputs.tf ): - defines the outputs Terraform displays after applying the Main and Variables configuration. Includes: IP addresses, DNS names, or information for resources.","title":"File Organization"},{"location":"orchestration/terra/#other-optional-files","text":"Provider Configuration File ( provider.tf ): - includes the provider(s) used in the configuration, such as OpenStack (on commerical cloud: Amazon Web Services (AWS), Azure, or Google Cloud Platform(GCP)) along with their authentication and regional settings. Modules and Reusable Configurations: - create separate .tf files for reusable modules and configurations. Reuse across multiple projects or within the same project on multiple VMs.","title":"Other optional files"},{"location":"orchestration/terra/#terraform-configuration-files","text":"Create the main.tf file in the ~/terraform/ directory","title":"Terraform configuration files."},{"location":"orchestration/terra/#maintf","text":"terraform { required_version = \">= 0.14.0\" required_providers { openstack = { source = \"terraform-provider-openstack/openstack\" version = \">=1.47.0\" } } } provider \"openstack\" { auth_url = \"https://js2.jetstream-cloud.org:5000/v3/\" region = \"IU\" } The main.tf file has just the basics for calling out to an OpenStack provider - we created the necessary configurations for this in the prior steps by sourcing the *-openrc.sh file and running terraform init .","title":" main.tf"},{"location":"orchestration/terra/#variablestf","text":"Create variables.tf , it can also be called inputs.tf Here you need to go back to OpenStack and get a couple of additional variables: vm_number - defines the number of VMs you wish to launch public_key - you need the name of your paired SSH key that you generated in the prior step image_name - select the name of a featured or unique image you wish to launch. variable \"vm_number\" { # creates a single VM # replace with a larger number to launch more than one VM default = \"1\" } variable \"public_key\" { # replace this with the name of the public ssh key you uploaded to Jetstream 2 # https://docs.jetstream-cloud.org/ui/cli/managing-ssh-keys/ default = \"tswetnam-terraform-key\" } variable \"image_name\" { # replace this with the image name of the ubuntu iso you want to use # https://js2.jetstream-cloud.org/project/images default = \"Featured-Ubuntu20\" } variable \"network_name\" { # replace this with the id of the public interface on JS2 in Project / Network / Networks / public # https://js2.jetstream-cloud.org/project/networks/ default = \"auto_allocated_network\" }","title":" variables.tf"},{"location":"orchestration/terra/#securitytf","text":"This file produces two distinct security groups, terraform_ssh_ping creates a group with ssh access and ping , terraform_tcp_1 creates a group which opens the HTTP (80, 8080), HTTPS (443) ports for connecting a browser based service. ################ #Security section ################ # Creating Compute Security group resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" { name = \"terraform_ssh_ping\" description = \"Security group with SSH and PING open to 0.0.0.0/0\" #ssh rule rule { ip_protocol = \"tcp\" from_port = \"22\" to_port = \"22\" cidr = \"0.0.0.0/0\" } rule { from_port = -1 to_port = -1 ip_protocol = \"icmp\" cidr = \"0.0.0.0/0\" } } # Create a Netowrking Security group resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" { name = \"terraform_tcp_1\" description = \"Security group with TCP open to 0.0.0.0/0\" } # Allow HTTP (port 80) traffic resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { direction = \"ingress\" ethertype = \"IPv4\" protocol = \"tcp\" port_range_min = 80 port_range_max = 80 remote_ip_prefix = \"0.0.0.0/0\" security_group_id = \" ${ openstack_networking_secgroup_v2 .terraform_tcp_1.id } \" } # Allow HTTPS (port 443) traffic resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" { direction = \"ingress\" ethertype = \"IPv4\" protocol = \"tcp\" port_range_min = 443 port_range_max = 443 remote_ip_prefix = \"0.0.0.0/0\" security_group_id = \" ${ openstack_networking_secgroup_v2 .terraform_tcp_1.id } \" } # Allow Service (port 8080) traffic resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" { direction = \"ingress\" ethertype = \"IPv4\" protocol = \"tcp\" port_range_min = 8080 port_range_max = 8080 remote_ip_prefix = \"0.0.0.0/0\" security_group_id = \" ${ openstack_networking_secgroup_v2 .terraform_tcp_1.id } \" }","title":" security.tf"},{"location":"orchestration/terra/#instancestf","text":"################ # Instance OS ################ # create each Ubuntu20 instance resource \"openstack_compute_instance_v2\" \"Ubuntu20\" { name = \"container_camp_Ubuntu20_ ${ count .index } \" # ID of Featured-Ubuntu20 image_name = var.image_name # flavor_id is the size of the VM # https://docs.jetstream-cloud.org/general/vmsizes/ flavor_name = \"m3.tiny\" # this public key is set above in security section key_pair = var.public_key security_groups = [ \"terraform_ssh_ping\" , \"default\" ] count = var.vm_number metadata = { terraform_controlled = \"yes\" } network { name = var.network_name } #depends_on = [openstack_networking_network_v2.terraform_network] } # creating floating ips from the public ip pool resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" { pool = \"public\" count = var.vm_number } # assign floating ip to each Ubuntu20 VM resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" { floating_ip = \" ${ openstack_networking_floatingip_v2 .terraform_floatip_ubuntu20[count.index].address } \" instance_id = \" ${ openstack_compute_instance_v2 .Ubuntu20[count.index].id } \" count = var.vm_number }","title":" instances.tf"},{"location":"orchestration/terra/#outputtf","text":"################ #Output ################ output \"floating_ip_ubuntu20\" { value = openstack_networking_floatingip_v2.terraform_floatip_ubuntu20.*.address description = \"Public IP for Ubuntu 20\" }","title":" output.tf"},{"location":"orchestration/terra/#terraformtfvars","text":"A terraform.tfvars file is used to define the values of input variables. It can also be renamed *.auto.tfvars . It serves as a convenient way to store and manage variable values that you don't want to hardcode in your .tf files or provide via command-line arguments. By using a terraform.tfvars file, you can easily customize and update the variable values for different environments or scenarios. The file should contain key-value pairs, where the key is the variable name and the value is the corresponding variable value. The syntax for defining variables in the terraform.tfvars file can be either HCL or JSON. Add your OpenStack credentials and other required information to terraform.tfvars : In HCL: openstack_user_name = \"your-openstack-username\" openstack_password = \"your-openstack-password\" openstack_tenant_name = \"your-openstack-tenant-name\" openstack_auth_url = \"your-openstack-auth-url\" In JSON: { \"openstack_user_name\" : \"your-openstack-username\" , \"openstack_password\" : \"your-openstack-password\" , \"openstack_tenant_name\" : \"your-openstack-tenant-name\" , \"openstack_auth_url\" : \"your-openstack-auth-url\" } When you run terraform apply or terraform plan , Terraform will automatically load the values from the terraform.tfvars file if it exists in the working directory. You can also create multiple .tfvars files and specify which one to use by passing the -var-file flag when executing Terraform commands: terraform apply -var-file = \"custom.tfvars\"","title":" terraform.tfvars"},{"location":"orchestration/terra/#intermediate-directories-and-files","text":"When terraform plan and terraform apply are executed, Terraform generates some new project files, notably .terraform/ - this directory will contain the terraform-provider-openstack_version and a README.md , LICENCE , and CHANGELOG.md terraform.lock.hcl terraform.tfstate terraform.tfstate.backup","title":" Intermediate directories and files"},{"location":"orchestration/terra/#terraform-commands","text":"","title":" Terraform Commands"},{"location":"orchestration/terra/#init","text":"Now, you are ready to initialize the Terraform project cd ~/terraform terraform init Expected Response Initializing the backend... Initializing provider plugins... - Reusing previous version of terraform-provider-openstack/openstack from the dependency lock file - Using previously-installed terraform-provider-openstack/openstack v1.51.1 Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.","title":" init"},{"location":"orchestration/terra/#plan","text":"plan - generates the configuration based on the .tf files in the initialized directory terraform plan Expected Response Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0] will be created + resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" { + floating_ip = ( known after apply ) + id = ( known after apply ) + instance_id = ( known after apply ) + region = ( known after apply ) } # openstack_compute_instance_v2.Ubuntu20[0] will be created + resource \"openstack_compute_instance_v2\" \"Ubuntu20\" { + access_ip_v4 = ( known after apply ) + access_ip_v6 = ( known after apply ) + all_metadata = ( known after apply ) + all_tags = ( known after apply ) + availability_zone = ( known after apply ) + created = ( known after apply ) + flavor_id = ( known after apply ) + flavor_name = \"m3.tiny\" + force_delete = false + id = ( known after apply ) + image_id = ( known after apply ) + image_name = \"Featured-Ubuntu20\" + key_pair = \"tswetnam-terraform-key\" + metadata = { + \"terraform_controlled\" = \"yes\" } + name = \"container_camp_Ubuntu20_0\" + power_state = \"active\" + region = ( known after apply ) + security_groups = [ + \"default\" , + \"terraform_ssh_ping\" , ] + stop_before_destroy = false + updated = ( known after apply ) + network { + access_network = false + fixed_ip_v4 = ( known after apply ) + fixed_ip_v6 = ( known after apply ) + floating_ip = ( known after apply ) + mac = ( known after apply ) + name = \"auto_allocated_network\" + port = ( known after apply ) + uuid = ( known after apply ) } } # openstack_compute_secgroup_v2.terraform_ssh_ping will be created + resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" { + description = \"Security group with SSH and PING open to 0.0.0.0/0\" + id = ( known after apply ) + name = \"terraform_ssh_ping\" + region = ( known after apply ) + rule { + cidr = \"0.0.0.0/0\" + from_port = -1 + id = ( known after apply ) + ip_protocol = \"icmp\" + self = false + to_port = -1 } + rule { + cidr = \"0.0.0.0/0\" + from_port = 22 + id = ( known after apply ) + ip_protocol = \"tcp\" + self = false + to_port = 22 } } # openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0] will be created + resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" { + address = ( known after apply ) + all_tags = ( known after apply ) + dns_domain = ( known after apply ) + dns_name = ( known after apply ) + fixed_ip = ( known after apply ) + id = ( known after apply ) + pool = \"public\" + port_id = ( known after apply ) + region = ( known after apply ) + subnet_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.http_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 80 + port_range_min = 80 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.https_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 443 + port_range_min = 443 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.service_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 8080 + port_range_min = 8080 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_v2.terraform_tcp_1 will be created + resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" { + all_tags = ( known after apply ) + description = \"Security group with TCP open to 0.0.0.0/0\" + id = ( known after apply ) + name = \"terraform_tcp_1\" + region = ( known after apply ) + tenant_id = ( known after apply ) } Plan: 8 to add, 0 to change, 0 to destroy. Changes to Outputs: + floating_ip_ubuntu20 = [ + null, ] \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn 't use the -out option to save this plan, so Terraform can' t guarantee to take exactly these actions if you run \"terraform apply\" now.","title":" plan"},{"location":"orchestration/terra/#apply","text":"terraform apply Expected response Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0] will be created + resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" { + floating_ip = ( known after apply ) + id = ( known after apply ) + instance_id = ( known after apply ) + region = ( known after apply ) } # openstack_compute_instance_v2.Ubuntu20[0] will be created + resource \"openstack_compute_instance_v2\" \"Ubuntu20\" { + access_ip_v4 = ( known after apply ) + access_ip_v6 = ( known after apply ) + all_metadata = ( known after apply ) + all_tags = ( known after apply ) + availability_zone = ( known after apply ) + created = ( known after apply ) + flavor_id = ( known after apply ) + flavor_name = \"m3.tiny\" + force_delete = false + id = ( known after apply ) + image_id = ( known after apply ) + image_name = \"Featured-Ubuntu20\" + key_pair = \"tswetnam-terraform-key\" + metadata = { + \"terraform_controlled\" = \"yes\" } + name = \"container_camp_Ubuntu20_0\" + power_state = \"active\" + region = ( known after apply ) + security_groups = [ + \"default\" , + \"terraform_ssh_ping\" , ] + stop_before_destroy = false + updated = ( known after apply ) + network { + access_network = false + fixed_ip_v4 = ( known after apply ) + fixed_ip_v6 = ( known after apply ) + floating_ip = ( known after apply ) + mac = ( known after apply ) + name = \"auto_allocated_network\" + port = ( known after apply ) + uuid = ( known after apply ) } } # openstack_compute_secgroup_v2.terraform_ssh_ping will be created + resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" { + description = \"Security group with SSH and PING open to 0.0.0.0/0\" + id = ( known after apply ) + name = \"terraform_ssh_ping\" + region = ( known after apply ) + rule { + cidr = \"0.0.0.0/0\" + from_port = -1 + id = ( known after apply ) + ip_protocol = \"icmp\" + self = false + to_port = -1 } + rule { + cidr = \"0.0.0.0/0\" + from_port = 22 + id = ( known after apply ) + ip_protocol = \"tcp\" + self = false + to_port = 22 } } # openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0] will be created + resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" { + address = ( known after apply ) + all_tags = ( known after apply ) + dns_domain = ( known after apply ) + dns_name = ( known after apply ) + fixed_ip = ( known after apply ) + id = ( known after apply ) + pool = \"public\" + port_id = ( known after apply ) + region = ( known after apply ) + subnet_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.http_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 80 + port_range_min = 80 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.https_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 443 + port_range_min = 443 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_rule_v2.service_rule will be created + resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" { + direction = \"ingress\" + ethertype = \"IPv4\" + id = ( known after apply ) + port_range_max = 8080 + port_range_min = 8080 + protocol = \"tcp\" + region = ( known after apply ) + remote_group_id = ( known after apply ) + remote_ip_prefix = \"0.0.0.0/0\" + security_group_id = ( known after apply ) + tenant_id = ( known after apply ) } # openstack_networking_secgroup_v2.terraform_tcp_1 will be created + resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" { + all_tags = ( known after apply ) + description = \"Security group with TCP open to 0.0.0.0/0\" + id = ( known after apply ) + name = \"terraform_tcp_1\" + region = ( known after apply ) + tenant_id = ( known after apply ) } Plan: 8 to add, 0 to change, 0 to destroy. Changes to Outputs: + floating_ip_ubuntu20 = [ + null, ] Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. You should be prompted Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Expected Response after choosing 'yes' openstack_networking_secgroup_v2.terraform_tcp_1: Creating... openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Creating... openstack_compute_secgroup_v2.terraform_ssh_ping: Creating... openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Creating... openstack_networking_secgroup_v2.terraform_tcp_1: Creation complete after 1s [ id = 4f0ab1d5-ca29-4f60-9a65-c38d2380719c ] openstack_networking_secgroup_rule_v2.service_rule: Creating... openstack_networking_secgroup_rule_v2.https_rule: Creating... openstack_networking_secgroup_rule_v2.http_rule: Creating... openstack_networking_secgroup_rule_v2.http_rule: Creation complete after 1s [ id = 2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41 ] openstack_networking_secgroup_rule_v2.service_rule: Creation complete after 2s [ id = ead48696-4aae-436f-a12b-8bf3ceac253a ] openstack_networking_secgroup_rule_v2.https_rule: Creation complete after 2s [ id = 20225b99-fd2a-4a05-912c-61056fa21e3b ] openstack_compute_secgroup_v2.terraform_ssh_ping: Creation complete after 4s [ id = 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Creation complete after 7s [ id = 6af630bc-9c16-450c-b16f-0483294d8d75 ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Still creating... [ 10s elapsed ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Creation complete after 15s [ id = afc53214-d2bd-45d5-b8b5-b6886976df8c ] openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Creating... openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Creation complete after 2s [ id = 149 .165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/ ] Apply complete! Resources: 8 added, 0 changed, 0 destroyed. Outputs: floating_ip_ubuntu20 = [ \"149.165.168.217\" , ] Congratulations! You now have a running VM, you can check its status on Horizon, or try to connect to it over ssh The floating_ip_ubuntu20 should be the new VM's IP address (yours will be different than this example). ssh ubuntu@<IP-ADDRESS> Make sure that","title":" apply"},{"location":"orchestration/terra/#destroy","text":"When you are ready to complete the use of your VMs or your deployment, you can destroy the project. terraform destroy Expected Response openstack_compute_secgroup_v2.terraform_ssh_ping: Refreshing state... [ id = 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Refreshing state... [ id = afc53214-d2bd-45d5-b8b5-b6886976df8c ] openstack_networking_secgroup_v2.terraform_tcp_1: Refreshing state... [ id = 4f0ab1d5-ca29-4f60-9a65-c38d2380719c ] openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Refreshing state... [ id = 6af630bc-9c16-450c-b16f-0483294d8d75 ] openstack_networking_secgroup_rule_v2.service_rule: Refreshing state... [ id = ead48696-4aae-436f-a12b-8bf3ceac253a ] openstack_networking_secgroup_rule_v2.https_rule: Refreshing state... [ id = 20225b99-fd2a-4a05-912c-61056fa21e3b ] openstack_networking_secgroup_rule_v2.http_rule: Refreshing state... [ id = 2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41 ] openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Refreshing state... [ id = 149 .165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/ ] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20[0] will be destroyed - resource \"openstack_compute_floatingip_associate_v2\" \"terraform_floatip_ubuntu20\" { - floating_ip = \"149.165.168.217\" -> null - id = \"149.165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/\" -> null - instance_id = \"afc53214-d2bd-45d5-b8b5-b6886976df8c\" -> null - region = \"IU\" -> null } # openstack_compute_instance_v2.Ubuntu20[0] will be destroyed - resource \"openstack_compute_instance_v2\" \"Ubuntu20\" { - access_ip_v4 = \"10.0.24.205\" -> null - all_metadata = { - \"terraform_controlled\" = \"yes\" } -> null - all_tags = [] -> null - availability_zone = \"nova\" -> null - created = \"2023-03-23 23:09:42 +0000 UTC\" -> null - flavor_id = \"1\" -> null - flavor_name = \"m3.tiny\" -> null - force_delete = false -> null - id = \"afc53214-d2bd-45d5-b8b5-b6886976df8c\" -> null - image_id = \"27424df1-c3ea-4c4a-ad8b-6ea9a476f6f8\" -> null - image_name = \"Featured-Ubuntu20\" -> null - key_pair = \"tswetnam-terraform-key\" -> null - metadata = { - \"terraform_controlled\" = \"yes\" } -> null - name = \"container_camp_Ubuntu20_0\" -> null - power_state = \"active\" -> null - region = \"IU\" -> null - security_groups = [ - \"default\" , - \"terraform_ssh_ping\" , ] -> null - stop_before_destroy = false -> null - tags = [] -> null - updated = \"2023-03-23 23:09:52 +0000 UTC\" -> null - network { - access_network = false -> null - fixed_ip_v4 = \"10.0.24.205\" -> null - mac = \"fa:16:3e:16:6e:8f\" -> null - name = \"auto_allocated_network\" -> null - uuid = \"e4ff98ff-d29a-48f4-a95b-88ebd6b0662f\" -> null } } # openstack_compute_secgroup_v2.terraform_ssh_ping will be destroyed - resource \"openstack_compute_secgroup_v2\" \"terraform_ssh_ping\" { - description = \"Security group with SSH and PING open to 0.0.0.0/0\" -> null - id = \"1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f\" -> null - name = \"terraform_ssh_ping\" -> null - region = \"IU\" -> null - rule { - cidr = \"0.0.0.0/0\" -> null - from_port = -1 -> null - id = \"611e1c0a-edc1-462d-8e1a-5f8a72c4d968\" -> null - ip_protocol = \"icmp\" -> null - self = false -> null - to_port = -1 -> null } - rule { - cidr = \"0.0.0.0/0\" -> null - from_port = 22 -> null - id = \"ce99a035-5df2-4131-ba90-268d045d0ff3\" -> null - ip_protocol = \"tcp\" -> null - self = false -> null - to_port = 22 -> null } } # openstack_networking_floatingip_v2.terraform_floatip_ubuntu20[0] will be destroyed - resource \"openstack_networking_floatingip_v2\" \"terraform_floatip_ubuntu20\" { - address = \"149.165.168.217\" -> null - all_tags = [] -> null - fixed_ip = \"10.0.24.205\" -> null - id = \"6af630bc-9c16-450c-b16f-0483294d8d75\" -> null - pool = \"public\" -> null - port_id = \"71e9ae50-e281-4f98-afab-f1b6c1932806\" -> null - region = \"IU\" -> null - tags = [] -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } # openstack_networking_secgroup_rule_v2.http_rule will be destroyed - resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { - direction = \"ingress\" -> null - ethertype = \"IPv4\" -> null - id = \"2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41\" -> null - port_range_max = 80 -> null - port_range_min = 80 -> null - protocol = \"tcp\" -> null - region = \"IU\" -> null - remote_ip_prefix = \"0.0.0.0/0\" -> null - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } # openstack_networking_secgroup_rule_v2.https_rule will be destroyed - resource \"openstack_networking_secgroup_rule_v2\" \"https_rule\" { - direction = \"ingress\" -> null - ethertype = \"IPv4\" -> null - id = \"20225b99-fd2a-4a05-912c-61056fa21e3b\" -> null - port_range_max = 443 -> null - port_range_min = 443 -> null - protocol = \"tcp\" -> null - region = \"IU\" -> null - remote_ip_prefix = \"0.0.0.0/0\" -> null - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } # openstack_networking_secgroup_rule_v2.service_rule will be destroyed - resource \"openstack_networking_secgroup_rule_v2\" \"service_rule\" { - direction = \"ingress\" -> null - ethertype = \"IPv4\" -> null - id = \"ead48696-4aae-436f-a12b-8bf3ceac253a\" -> null - port_range_max = 8080 -> null - port_range_min = 8080 -> null - protocol = \"tcp\" -> null - region = \"IU\" -> null - remote_ip_prefix = \"0.0.0.0/0\" -> null - security_group_id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } # openstack_networking_secgroup_v2.terraform_tcp_1 will be destroyed - resource \"openstack_networking_secgroup_v2\" \"terraform_tcp_1\" { - all_tags = [] -> null - description = \"Security group with TCP open to 0.0.0.0/0\" -> null - id = \"4f0ab1d5-ca29-4f60-9a65-c38d2380719c\" -> null - name = \"terraform_tcp_1\" -> null - region = \"IU\" -> null - tags = [] -> null - tenant_id = \"db016a81886b4f918705e5dee2b24298\" -> null } Plan: 0 to add, 0 to change, 8 to destroy. Changes to Outputs: - floating_ip_ubuntu20 = [ - \"149.165.168.217\" , ] -> null Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only 'yes' will be accepted to confirm. Enter a value: yes Expected Response after choosing 'yes' openstack_networking_secgroup_rule_v2.service_rule: Destroying... [ id = ead48696-4aae-436f-a12b-8bf3ceac253a ] openstack_networking_secgroup_rule_v2.https_rule: Destroying... [ id = 20225b99-fd2a-4a05-912c-61056fa21e3b ] openstack_compute_secgroup_v2.terraform_ssh_ping: Destroying... [ id = 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] openstack_networking_secgroup_rule_v2.http_rule: Destroying... [ id = 2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41 ] openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Destroying... [ id = 149 .165.168.217/afc53214-d2bd-45d5-b8b5-b6886976df8c/ ] openstack_compute_floatingip_associate_v2.terraform_floatip_ubuntu20 [ 0 ] : Destruction complete after 3s openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Destroying... [ id = 6af630bc-9c16-450c-b16f-0483294d8d75 ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Destroying... [ id = afc53214-d2bd-45d5-b8b5-b6886976df8c ] openstack_networking_secgroup_rule_v2.https_rule: Destruction complete after 6s openstack_networking_floatingip_v2.terraform_floatip_ubuntu20 [ 0 ] : Destruction complete after 6s openstack_networking_secgroup_rule_v2.service_rule: Still destroying... [ id = ead48696-4aae-436f-a12b-8bf3ceac253a, 10s elapsed ] openstack_networking_secgroup_rule_v2.http_rule: Still destroying... [ id = 2d7d6f4c-a3db-48ca-996a-1fe0b18f4f41, 10s elapsed ] openstack_networking_secgroup_rule_v2.service_rule: Destruction complete after 11s openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Still destroying... [ id = afc53214-d2bd-45d5-b8b5-b6886976df8c, 10s elapsed ] openstack_compute_instance_v2.Ubuntu20 [ 0 ] : Destruction complete after 10s openstack_networking_secgroup_rule_v2.http_rule: Destruction complete after 16s openstack_networking_secgroup_v2.terraform_tcp_1: Destroying... [ id = 4f0ab1d5-ca29-4f60-9a65-c38d2380719c ] openstack_networking_secgroup_v2.terraform_tcp_1: Destruction complete after 9s openstack_compute_secgroup_v2.terraform_ssh_ping: Destroying... [ id = 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] openstack_compute_secgroup_v2.terraform_ssh_ping: Destruction complete after 2s Destroy complete! Resources: 8 destroyed.","title":" destroy"},{"location":"orchestration/terra/#troubleshooting","text":"My deployment cannot authenticate to the provider Make sure that you run source *-openrc.sh to provide your OpenStack credentials. Also make sure that you are using the correct ~/.ssh/id_rsa.pub key and that it has been injected to OpenStack terraform destroy did not complete there was an error like: Error: Error deleting openstack_compute_secgroup_v2 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f: Bad request with: [ DELETE https://js2.jetstream-cloud.org:8774/v2.1/os-security-groups/1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f ] , error message: { \"badRequest\" : { \"code\" : 400 , \"message\" : \"Security Group 1af38fb4-1d41-4ac2-9c9f-91fc6bbaa72f in use.\\nNeutron server returns request_ids: ['req-e3d1548c-9d4c-4445-9642-952977a44853']\" }} Try running terraform destroy one more time, occasionally Horizon times out while destroying deployments.","title":"Troubleshooting"},{"location":"resources/docker/","text":"Useful Resources related to Docker \u00b6 Awesome Docker List Docker Labs Docker Community Forums Docker Hub Docker Official Documentation Docker on StackOverflow Official Twitter Play With Docker Docker Cloud Useful Resources related to Private and Self-Hosted Container Registries \u00b6 Harbor.io - Go Harbor Kubernetes Docker registry NVIDIA GPU Cloud BioContainers Useful Resources related to Docker-Compose \u00b6 Awesome Docker Compose Official Documentation","title":"Docker"},{"location":"resources/docker/#useful-resources-related-to-docker","text":"Awesome Docker List Docker Labs Docker Community Forums Docker Hub Docker Official Documentation Docker on StackOverflow Official Twitter Play With Docker Docker Cloud","title":"Useful Resources related to Docker"},{"location":"resources/docker/#useful-resources-related-to-private-and-self-hosted-container-registries","text":"Harbor.io - Go Harbor Kubernetes Docker registry NVIDIA GPU Cloud BioContainers","title":"Useful Resources related to Private and Self-Hosted Container Registries"},{"location":"resources/docker/#useful-resources-related-to-docker-compose","text":"Awesome Docker Compose Official Documentation","title":"Useful Resources related to Docker-Compose"},{"location":"resources/k8s/","text":"Useful resources related to Kubernetes \u00b6 Zero to JupyterHub with Kubernetes Nubenetes Awesome Kubernetes & Cloud - A curated list of awesome references collected since 2018. Microservices architectures rely on DevOps practices, automation, CI/CD (Continuous Integration & Delivery), and API-focused designs. Golang awesome Kubernetes tools and resources Pacific Research Platform provides K8s through its Nautilus Clusters Admiralty - multi-cluster Kubernetes controller Rancher (K3s) Lightweight Kubernetes for Edge, IOT, CI deployments KEDA - K8s event driven autoscaler. Useful resources related to Automation \u00b6 Terraform by Hashicorp Terraform Awesome List","title":"Kubernetes"},{"location":"resources/k8s/#useful-resources-related-to-kubernetes","text":"Zero to JupyterHub with Kubernetes Nubenetes Awesome Kubernetes & Cloud - A curated list of awesome references collected since 2018. Microservices architectures rely on DevOps practices, automation, CI/CD (Continuous Integration & Delivery), and API-focused designs. Golang awesome Kubernetes tools and resources Pacific Research Platform provides K8s through its Nautilus Clusters Admiralty - multi-cluster Kubernetes controller Rancher (K3s) Lightweight Kubernetes for Edge, IOT, CI deployments KEDA - K8s event driven autoscaler.","title":"Useful resources related to  Kubernetes"},{"location":"resources/k8s/#useful-resources-related-to-automation","text":"Terraform by Hashicorp Terraform Awesome List","title":"Useful resources related to Automation"},{"location":"resources/post/","text":"General Notes and Discussions \u00b6 This section summarizes general notes, discussions and questions raised during both the Basics and Advanced section of Container Camp. Before you Continue This page is for archival and informative use only, reproduction (such as access to JetStream2 allocations, CodeSpaces) is limited. Basics \u00b6 Queries \u00b6 Q: What is an image? A: A file that lives in the cache on your computer... where 'cache' can be thought of like a desk. It's faster to retrieve a file from your desk than from the filing cabinet Q: What is a container? A: It's a virtualized run-time environment which starts from the image. A docker image is what you build. It has all of the software necessary to run the code. The Container is when you \"activate\" the image, an extra layer where you can work on top of the software you put in the image. The built image will contain its own OS - it will make no difference where you build your container. When you build an image, you can specify the architecture of the machine you want it to run on. Manage resources for your container by using commands to stop, pause, restart, remove a container. Q: How do I work with data and containers? A: Containers do not contain large amounts of data, as these will take space in the writable layer of the container (see above image). Instead, it is suggested to use Volumes as a best practice. A Volume is a directory that lives outside of the container that can be attached to said container. Once attached, the contents of the directory will be viewable and accessible to the container. In order to attach the volume, one must specify the directory on the computer AND the destination folder, separated by a colon (:). The format is as follows -v <directory on computer>:<directory in container> . Q: Ports. What are ports and why do we need them? A: Ports are where network connections start and end. These are not physical, and these allow software to connect to a network. Our computers run hundreds of processes, and chance is a lot of these processes require a connection to the network. In order for these processes to access the network, ports are required. A single process is assigned a single port - and this is how these software can connect to the internet. The reason why we need to open a port for Docker, is because the Docker container is trying to communicate with the network, however it requires us, the user, to assign a port for it. Without us assigning the port to the Docker container, the connection to the network cannot happen. More information on ports can be found at: - CloudFlare: Understanding Ports - Internet Assigned Numbers Authority Port List - List of TCP and UDP port numbers Q: What is Docker's relationship with things like networking, ethernet, USB, HDMI? Are these things naturally closed or naturally open? Are there interfaces that you cannot access from docker? A: Docker is able to do Networking as it has its own networking subsystem (and commands). As this is an advanced topic, let me direct you to the official networking documentation here: https://docs.docker.com/network/ Q: Is there a way to emulate a display in Docker so that certain rendering code (like the plotting libraries in python) don't break when run in a container? A: [unsure if this is what you were looking for] Docker is able to run GUI applications; A display can be specified using the -e (environment) flag such as -e DISPLAY=$DISPLAY . $DISPLAY can usually be specified to $0 , targeting the primary display. This little blog post may be able to help you further. Q: What should we know about accessing GPUs from docker? Won't the hardware you're running on affect the runnability of a container, despite the containerization of the image? A: NVIDIA Docker now uses a special flag for Docker (rather than needing its own installation) https://github.com/NVIDIA/nvidia-docker . Q: Is malware ever a problem with Dockerfiles? Can you run a malicious image? A: It seems that Docker (and Kubernetes) related malware are now a thing. From personal experience, I have never run into issues. Q: If containers are software, why should I bother using a container instead of the software itself? A: Containers offer 3 great solutions to common problems: (1) reproducibility (2) version control. Docker images contain all of the required software in the form of layers, including specific versions of libraries. This allows to easily share your image and software without worring about collaborators having to install the correct software and version. (3) portability, so you can run it anywhere. Dockerfiles Hands-on \u00b6 Go to an example directory in the intro2docker repository with a Dockerfile cd alpine Build the container using the build command. Make sure to include the . docker build -t test/alpine . Note The container should get the default latest tag if it is not specified in the docker build command with the name test/alpine Start the container using the run command. docker run --rm test/alpine:latest To run the container and override its CMD, it will use its own shell sh : docker run -it --rm test/alpine:latest sh Dockerfiles are like your recipie book, and like every recipie book you have instructions. The instructions aren't for the user, but for Docker itself. These instruction are the capitalized commands you see at the beginning of lines, and these tell Docker what to do: Instruction Command FROM Instructs to use a specific Docker image LABEL Adds metadata to the image RUN Executes a specific command ENV Sets environmental variables COPY Copies a file from a specified location to the image CMD Sets a command to be executed when running a container ENTRYPOINT Configures and run a container as an executable USER Used to set User specific information EXPOSE exposes a specific port *the above list is nonexhaustive, visit the official Docker documentation for more information and further instructions. Pushing to DockerHub \u00b6 Build your docker image with docker build -t <Dockerhub username>/<Docker image>:<version> . then, log in to Docker with docker login -u <username> This will then ask for your Password; type in your password ( it will NOT show you the password ). If it does not login automatically, please follow the instructions here . Once you have logged in, push your docker to the DockerHub registry with docker push <Dockerhub username>/<Docker image>:<version> Your newly built Docker image now lives on DockerHub. You can view it at https://hub.docker.com/r/<username>/<Docker image> Assigning Users \u00b6 Create a new folder called ubuntu mkdir ubuntu Change into the folder cd ubuntu Create a Dockerfile ARG VERSION=18.04 FROM ubuntu:$VERSION RUN apt-get update -y && apt-get install -y gnupg wget python3 python3-distro && \\ wget -qO - https://packages.irods.org/irods-signing-key.asc | apt-key add - && \\ echo \"deb [arch=amd64] https://packages.irods.org/apt/ $(lsb_release -sc) main\" >> /etc/apt/sources.list.d/renci-irods.list && \\ apt-get update && apt-get install irods-icommands -y COPY irods_environment.json /home/ubuntu/.irods/ RUN useradd ubuntu && \\ chown -R ubuntu:ubuntu /home/ubuntu USER ubuntu Create a file called irods_environment.json { \"irods_host\": \"data.cyverse.org\", \"irods_port\": 1247, \"irods_zone_name\": \"iplant\" } Build the container using your dockerhub username docker build -t <yourusername>/ubuntu-irods:18.04 . Run with docker run -it --rm <yourusername>/ubuntu-irods:18.04 Q: What did we do? A: We created an image whose the user is specified. Q: Why? A: When creating interactive containers, these containers are not built with root privileges. Assigning a specific user helps with defining the priviledges you want users to have. Q: Wait, what? A: When pulling a base image with the FROM instruction, sometimes the user is already defined. The only user with priviledges will be that already defined user. Therefore, in order to have the \"right\" priviledges, you have to assign the right user in your Dockerfile . RStudio Dockerfile \u00b6 The above steps where necessary in order to understand why in this following step we need to define a user. Navigate to rstudio/verse with cd rstudio/verse and create a Dockerfile: FROM rocker/verse:4.2.0 # Install your own stuff below RUN install2.r --error \\ # Added Packages PerformanceAnalytics \\ boot \\ devtools \\ dlm \\ dplyr \\ foreign \\ lubridate \\ plotly \\ truncreg \\ ggridges Build the Docker image with: docker build -t <yourusername>/rstudio:tag . Execute with docker run --rm -p 8787:8787 -e DISABLE_AUTH=true <username>/rstudio:<version> Docker Commands Cheat Sheets: \u00b6 https://cyverse-learning-materials.github.io/container-camp/docker/intro/#docker-commands https://dockerlabs.collabnix.com/docker/cheatsheet/ https://github.com/wsargent/docker-cheat-sheet Advanced \u00b6 Github Actions: \u00b6 GitHub not only can host your code, but it can be used to create Docker images right within the GitHub repo using GitHub Actions. You first require to connect your DockerHub and your GitHub accounts: Navigate to Settings (in your repository) Select Secrets > Actions. Add a New Repository Secret and name it \"DOCKER_HUB_USERNAME\"; Paste here your Docker Account name. Add a New Repository Secret and name it \"DOCKER_HUB_ACCESS_TOKEN\", and DO NOT CLOSE IT Navigate to DockerHub and click Account Settings > Security > New Access Token Add a name you can recognize and COPY the password and add it to \"DOCKER_HUB_ACCESS_TOKEN\" (step 4). Now save and close. Your GitHub and DockerHub are connected! To create a new GitHub Action, navigate to the Action button on the top of your repository (create a repository first!), and click on New Workflow ; select Docker Image . Delete the template pase the following: name: <NAME OF ACTION> on: push: branches: [ main ] pull_request: branches: [ main ] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Login to Docker Hub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKER_HUB_USERNAME }} password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }} - name: Build and push uses: docker/build-push-action@v2 with: context: . file: ./Dockerfile push: true tags: ${{ secrets.DOCKER_HUB_USERNAME }}/<NAME OF IMAGE ON DOCKERHUB>:<VERSION> - For the above code, modify all content in \" as you deem necessary. Remember to add a Dockerfile to your main repository! Working on the Cloud \u00b6 We discussed working with JetStream2 . - Access JetStream2 exoshpere - Access JetStream2 CyVerse (CACAO) Summary of running JS2 GPU Virtual Machines (VM): Navigate to : https://jetstream2.exosphere.app/ Click Create > Instance Select Ubuntu 20.04 Select a g3.small Instance and select (enable web desktop), click create. Wait for VM to spin up, test for functionality by: 1. click on web shell 2. From web shell, run nvidia-smi (if it returns a table, you are good to go) Running xpra on JS2: $ export DISPLAY=:0 $ xinit & $ docker run --gpus all --rm -it -p 9876:9876 -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY -e XAUTHORITY -e QT_X11_NO_MITSHM=1 -e NVIDIA_DRIVER_CAPABILITIES=all harbor.cyverse.org/vice/xpra/cudagl:20.04 Running jupyter-pytorch on JS2: $ docker run --gpus all -p 8888:8888 -e REDIRECT_URL=http://localhost:8888 harbor.cyverse.org/vice/jupyter/pytorch:latest Adding tools to CyVerse \u00b6 Running an app in CyVerse is a 2 step process: 1. Adding a Tool 2. Using the Tool for the App Adding the Tool: \u00b6 Navigate to Apps > Tools > More Actions > Add Tools Add Name, Version (for CyVerse, first block) Either paste your docker image DockerHub URL or the <repo>/<image> ; add the <tag> in the Tag field. Specifically for RStudio, add Port 80 (due to reverse port). Creating the App: \u00b6 Navigate to Apps, and select Create then Create New App Add a unique App Name and Description. Click on Select Tool and select the tool you just created, and select interactive . Click Next (and next, and next...), then click Save and Launch. Your app should be avaiable to access in a few minutes. Singularity \u00b6 A solution to run containers on a cluster/HPC. Q: Why can't I run docker on the HPC? A: Docker requires root proviledges ( sudo ), and these cannot be executed on the HPC. The HPC is a shared space, imagine everyone having root priviledges! All the possible unforseen changes to the system made by everyone will lead to a system error. Using the UArizona HPC as an example: with terminal, ssh into the HPC with ssh </user>@hpc.arizona.edu , enter your password (and DUO auth). Choose your cluser node (Puma is suggested, accessed with puma ), type interactive and wait for your interactive session to spin up. Follow lesson examples at TACC singularity basics . Submitting jobs on an HPC system running the SLURM manager \u00b6 SLURM is a job scheduler/manager, and in order to run jobs on a cluster, you need to write a shell script ( .sh ) with a list of instructions. Q: Why can't I just run things on my node? A: As HPCs are shared machines, there are numbers of submitted concurrent jobs. Ultimately, the best way to run all of these jobs, is to schedule when to run all jobs submitted by users. Said script requires a list of SLURM instructions (\"batch commands\") prior to the actual job commands. Following is an example of said script: #!/bin/bash #SBATCH -J myjob # Job name #SBATCH -o output.%j # Name of stdout output file (%j expands to jobId) #SBATCH -p development # Queue name #SBATCH -N 1 # Total number of nodes requested (56 cores/node) #SBATCH -n 1 # Total number of mpi tasks requested #SBATCH -t 02:00:00 # Run time (hh:mm:ss) - 4 hours #SBATCH --reservation ContainerTraining # a reservation only active during the training singularity exec docker://python:latest /usr/local/bin/python For more examples see the TACC documentation on batch commands . Open Science Grid : Distributed High Throughput Computing \u00b6 OSG : consortium of rsearchers and institutions who share compute and data resources for distributed HTC in support for open science. (OSG requires you to sign up through a meeting*) Using Docker images/containres offers consistent, complete and reproducible environment across the OSG: \"running containers on the OSG is like bringing your own utensils to someone else's kitchen\". OSG keeps dockers in extracted format in the cvmfs ( official cvmfs documentation ), making it \"snappier\" to load and run the image. IT stores each of the layers in an uncompressed format, thus you users only spin up the necessary layers. Running Singularity \u00b6 Installing Singularity \u00b6 Installing Singularity requires Go: see instructions in the Singularity documentation . In summary: $ wget https://go.dev/dl/go1.18.2.linux-amd64.tar.gz $ sudo tar -C /usr/local -xzvf go$VERSION.1.18.2-amd64.tar.gz $ echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && source ~/.bashrc After installing Go, you can install Singularity: $ export VERSION=3.9.5 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \\ tar -xzf singularity-ce-${VERSION}.tar.gz && \\ cd singularity-ce-${VERSION} $ ./mconfig && \\ make -C builddir && \\ sudo make -C builddir install Running Docker Images with Singularity \u00b6 Singularity is able to create a .sif (singularity Image) from a Docker Image. To pull Docker Images with Singularity do $ singularity pull --name <name>.sif docker://<docker_user>/<container>:<version> To run a Docker Image with Singularity do $ singularity run docker://<docker_user>/<container>:<version> Singularity also comes with an exec function, which executes a command within the container singularity exec --bind /data:/mnt <my_container>.sif ls /mnt In the above example, we see the --bind flag, which mounts a volume (the /data folder) to the container. The command also lists ( ls ) the content of the data folder. Reminder : it is important that you don't forget the version!! Writing the Singularity recipie \u00b6 An example of a recipie is here below: Bootstrap: docker From: ubuntu:16.04 %help This is the container help section. %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL=C export PATH=/usr/games:$PATH %runscript fortune | cowsay | lolcat Singularity, like Docker, has a list of instructions. Here is the equivalent of Docker instructions: Dockerfile FROM ARG COPY RUN ENV LABEL CMD Singularity Bootstrap: %post %files %post %environment %label %runscript To build the image, do $ sudo singularity build <name_of_container>.sif <singularity recipie> Notice how the above command uses sudo ! Singularity has its own Hub (like DockerHub ): https://cloud.sylabs.io/ The SyLabs singularity allows to build singularity Images directly in the GUI online. Kubernetes (K8s) \u00b6 Q: What is Kubernentes? A: Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Here is an image of how a K8s cluseter looks like: Image: A control plane (could be made of a single machine) manages and distributes jobs to worker nodes. Installing K8s (on a UNIX system) \u00b6 Installation of K8s has a number of documentations, here are the basic instructions. $ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl $ chmod +x kubectl $ sudo mv ./kubectl /usr/local/bin/kubectl Initiating K8s head \u00b6 K8s uses kubectl to control various pods. Prior to running kubectl , during the installation, a folder named kube was created in your ~/. folder. There is an unpopulated config file in ~/.kube/ - for today's example, populate the config with apiVersion: v1 clusters: - cluster: certificate-authority-data: <authority data> server: <IP:port> name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: <certificate data> client-key-data: <key data> Warning The above code has been modified <data> to protect information. The K8s head isn't online at the moment, and certificates change between config files. Interacting with Pods ( basically a container ) \u00b6 To launch a pod you require a pod config file. An example is apiVersion: v1 kind: Pod metadata: name: pod-<change-this-to-your-username> spec: containers: - name: mypod image: alpine:3.14 resources: limits: memory: 100Mi cpu: 100m requests: memory: 100Mi cpu: 100m command: [\"sh\", \"-c\", \"sleep infinity\"] To create your pod, do kubetcl create -f pod1.yml To initiate your pod, do kubectl exec -it pod-<change-this-to-your-username> /bin/sh The above command will enter the shell of the pod. You can see what pods are running with kubectl get pods To remove a pod, do kubectl delete -f pod1.yml These are the absolute basics of Kubernentes. To read more, refer to the Kubernetes official documentation (and other massive number of online resources, Google is your friend!) . CACAO (alpha stage) \u00b6 CACAO is \"An event-driven multi-cloud service that enables resarchers and educators to effortlessly manage, scale and share their tools and workflows to any research-capable cloud using declarative templates\" - Edwin Skidmore Accessing CACAO (creating an SSH key) \u00b6 CACAO can be access at https://js2.cyverse.org/ . You can access the CACAO deployments with an SSH key (or web browser). In order to create an ssh key, do $ ssh-keygen -t rsa -b 8192 -f joking Creates a file public key (your keys are usually in ~/.ssh/ ). To create a Kubernetes cluster, select \"+ Deployment\" and ensure that you're choosing Ubuntu 20 as your OS. To launch a multi VM running JupyterHub, choose \"+ Deployment\" and \"launch a multi-vm zero-to-jupyterhub\". Note: CACAO is still in Alpha phase . Read more on CACAO here .","title":"Camp Discussion and Notes"},{"location":"resources/post/#general-notes-and-discussions","text":"This section summarizes general notes, discussions and questions raised during both the Basics and Advanced section of Container Camp. Before you Continue This page is for archival and informative use only, reproduction (such as access to JetStream2 allocations, CodeSpaces) is limited.","title":"General Notes and Discussions"},{"location":"resources/post/#basics","text":"","title":"Basics"},{"location":"resources/post/#queries","text":"Q: What is an image? A: A file that lives in the cache on your computer... where 'cache' can be thought of like a desk. It's faster to retrieve a file from your desk than from the filing cabinet Q: What is a container? A: It's a virtualized run-time environment which starts from the image. A docker image is what you build. It has all of the software necessary to run the code. The Container is when you \"activate\" the image, an extra layer where you can work on top of the software you put in the image. The built image will contain its own OS - it will make no difference where you build your container. When you build an image, you can specify the architecture of the machine you want it to run on. Manage resources for your container by using commands to stop, pause, restart, remove a container. Q: How do I work with data and containers? A: Containers do not contain large amounts of data, as these will take space in the writable layer of the container (see above image). Instead, it is suggested to use Volumes as a best practice. A Volume is a directory that lives outside of the container that can be attached to said container. Once attached, the contents of the directory will be viewable and accessible to the container. In order to attach the volume, one must specify the directory on the computer AND the destination folder, separated by a colon (:). The format is as follows -v <directory on computer>:<directory in container> . Q: Ports. What are ports and why do we need them? A: Ports are where network connections start and end. These are not physical, and these allow software to connect to a network. Our computers run hundreds of processes, and chance is a lot of these processes require a connection to the network. In order for these processes to access the network, ports are required. A single process is assigned a single port - and this is how these software can connect to the internet. The reason why we need to open a port for Docker, is because the Docker container is trying to communicate with the network, however it requires us, the user, to assign a port for it. Without us assigning the port to the Docker container, the connection to the network cannot happen. More information on ports can be found at: - CloudFlare: Understanding Ports - Internet Assigned Numbers Authority Port List - List of TCP and UDP port numbers Q: What is Docker's relationship with things like networking, ethernet, USB, HDMI? Are these things naturally closed or naturally open? Are there interfaces that you cannot access from docker? A: Docker is able to do Networking as it has its own networking subsystem (and commands). As this is an advanced topic, let me direct you to the official networking documentation here: https://docs.docker.com/network/ Q: Is there a way to emulate a display in Docker so that certain rendering code (like the plotting libraries in python) don't break when run in a container? A: [unsure if this is what you were looking for] Docker is able to run GUI applications; A display can be specified using the -e (environment) flag such as -e DISPLAY=$DISPLAY . $DISPLAY can usually be specified to $0 , targeting the primary display. This little blog post may be able to help you further. Q: What should we know about accessing GPUs from docker? Won't the hardware you're running on affect the runnability of a container, despite the containerization of the image? A: NVIDIA Docker now uses a special flag for Docker (rather than needing its own installation) https://github.com/NVIDIA/nvidia-docker . Q: Is malware ever a problem with Dockerfiles? Can you run a malicious image? A: It seems that Docker (and Kubernetes) related malware are now a thing. From personal experience, I have never run into issues. Q: If containers are software, why should I bother using a container instead of the software itself? A: Containers offer 3 great solutions to common problems: (1) reproducibility (2) version control. Docker images contain all of the required software in the form of layers, including specific versions of libraries. This allows to easily share your image and software without worring about collaborators having to install the correct software and version. (3) portability, so you can run it anywhere.","title":"Queries"},{"location":"resources/post/#dockerfiles-hands-on","text":"Go to an example directory in the intro2docker repository with a Dockerfile cd alpine Build the container using the build command. Make sure to include the . docker build -t test/alpine . Note The container should get the default latest tag if it is not specified in the docker build command with the name test/alpine Start the container using the run command. docker run --rm test/alpine:latest To run the container and override its CMD, it will use its own shell sh : docker run -it --rm test/alpine:latest sh Dockerfiles are like your recipie book, and like every recipie book you have instructions. The instructions aren't for the user, but for Docker itself. These instruction are the capitalized commands you see at the beginning of lines, and these tell Docker what to do: Instruction Command FROM Instructs to use a specific Docker image LABEL Adds metadata to the image RUN Executes a specific command ENV Sets environmental variables COPY Copies a file from a specified location to the image CMD Sets a command to be executed when running a container ENTRYPOINT Configures and run a container as an executable USER Used to set User specific information EXPOSE exposes a specific port *the above list is nonexhaustive, visit the official Docker documentation for more information and further instructions.","title":"Dockerfiles Hands-on"},{"location":"resources/post/#pushing-to-dockerhub","text":"Build your docker image with docker build -t <Dockerhub username>/<Docker image>:<version> . then, log in to Docker with docker login -u <username> This will then ask for your Password; type in your password ( it will NOT show you the password ). If it does not login automatically, please follow the instructions here . Once you have logged in, push your docker to the DockerHub registry with docker push <Dockerhub username>/<Docker image>:<version> Your newly built Docker image now lives on DockerHub. You can view it at https://hub.docker.com/r/<username>/<Docker image>","title":"Pushing to DockerHub"},{"location":"resources/post/#assigning-users","text":"Create a new folder called ubuntu mkdir ubuntu Change into the folder cd ubuntu Create a Dockerfile ARG VERSION=18.04 FROM ubuntu:$VERSION RUN apt-get update -y && apt-get install -y gnupg wget python3 python3-distro && \\ wget -qO - https://packages.irods.org/irods-signing-key.asc | apt-key add - && \\ echo \"deb [arch=amd64] https://packages.irods.org/apt/ $(lsb_release -sc) main\" >> /etc/apt/sources.list.d/renci-irods.list && \\ apt-get update && apt-get install irods-icommands -y COPY irods_environment.json /home/ubuntu/.irods/ RUN useradd ubuntu && \\ chown -R ubuntu:ubuntu /home/ubuntu USER ubuntu Create a file called irods_environment.json { \"irods_host\": \"data.cyverse.org\", \"irods_port\": 1247, \"irods_zone_name\": \"iplant\" } Build the container using your dockerhub username docker build -t <yourusername>/ubuntu-irods:18.04 . Run with docker run -it --rm <yourusername>/ubuntu-irods:18.04 Q: What did we do? A: We created an image whose the user is specified. Q: Why? A: When creating interactive containers, these containers are not built with root privileges. Assigning a specific user helps with defining the priviledges you want users to have. Q: Wait, what? A: When pulling a base image with the FROM instruction, sometimes the user is already defined. The only user with priviledges will be that already defined user. Therefore, in order to have the \"right\" priviledges, you have to assign the right user in your Dockerfile .","title":"Assigning Users"},{"location":"resources/post/#rstudio-dockerfile","text":"The above steps where necessary in order to understand why in this following step we need to define a user. Navigate to rstudio/verse with cd rstudio/verse and create a Dockerfile: FROM rocker/verse:4.2.0 # Install your own stuff below RUN install2.r --error \\ # Added Packages PerformanceAnalytics \\ boot \\ devtools \\ dlm \\ dplyr \\ foreign \\ lubridate \\ plotly \\ truncreg \\ ggridges Build the Docker image with: docker build -t <yourusername>/rstudio:tag . Execute with docker run --rm -p 8787:8787 -e DISABLE_AUTH=true <username>/rstudio:<version>","title":"RStudio Dockerfile"},{"location":"resources/post/#docker-commands-cheat-sheets","text":"https://cyverse-learning-materials.github.io/container-camp/docker/intro/#docker-commands https://dockerlabs.collabnix.com/docker/cheatsheet/ https://github.com/wsargent/docker-cheat-sheet","title":"Docker Commands Cheat Sheets:"},{"location":"resources/post/#advanced","text":"","title":"Advanced"},{"location":"resources/post/#github-actions","text":"GitHub not only can host your code, but it can be used to create Docker images right within the GitHub repo using GitHub Actions. You first require to connect your DockerHub and your GitHub accounts: Navigate to Settings (in your repository) Select Secrets > Actions. Add a New Repository Secret and name it \"DOCKER_HUB_USERNAME\"; Paste here your Docker Account name. Add a New Repository Secret and name it \"DOCKER_HUB_ACCESS_TOKEN\", and DO NOT CLOSE IT Navigate to DockerHub and click Account Settings > Security > New Access Token Add a name you can recognize and COPY the password and add it to \"DOCKER_HUB_ACCESS_TOKEN\" (step 4). Now save and close. Your GitHub and DockerHub are connected! To create a new GitHub Action, navigate to the Action button on the top of your repository (create a repository first!), and click on New Workflow ; select Docker Image . Delete the template pase the following: name: <NAME OF ACTION> on: push: branches: [ main ] pull_request: branches: [ main ] jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Login to Docker Hub uses: docker/login-action@v1 with: username: ${{ secrets.DOCKER_HUB_USERNAME }} password: ${{ secrets.DOCKER_HUB_ACCESS_TOKEN }} - name: Build and push uses: docker/build-push-action@v2 with: context: . file: ./Dockerfile push: true tags: ${{ secrets.DOCKER_HUB_USERNAME }}/<NAME OF IMAGE ON DOCKERHUB>:<VERSION> - For the above code, modify all content in \" as you deem necessary. Remember to add a Dockerfile to your main repository!","title":"Github Actions:"},{"location":"resources/post/#working-on-the-cloud","text":"We discussed working with JetStream2 . - Access JetStream2 exoshpere - Access JetStream2 CyVerse (CACAO) Summary of running JS2 GPU Virtual Machines (VM): Navigate to : https://jetstream2.exosphere.app/ Click Create > Instance Select Ubuntu 20.04 Select a g3.small Instance and select (enable web desktop), click create. Wait for VM to spin up, test for functionality by: 1. click on web shell 2. From web shell, run nvidia-smi (if it returns a table, you are good to go) Running xpra on JS2: $ export DISPLAY=:0 $ xinit & $ docker run --gpus all --rm -it -p 9876:9876 -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY -e XAUTHORITY -e QT_X11_NO_MITSHM=1 -e NVIDIA_DRIVER_CAPABILITIES=all harbor.cyverse.org/vice/xpra/cudagl:20.04 Running jupyter-pytorch on JS2: $ docker run --gpus all -p 8888:8888 -e REDIRECT_URL=http://localhost:8888 harbor.cyverse.org/vice/jupyter/pytorch:latest","title":"Working on the Cloud"},{"location":"resources/post/#adding-tools-to-cyverse","text":"Running an app in CyVerse is a 2 step process: 1. Adding a Tool 2. Using the Tool for the App","title":"Adding tools to CyVerse"},{"location":"resources/post/#adding-the-tool","text":"Navigate to Apps > Tools > More Actions > Add Tools Add Name, Version (for CyVerse, first block) Either paste your docker image DockerHub URL or the <repo>/<image> ; add the <tag> in the Tag field. Specifically for RStudio, add Port 80 (due to reverse port).","title":"Adding the Tool:"},{"location":"resources/post/#creating-the-app","text":"Navigate to Apps, and select Create then Create New App Add a unique App Name and Description. Click on Select Tool and select the tool you just created, and select interactive . Click Next (and next, and next...), then click Save and Launch. Your app should be avaiable to access in a few minutes.","title":"Creating the App:"},{"location":"resources/post/#singularity","text":"A solution to run containers on a cluster/HPC. Q: Why can't I run docker on the HPC? A: Docker requires root proviledges ( sudo ), and these cannot be executed on the HPC. The HPC is a shared space, imagine everyone having root priviledges! All the possible unforseen changes to the system made by everyone will lead to a system error. Using the UArizona HPC as an example: with terminal, ssh into the HPC with ssh </user>@hpc.arizona.edu , enter your password (and DUO auth). Choose your cluser node (Puma is suggested, accessed with puma ), type interactive and wait for your interactive session to spin up. Follow lesson examples at TACC singularity basics .","title":"Singularity"},{"location":"resources/post/#submitting-jobs-on-an-hpc-system-running-the-slurm-manager","text":"SLURM is a job scheduler/manager, and in order to run jobs on a cluster, you need to write a shell script ( .sh ) with a list of instructions. Q: Why can't I just run things on my node? A: As HPCs are shared machines, there are numbers of submitted concurrent jobs. Ultimately, the best way to run all of these jobs, is to schedule when to run all jobs submitted by users. Said script requires a list of SLURM instructions (\"batch commands\") prior to the actual job commands. Following is an example of said script: #!/bin/bash #SBATCH -J myjob # Job name #SBATCH -o output.%j # Name of stdout output file (%j expands to jobId) #SBATCH -p development # Queue name #SBATCH -N 1 # Total number of nodes requested (56 cores/node) #SBATCH -n 1 # Total number of mpi tasks requested #SBATCH -t 02:00:00 # Run time (hh:mm:ss) - 4 hours #SBATCH --reservation ContainerTraining # a reservation only active during the training singularity exec docker://python:latest /usr/local/bin/python For more examples see the TACC documentation on batch commands .","title":"Submitting jobs on an HPC system running the SLURM manager"},{"location":"resources/post/#open-science-grid-distributed-high-throughput-computing","text":"OSG : consortium of rsearchers and institutions who share compute and data resources for distributed HTC in support for open science. (OSG requires you to sign up through a meeting*) Using Docker images/containres offers consistent, complete and reproducible environment across the OSG: \"running containers on the OSG is like bringing your own utensils to someone else's kitchen\". OSG keeps dockers in extracted format in the cvmfs ( official cvmfs documentation ), making it \"snappier\" to load and run the image. IT stores each of the layers in an uncompressed format, thus you users only spin up the necessary layers.","title":"Open Science Grid: Distributed High Throughput Computing"},{"location":"resources/post/#running-singularity","text":"","title":"Running Singularity"},{"location":"resources/post/#installing-singularity","text":"Installing Singularity requires Go: see instructions in the Singularity documentation . In summary: $ wget https://go.dev/dl/go1.18.2.linux-amd64.tar.gz $ sudo tar -C /usr/local -xzvf go$VERSION.1.18.2-amd64.tar.gz $ echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && source ~/.bashrc After installing Go, you can install Singularity: $ export VERSION=3.9.5 && # adjust this as necessary \\ wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \\ tar -xzf singularity-ce-${VERSION}.tar.gz && \\ cd singularity-ce-${VERSION} $ ./mconfig && \\ make -C builddir && \\ sudo make -C builddir install","title":"Installing Singularity"},{"location":"resources/post/#running-docker-images-with-singularity","text":"Singularity is able to create a .sif (singularity Image) from a Docker Image. To pull Docker Images with Singularity do $ singularity pull --name <name>.sif docker://<docker_user>/<container>:<version> To run a Docker Image with Singularity do $ singularity run docker://<docker_user>/<container>:<version> Singularity also comes with an exec function, which executes a command within the container singularity exec --bind /data:/mnt <my_container>.sif ls /mnt In the above example, we see the --bind flag, which mounts a volume (the /data folder) to the container. The command also lists ( ls ) the content of the data folder. Reminder : it is important that you don't forget the version!!","title":"Running Docker Images with Singularity"},{"location":"resources/post/#writing-the-singularity-recipie","text":"An example of a recipie is here below: Bootstrap: docker From: ubuntu:16.04 %help This is the container help section. %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL=C export PATH=/usr/games:$PATH %runscript fortune | cowsay | lolcat Singularity, like Docker, has a list of instructions. Here is the equivalent of Docker instructions: Dockerfile FROM ARG COPY RUN ENV LABEL CMD Singularity Bootstrap: %post %files %post %environment %label %runscript To build the image, do $ sudo singularity build <name_of_container>.sif <singularity recipie> Notice how the above command uses sudo ! Singularity has its own Hub (like DockerHub ): https://cloud.sylabs.io/ The SyLabs singularity allows to build singularity Images directly in the GUI online.","title":"Writing the Singularity recipie"},{"location":"resources/post/#kubernetes-k8s","text":"Q: What is Kubernentes? A: Kubernetes is an open-source system for automating deployment, scaling, and management of containerized applications. Here is an image of how a K8s cluseter looks like: Image: A control plane (could be made of a single machine) manages and distributes jobs to worker nodes.","title":"Kubernetes (K8s)"},{"location":"resources/post/#installing-k8s-on-a-unix-system","text":"Installation of K8s has a number of documentations, here are the basic instructions. $ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl $ chmod +x kubectl $ sudo mv ./kubectl /usr/local/bin/kubectl","title":"Installing K8s (on a UNIX system)"},{"location":"resources/post/#initiating-k8s-head","text":"K8s uses kubectl to control various pods. Prior to running kubectl , during the installation, a folder named kube was created in your ~/. folder. There is an unpopulated config file in ~/.kube/ - for today's example, populate the config with apiVersion: v1 clusters: - cluster: certificate-authority-data: <authority data> server: <IP:port> name: default contexts: - context: cluster: default user: default name: default current-context: default kind: Config preferences: {} users: - name: default user: client-certificate-data: <certificate data> client-key-data: <key data> Warning The above code has been modified <data> to protect information. The K8s head isn't online at the moment, and certificates change between config files.","title":"Initiating K8s head"},{"location":"resources/post/#interacting-with-pods-basically-a-container","text":"To launch a pod you require a pod config file. An example is apiVersion: v1 kind: Pod metadata: name: pod-<change-this-to-your-username> spec: containers: - name: mypod image: alpine:3.14 resources: limits: memory: 100Mi cpu: 100m requests: memory: 100Mi cpu: 100m command: [\"sh\", \"-c\", \"sleep infinity\"] To create your pod, do kubetcl create -f pod1.yml To initiate your pod, do kubectl exec -it pod-<change-this-to-your-username> /bin/sh The above command will enter the shell of the pod. You can see what pods are running with kubectl get pods To remove a pod, do kubectl delete -f pod1.yml These are the absolute basics of Kubernentes. To read more, refer to the Kubernetes official documentation (and other massive number of online resources, Google is your friend!) .","title":"Interacting with Pods (basically a container)"},{"location":"resources/post/#cacao-alpha-stage","text":"CACAO is \"An event-driven multi-cloud service that enables resarchers and educators to effortlessly manage, scale and share their tools and workflows to any research-capable cloud using declarative templates\" - Edwin Skidmore","title":"CACAO (alpha stage)"},{"location":"resources/post/#accessing-cacao-creating-an-ssh-key","text":"CACAO can be access at https://js2.cyverse.org/ . You can access the CACAO deployments with an SSH key (or web browser). In order to create an ssh key, do $ ssh-keygen -t rsa -b 8192 -f joking Creates a file public key (your keys are usually in ~/.ssh/ ). To create a Kubernetes cluster, select \"+ Deployment\" and ensure that you're choosing Ubuntu 20 as your OS. To launch a multi VM running JupyterHub, choose \"+ Deployment\" and \"launch a multi-vm zero-to-jupyterhub\". Note: CACAO is still in Alpha phase . Read more on CACAO here .","title":"Accessing CACAO (creating an SSH key)"},{"location":"resources/singularity/","text":"Useful Resources related to Singularity \u00b6 SyLabs Singularity Community Edition (CE) Singularity-HPC Autamus Singularity Image Repository Carpentries Singularity Incubator Campus HPC resources for Singularity \u00b6 OpenScienceGrid Singularity Documentation TACC HPC Singularity Introduction TACC Containers .pdf NIH HPC University of Arizona Singularity Tutorial","title":"Singularity"},{"location":"resources/singularity/#useful-resources-related-to-singularity","text":"SyLabs Singularity Community Edition (CE) Singularity-HPC Autamus Singularity Image Repository Carpentries Singularity Incubator","title":"Useful Resources related to Singularity"},{"location":"resources/singularity/#campus-hpc-resources-for-singularity","text":"OpenScienceGrid Singularity Documentation TACC HPC Singularity Introduction TACC Containers .pdf NIH HPC University of Arizona Singularity Tutorial","title":"Campus HPC resources for Singularity"},{"location":"singularity/advanced/","text":"Singularity CLI continued \u00b6 Build Singularity .sif images \u00b6 Similar to Docker which uses a Dockerfile to build its images, Singularity uses a file called Singularity . Important Singularity images use the .sif extension and appear as a single compressed file, versus Docker which uses many cached file layers to create a single image. .sif files also use layers, but these are not apparent. .sif images are cached in the folder where you build them, or designate them. When building from docker:// the Docker image layers are downloaded and cached by Singularity in a /.Singularity folder on your build environment. build \u00b6 Create a .sif image using a Docker image as the template \u00b6 As we've learned from the HPC and HTC groups, building Singularity images is not necessarily the most accessible and reproducible method for managing containers. Most groups suggest that you build your containers with Docker, and host them on a Docker Registry first. The market dominance of Docker and its wide acceptance as a container format, has led us to use Singularity with Docker in most cases. We've already covered how you can pull an existing image from Docker Hub, but we can also build a Singularity image from the Docker Hub using the build command: $ sudo singularity build --sandbox ubuntu-latest/ docker://ubuntu Test the new .sif image: $ singularity shell --writable ubuntu-latest/ Singularity ubuntu-latest.sif:~> apt-get update Does it work? $ sudo singularity shell ubuntu-latest.sif Singularity: Invoking an interactive shell within container... Singularity ubuntu-latest.sif:~> apt-get update When I try to install software to the image without sudo it is denied, because root is the owner inside the container. When I use sudo I can install software into the container. The software remains in the sandbox container after closing the container and restart. In order to make these changes permanant, I need to rebuild the sandbox as a .sif image $ sudo singularity build ubuntu-latest.sif ubuntu-latest/ Creating Singularity .sif from scratch \u00b6 The contents of the Singularity file differ from Dockerfile %help - create text for a help menu associated with your container %setup - executed on the host system outside of the container, after the base OS has been installed. %files - copy files from your host system into the container %labels - store metadata in the container %environment - loads environment variables at the time the container is run (not built) %post - set environment variables during the build %runscript - executes a script when the container runs %test - runs a test on the build of the container Dockerfile FROM ARG COPY RUN ENV LABEL CMD Singularity Bootstrap: %post %files %post %environment %label %runscript Writing the Singularity file \u00b6 SyLabs User-Guide A Singularity file can be hosted on GitHub and will be auto-detected by Singularity-Hub when you set up your container Collection. When you are building locally, you can name the Singularity file whatever you wish, but a better practice is to put it in a specified directory and name it Singularity . Building your own containers requires that you have sudo privileges - therefore you'll need to develop these on your local machine or on a VM that you can gain root access on. Header \u00b6 The top of the file, selects the base OS for the container, just like FROM in Docker. Bootstrap: references another registry (e.g. docker for DockerHub, debootstrap , or library for Sylabs Container Library ). From: selects the tag name. Using debootstrap with a build that uses a mirror: BootStrap: debootstrap OSVersion: jammy MirrorURL: http://us.archive.ubuntu.com/ubuntu/ Using CentOS-like container: Bootstrap: yum OSVersion: 7 MirrorURL: http://mirror.centos.org/centos-7/7/os/x86_64/ Include:yum Note: to use yum to build a container you should be operating on a RHEL system, or an Ubuntu system with yum installed. Using a localimage to build: Bootstrap: localimage From: /path/to/container/file/or/directory The container registries which Singularity uses are listed in the Introduction Section 3.1 . The Singularity file uses sections to specify the dependencies, environmental settings, and runscripts when it builds. help \u00b6 %help section can be as verbose as you want Bootstrap: docker From: ubuntu %help This is the container help section. setup \u00b6 %setup commands are executed on the localhost system outside of the container - these files could include necessary build dependencies. We can copy files to the $SINGULARITY_ROOTFS file system can be done during setup files \u00b6 %files include any files that you want to copy from your localhost into the container. post \u00b6 %post includes all of the environment variables and dependencies that you want to see installed into the container at build time. %post apt-get -y update apt-get -y install fortune cowsay lolcat environment \u00b6 %environment includes the environment variables which we want to be run when we start the container %environment export LC_ALL=C export PATH=/usr/games:$PATH runscript \u00b6 %runscript does what it says, it executes a set of commands when the container is run. %runscript fortune | cowsay | lolcat labels \u00b6 %labels are used similar to Dockerfile which allow you to add metadata to the image file in key-value pairs %labels Author Your Name Email email@address.org Version v2022 CustomLabel statement here Labels can be read using the inspect command Example File \u00b6 Example Singularity file bootstrapping an Ubuntu (22.04) image. Bootstrap: docker From: ubuntu:22.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL=C export PATH=/usr/games:$PATH %runscript fortune | cowsay | lolcat %labels Maintainer Your Name Version v2022 Build the container: singularity build cowsay.sif Singularity Run the container: singularity run cowsay.sif Sandbox \u00b6 Sandboxing is another approach to building up a container image.","title":"Advanced use of Singularity"},{"location":"singularity/advanced/#singularity-cli-continued","text":"","title":" Singularity CLI continued"},{"location":"singularity/advanced/#build-singularity-sif-images","text":"Similar to Docker which uses a Dockerfile to build its images, Singularity uses a file called Singularity . Important Singularity images use the .sif extension and appear as a single compressed file, versus Docker which uses many cached file layers to create a single image. .sif files also use layers, but these are not apparent. .sif images are cached in the folder where you build them, or designate them. When building from docker:// the Docker image layers are downloaded and cached by Singularity in a /.Singularity folder on your build environment.","title":"Build Singularity .sif images"},{"location":"singularity/advanced/#build","text":"","title":" build"},{"location":"singularity/advanced/#create-a-sif-image-using-a-docker-image-as-the-template","text":"As we've learned from the HPC and HTC groups, building Singularity images is not necessarily the most accessible and reproducible method for managing containers. Most groups suggest that you build your containers with Docker, and host them on a Docker Registry first. The market dominance of Docker and its wide acceptance as a container format, has led us to use Singularity with Docker in most cases. We've already covered how you can pull an existing image from Docker Hub, but we can also build a Singularity image from the Docker Hub using the build command: $ sudo singularity build --sandbox ubuntu-latest/ docker://ubuntu Test the new .sif image: $ singularity shell --writable ubuntu-latest/ Singularity ubuntu-latest.sif:~> apt-get update Does it work? $ sudo singularity shell ubuntu-latest.sif Singularity: Invoking an interactive shell within container... Singularity ubuntu-latest.sif:~> apt-get update When I try to install software to the image without sudo it is denied, because root is the owner inside the container. When I use sudo I can install software into the container. The software remains in the sandbox container after closing the container and restart. In order to make these changes permanant, I need to rebuild the sandbox as a .sif image $ sudo singularity build ubuntu-latest.sif ubuntu-latest/","title":"Create a .sif image using a Docker image as the template"},{"location":"singularity/advanced/#creating-singularity-sif-from-scratch","text":"The contents of the Singularity file differ from Dockerfile %help - create text for a help menu associated with your container %setup - executed on the host system outside of the container, after the base OS has been installed. %files - copy files from your host system into the container %labels - store metadata in the container %environment - loads environment variables at the time the container is run (not built) %post - set environment variables during the build %runscript - executes a script when the container runs %test - runs a test on the build of the container Dockerfile FROM ARG COPY RUN ENV LABEL CMD Singularity Bootstrap: %post %files %post %environment %label %runscript","title":"Creating Singularity .sif from scratch"},{"location":"singularity/advanced/#writing-the-singularity-file","text":"SyLabs User-Guide A Singularity file can be hosted on GitHub and will be auto-detected by Singularity-Hub when you set up your container Collection. When you are building locally, you can name the Singularity file whatever you wish, but a better practice is to put it in a specified directory and name it Singularity . Building your own containers requires that you have sudo privileges - therefore you'll need to develop these on your local machine or on a VM that you can gain root access on.","title":"Writing the Singularity file"},{"location":"singularity/advanced/#header","text":"The top of the file, selects the base OS for the container, just like FROM in Docker. Bootstrap: references another registry (e.g. docker for DockerHub, debootstrap , or library for Sylabs Container Library ). From: selects the tag name. Using debootstrap with a build that uses a mirror: BootStrap: debootstrap OSVersion: jammy MirrorURL: http://us.archive.ubuntu.com/ubuntu/ Using CentOS-like container: Bootstrap: yum OSVersion: 7 MirrorURL: http://mirror.centos.org/centos-7/7/os/x86_64/ Include:yum Note: to use yum to build a container you should be operating on a RHEL system, or an Ubuntu system with yum installed. Using a localimage to build: Bootstrap: localimage From: /path/to/container/file/or/directory The container registries which Singularity uses are listed in the Introduction Section 3.1 . The Singularity file uses sections to specify the dependencies, environmental settings, and runscripts when it builds.","title":" Header"},{"location":"singularity/advanced/#help","text":"%help section can be as verbose as you want Bootstrap: docker From: ubuntu %help This is the container help section.","title":" help"},{"location":"singularity/advanced/#setup","text":"%setup commands are executed on the localhost system outside of the container - these files could include necessary build dependencies. We can copy files to the $SINGULARITY_ROOTFS file system can be done during setup","title":" setup"},{"location":"singularity/advanced/#files","text":"%files include any files that you want to copy from your localhost into the container.","title":" files"},{"location":"singularity/advanced/#post","text":"%post includes all of the environment variables and dependencies that you want to see installed into the container at build time. %post apt-get -y update apt-get -y install fortune cowsay lolcat","title":" post"},{"location":"singularity/advanced/#environment","text":"%environment includes the environment variables which we want to be run when we start the container %environment export LC_ALL=C export PATH=/usr/games:$PATH","title":" environment"},{"location":"singularity/advanced/#runscript","text":"%runscript does what it says, it executes a set of commands when the container is run. %runscript fortune | cowsay | lolcat","title":" runscript"},{"location":"singularity/advanced/#labels","text":"%labels are used similar to Dockerfile which allow you to add metadata to the image file in key-value pairs %labels Author Your Name Email email@address.org Version v2022 CustomLabel statement here Labels can be read using the inspect command","title":" labels"},{"location":"singularity/advanced/#example-file","text":"Example Singularity file bootstrapping an Ubuntu (22.04) image. Bootstrap: docker From: ubuntu:22.04 %post apt-get -y update apt-get -y install fortune cowsay lolcat %environment export LC_ALL=C export PATH=/usr/games:$PATH %runscript fortune | cowsay | lolcat %labels Maintainer Your Name Version v2022 Build the container: singularity build cowsay.sif Singularity Run the container: singularity run cowsay.sif","title":"Example File"},{"location":"singularity/advanced/#sandbox","text":"Sandboxing is another approach to building up a container image.","title":"Sandbox"},{"location":"singularity/hpc/","text":"Singularity and High Performance Computing \u00b6 High Performance Computing resources fill an important role in research computing and can support container execution through runtimes such as Singularity or, hopefully soon, rootless Docker, among other options. Conducting analyses on HPC clusters happens through different patterns of interaction than running analyses on a cloud VM. When you login, you are on a node that is shared with lots of people, typically called the \"login node\". Trying to run jobs on the login node is not \"high performance\" at all (and will likely get you an admonishing email from the system administrator). Login nodes are intended to be used for moving files, editing files, and launching jobs. Importantly, most jobs run on an HPC cluster are neither interactive , nor realtime . When you submit a job to the scheduler, you must tell it what resources you need (e.g. how many nodes, how much RAM, what type of nodes, and for how long) in addition to what you want to run. Then the scheduler finally has resources matching your requirements, it runs the job for you. If your request is very large, or very long, you may never make it out of the queue. For example, on a VM if you run the command: singularity exec docker://python:latest /usr/local/bin/python The container will immediately start. On an HPC system, your job submission script would look something like: #!/bin/bash # #SBATCH -J myjob # Job name #SBATCH -o output.%j # Name of stdout output file (%j expands to jobId) #SBATCH -p development # Queue name #SBATCH -N 1 # Total number of nodes requested (68 cores/node) #SBATCH -n 17 # Total number of mpi tasks requested #SBATCH -t 02:00:00 # Run time (hh:mm:ss) - 4 hours module load singularity/3/3.1 singularity exec docker://python:latest /usr/local/bin/python This example is for the Slurm scheduler. Each of the #SBATCH lines looks like a comment to the bash kernel, but the scheduler reads all those lines to know what resources to reserve for you. It is usually possible to get an interactive session as well, by using an interactive flag, -i. Warning Every HPC cluster is a little different, but they almost universally have a \"User's Guide\" that serves both as a quick reference for helpful commands and contains guidelines for how to be a \"good citizen\" while using the system. For TACC's Stampede2 system, see the user guide . For The University of Arizona, see the user guide . How do HPC systems fit into the development workflow? \u00b6 A few things to consider when using HPC systems: Using sudo is not allowed on HPC systems, and building a Singularity container from scratch requires sudo. That means you have to build your containers on a different development system. You can pull a docker image on HPC systems If you need to edit text files, command line text editors don't support using a mouse, so working efficiently has a learning curve. There are text editors that support editing files over SSH. This lets you use a local text editor and just save the changes to the HPC system. These constraints make HPC systems perfectly suitable for execution environments, but currently a limiting choice for a development environment. We usually recommend your local laptop or a VM as a development environment where you can iterate on your code rapidly and test container building and execution. Singularity and MPI \u00b6 Singularity supports MPI fairly well. Since (by default) the network is the same insde and outside the container, the communication between containers usually just works. The more complicated bit is making sure that the container has the right set of MPI libraries. MPI is an open specification, but there are several implementations (OpenMPI, MVAPICH2, and Intel MPI to name three) with some non-overlapping feature sets. If the host and container are running different MPI implementations, or even different versions of the same implementation, hilarity may ensue. The general rule is that you want the version of MPI inside the container to be the same version or newer than the host. You may be thinking that this is not good for the portability of your container, and you are right. Containerizing MPI applications is not terribly difficult with Singularity, but it comes at the cost of additional requirements for the host system. Warning Many HPC Systems, like Stampede2 at TACC and Ocelote at UAHPC, have high-speed, low-latency networks that have special drivers. Infiniband, Ares, and OmniPath are three different specs for these types of networks. When running MPI jobs, if the container doesn't have the right libraries, it won't be able to use those special interconnects to communicate between nodes. Base Docker images \u00b6 Depending on the system you will use, you may have to build your own MPI enabled Singularity images (to get the versions to match). When running at TACC, there is a set of curated Docker images for use in the FROM line of your own containers. You can see a list of availabe images at https://hub.docker.com/u/tacc Specifically, you can use the tacc/tacc-ubuntu18-mvapich2.3-psm2 image to satisfy the MPI architecture and version requirements for running on Stampede2. Because you may have to build your own MPI enabled Singularity images (to get the versions to match), here is a 3.1 compatible example of what it may look like: BootStrap: debootstrap OSVersion: xenial MirrorURL: http://us.archive.ubuntu.com/ubuntu/ %runscript echo \"This is what happens when you run the container...\" %post echo \"Hello from inside the container\" sed -i 's/$/ universe/' /etc/apt/sources.list apt update apt -y --allow-unauthenticated install vim build-essential wget gfortran bison libibverbs-dev libibmad-dev libibumad-dev librdmacm-dev libmlx5-dev libmlx4-dev wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.1.tar.gz tar xvf mvapich2-2.1.tar.gz cd mvapich2-2.1 ./configure --prefix=/usr/local make -j4 make install /usr/local/bin/mpicc examples/hellow.c -o /usr/bin/hellow You could also build in everything in a Dockerfile and convert the image to Singularity at the end. Once you have a working MPI container, invoking it would look something like: mpirun -np 4 singularity exec ./mycontainer.sif /app.py arg1 arg2 This will use the host MPI libraries to run in parallel, and assuming the image has what it needs, can work across many nodes. For a single node, you can also use the container MPI to run in parallel (usually you don't want this) singularity exec ./mycontainer.sif mpirun -np 4 /app.py arg1 arg2 Example Containerized MPI App \u00b6 In your Docker development environment, make a new directory in which to build up a new image and download (or copy and paste) two files in that directory: https://raw.githubusercontent.com/TACC/containers_at_tacc/master/docs/scripts/Dockerfile.mpi https://raw.githubusercontent.com/TACC/containers_at_tacc/master/docs/scripts/pi-mpi.py Take a look at both files. pi-mpi.py is a simple MPI Python script that approximates pi (very inefficiently) by random sampling. Dockerfile.mpi is an updated Dockerfile that uses the TACC base image to satisfy all the MPI requirements on Stampede2. Next, try building the new container. $ docker build -t USERNAME/pi-estimator:0.1-mpi -f Dockerfile.mpi . Don't forget to change USERNAME to your DockerHub username. Once you have successfully built an image, push it up to DockerHub with the docker push command so that we can pull it back down on Stampede2. Running an MPI Container on Stampede2 \u00b6 To test, we can grab an interactive session that has two nodes. That way we can see if we can make the two nodes work together. On TACC systems, the \"idev\" command will start an interactive session on a compute node: $ idev -m 60 -p normal -N 2 -n 128 Once you have nodes at your disposal and a container on DockerHub, invoking it would look something like: module load tacc-singularity cd $WORK singularity pull docker://USERNAME/pi-estimator:0.1-mpi time singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000 time ibrun singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000 Warning TACC uses a command called ibrun on all of its systems that configures MPI to use the high-speed, low-latency network. If you are familiar with MPI, this is the functional equivalent to mpirun The first singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000 command will use 1 CPU core to sample ten million times. The second command, using ibrun will run 128 processes that sample ten million times each and pass their results back to the \"rank 0\" MPI process to merge the results. This will use the host MPI libraries to run in parallel, and assuming the image has what it needs, can work across many nodes. As an aside, for a single node you can also use the container MPI to run in parallel (but usually you don't want this). When you are don with your interactive session, don't forget to exit to end the session and go back to the login node. Singularity and GPU Computing \u00b6 GPU support in Singularity is very good. Since Singularity supported docker containers, it has been fairly simple to utilize GPUs for machine learning code like TensorFlow. We will not do this as a hands-on exercise, but in general the procedule is as follows. # Load the singularity module module load singularity/3/3.1 # Pull your image singularity pull docker://nvidia/caffe:latest singularity exec --nv caffe-latest.sif caffe device_query -gpu 0 Please note that the --nv flag specifically passes the GPU drivers into the container. If you leave it out, the GPU will not be detected. # this is missing the --nv flag and will not work singularity exec caffe-latest.sif caffe device_query -gpu 0 The main requirement for GPU containers to work is that the version of the host drivers matches the major version of the library inside the container. So, for example, if CUDA 10 is on the host, the container needs to use CUDA 10 internally. For TensorFlow, you can directly pull their latest GPU image and utilize it as follows. # Change to your $WORK directory cd $WORK #Get the software git clone https://github.com/tensorflow/models.git ~/models # Pull the image singularity pull docker://tensorflow/tensorflow:latest-gpu # Run the code singularity exec --nv tensorflow-latest-gpu.sif python $HOME/models/tutorials/image/mnist/convolutional.py The University of Arizona HPS Singularity examples .","title":"Using Singularity on HPC/HTC"},{"location":"singularity/hpc/#singularity-and-high-performance-computing","text":"High Performance Computing resources fill an important role in research computing and can support container execution through runtimes such as Singularity or, hopefully soon, rootless Docker, among other options. Conducting analyses on HPC clusters happens through different patterns of interaction than running analyses on a cloud VM. When you login, you are on a node that is shared with lots of people, typically called the \"login node\". Trying to run jobs on the login node is not \"high performance\" at all (and will likely get you an admonishing email from the system administrator). Login nodes are intended to be used for moving files, editing files, and launching jobs. Importantly, most jobs run on an HPC cluster are neither interactive , nor realtime . When you submit a job to the scheduler, you must tell it what resources you need (e.g. how many nodes, how much RAM, what type of nodes, and for how long) in addition to what you want to run. Then the scheduler finally has resources matching your requirements, it runs the job for you. If your request is very large, or very long, you may never make it out of the queue. For example, on a VM if you run the command: singularity exec docker://python:latest /usr/local/bin/python The container will immediately start. On an HPC system, your job submission script would look something like: #!/bin/bash # #SBATCH -J myjob # Job name #SBATCH -o output.%j # Name of stdout output file (%j expands to jobId) #SBATCH -p development # Queue name #SBATCH -N 1 # Total number of nodes requested (68 cores/node) #SBATCH -n 17 # Total number of mpi tasks requested #SBATCH -t 02:00:00 # Run time (hh:mm:ss) - 4 hours module load singularity/3/3.1 singularity exec docker://python:latest /usr/local/bin/python This example is for the Slurm scheduler. Each of the #SBATCH lines looks like a comment to the bash kernel, but the scheduler reads all those lines to know what resources to reserve for you. It is usually possible to get an interactive session as well, by using an interactive flag, -i. Warning Every HPC cluster is a little different, but they almost universally have a \"User's Guide\" that serves both as a quick reference for helpful commands and contains guidelines for how to be a \"good citizen\" while using the system. For TACC's Stampede2 system, see the user guide . For The University of Arizona, see the user guide .","title":"Singularity and High Performance Computing"},{"location":"singularity/hpc/#how-do-hpc-systems-fit-into-the-development-workflow","text":"A few things to consider when using HPC systems: Using sudo is not allowed on HPC systems, and building a Singularity container from scratch requires sudo. That means you have to build your containers on a different development system. You can pull a docker image on HPC systems If you need to edit text files, command line text editors don't support using a mouse, so working efficiently has a learning curve. There are text editors that support editing files over SSH. This lets you use a local text editor and just save the changes to the HPC system. These constraints make HPC systems perfectly suitable for execution environments, but currently a limiting choice for a development environment. We usually recommend your local laptop or a VM as a development environment where you can iterate on your code rapidly and test container building and execution.","title":"How do HPC systems fit into the development workflow?"},{"location":"singularity/hpc/#singularity-and-mpi","text":"Singularity supports MPI fairly well. Since (by default) the network is the same insde and outside the container, the communication between containers usually just works. The more complicated bit is making sure that the container has the right set of MPI libraries. MPI is an open specification, but there are several implementations (OpenMPI, MVAPICH2, and Intel MPI to name three) with some non-overlapping feature sets. If the host and container are running different MPI implementations, or even different versions of the same implementation, hilarity may ensue. The general rule is that you want the version of MPI inside the container to be the same version or newer than the host. You may be thinking that this is not good for the portability of your container, and you are right. Containerizing MPI applications is not terribly difficult with Singularity, but it comes at the cost of additional requirements for the host system. Warning Many HPC Systems, like Stampede2 at TACC and Ocelote at UAHPC, have high-speed, low-latency networks that have special drivers. Infiniband, Ares, and OmniPath are three different specs for these types of networks. When running MPI jobs, if the container doesn't have the right libraries, it won't be able to use those special interconnects to communicate between nodes.","title":"Singularity and MPI"},{"location":"singularity/hpc/#base-docker-images","text":"Depending on the system you will use, you may have to build your own MPI enabled Singularity images (to get the versions to match). When running at TACC, there is a set of curated Docker images for use in the FROM line of your own containers. You can see a list of availabe images at https://hub.docker.com/u/tacc Specifically, you can use the tacc/tacc-ubuntu18-mvapich2.3-psm2 image to satisfy the MPI architecture and version requirements for running on Stampede2. Because you may have to build your own MPI enabled Singularity images (to get the versions to match), here is a 3.1 compatible example of what it may look like: BootStrap: debootstrap OSVersion: xenial MirrorURL: http://us.archive.ubuntu.com/ubuntu/ %runscript echo \"This is what happens when you run the container...\" %post echo \"Hello from inside the container\" sed -i 's/$/ universe/' /etc/apt/sources.list apt update apt -y --allow-unauthenticated install vim build-essential wget gfortran bison libibverbs-dev libibmad-dev libibumad-dev librdmacm-dev libmlx5-dev libmlx4-dev wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.1.tar.gz tar xvf mvapich2-2.1.tar.gz cd mvapich2-2.1 ./configure --prefix=/usr/local make -j4 make install /usr/local/bin/mpicc examples/hellow.c -o /usr/bin/hellow You could also build in everything in a Dockerfile and convert the image to Singularity at the end. Once you have a working MPI container, invoking it would look something like: mpirun -np 4 singularity exec ./mycontainer.sif /app.py arg1 arg2 This will use the host MPI libraries to run in parallel, and assuming the image has what it needs, can work across many nodes. For a single node, you can also use the container MPI to run in parallel (usually you don't want this) singularity exec ./mycontainer.sif mpirun -np 4 /app.py arg1 arg2","title":"Base Docker images"},{"location":"singularity/hpc/#example-containerized-mpi-app","text":"In your Docker development environment, make a new directory in which to build up a new image and download (or copy and paste) two files in that directory: https://raw.githubusercontent.com/TACC/containers_at_tacc/master/docs/scripts/Dockerfile.mpi https://raw.githubusercontent.com/TACC/containers_at_tacc/master/docs/scripts/pi-mpi.py Take a look at both files. pi-mpi.py is a simple MPI Python script that approximates pi (very inefficiently) by random sampling. Dockerfile.mpi is an updated Dockerfile that uses the TACC base image to satisfy all the MPI requirements on Stampede2. Next, try building the new container. $ docker build -t USERNAME/pi-estimator:0.1-mpi -f Dockerfile.mpi . Don't forget to change USERNAME to your DockerHub username. Once you have successfully built an image, push it up to DockerHub with the docker push command so that we can pull it back down on Stampede2.","title":"Example Containerized MPI App"},{"location":"singularity/hpc/#running-an-mpi-container-on-stampede2","text":"To test, we can grab an interactive session that has two nodes. That way we can see if we can make the two nodes work together. On TACC systems, the \"idev\" command will start an interactive session on a compute node: $ idev -m 60 -p normal -N 2 -n 128 Once you have nodes at your disposal and a container on DockerHub, invoking it would look something like: module load tacc-singularity cd $WORK singularity pull docker://USERNAME/pi-estimator:0.1-mpi time singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000 time ibrun singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000 Warning TACC uses a command called ibrun on all of its systems that configures MPI to use the high-speed, low-latency network. If you are familiar with MPI, this is the functional equivalent to mpirun The first singularity exec pi-estimator_0.1-mpi.sif pi-mpi.py 10000000 command will use 1 CPU core to sample ten million times. The second command, using ibrun will run 128 processes that sample ten million times each and pass their results back to the \"rank 0\" MPI process to merge the results. This will use the host MPI libraries to run in parallel, and assuming the image has what it needs, can work across many nodes. As an aside, for a single node you can also use the container MPI to run in parallel (but usually you don't want this). When you are don with your interactive session, don't forget to exit to end the session and go back to the login node.","title":"Running an MPI Container on Stampede2"},{"location":"singularity/hpc/#singularity-and-gpu-computing","text":"GPU support in Singularity is very good. Since Singularity supported docker containers, it has been fairly simple to utilize GPUs for machine learning code like TensorFlow. We will not do this as a hands-on exercise, but in general the procedule is as follows. # Load the singularity module module load singularity/3/3.1 # Pull your image singularity pull docker://nvidia/caffe:latest singularity exec --nv caffe-latest.sif caffe device_query -gpu 0 Please note that the --nv flag specifically passes the GPU drivers into the container. If you leave it out, the GPU will not be detected. # this is missing the --nv flag and will not work singularity exec caffe-latest.sif caffe device_query -gpu 0 The main requirement for GPU containers to work is that the version of the host drivers matches the major version of the library inside the container. So, for example, if CUDA 10 is on the host, the container needs to use CUDA 10 internally. For TensorFlow, you can directly pull their latest GPU image and utilize it as follows. # Change to your $WORK directory cd $WORK #Get the software git clone https://github.com/tensorflow/models.git ~/models # Pull the image singularity pull docker://tensorflow/tensorflow:latest-gpu # Run the code singularity exec --nv tensorflow-latest-gpu.sif python $HOME/models/tutorials/image/mnist/convolutional.py The University of Arizona HPS Singularity examples .","title":"Singularity and GPU Computing"},{"location":"singularity/intro/","text":"In this section we're going to be working with Singularity Community Edition (CE) Wait, what is \"Apptainer\", and what is the difference between SingularityCE and Apptainer? The Singularity project was split into multiple projects managed by different organizations since it was created in 2017. In a nutshell: Greg Kurtzer founded the Singularity project while at the Lawrence Berkeley National Laboratory Kurtzer created Sylabs , a private company, around Singularity Kurtzer left Sylabs to focus on CIQ , another private company, and moved Singularity to HPCng (a Community Org) Sylabs forked Singularity for control and professionially licensed support creating Singularity Community Edition\" HPCng gave the official project to Linux Foundation and renamed it \" Apptainer \" Apptainer is being marketed by CIQ At the present time, Apptainer and Singularity CE have highly similar syntax and will run Singularity .sif images interoperably Docker vs SingularityCE & Apptainer Apptainer and SingularityCE are 100% compatible with Docker but they do have some distinct differences Docker Docker containers run as root This privilege is almost never supported by administrators of High Performance Computing (HPC) centers. Meaning Docker is not, and will likely never be, installed natively on your HPC cluster. uses compressed layers to create one image SingularityCE & Apptainer : Same user and group identity inside as outside the container User only has root privileges if elevated with sudo when the container is run Can run and modify any existing Docker image These key differences allow Singularity to be installed on most HPC centers. Because you can run virtually all Docker containers in Singularity, you can effectively run Docker on an HPC. SingularityCE Installation \u00b6 Sylabs Singularity Community Edition (CE) homepage: https://www.sylabs.io/docs/ Apptainer Linux Foundation homepage: https://apptainer.org/ Conda \u00b6 SingularityCE or Apptainer can both be installed from Conda: conda install -c conda-forge singularityce conda install -c conda-forge apptainer Install Locally \u00b6 To Install Singularity follow the instructions for your specific OS: SingularityCE Module loading on HPC \u00b6 If you are interested in working with SingularityCE on HPC, you may need to contact your systems administrator and request they install SingularityCE . Because SingularityCE ideally needs setuid, your admins may have some qualms about giving SingularityCE this privilege. If that is the case, you might consider forwarding this letter to your admins. Most HPC systems are running Environment Modules with the simple command module . You can check to see what is available: $ module avail singularity If Singularity is listed as being installed, load a specific version, e.g.: $ module load singularity/3/3.9 Install in CodeSpaces \u00b6 Let's use Conda (or optionally, Mamba) conda install -c conda-forge singularityce mamba install -c conda-forge singularityce Singularity CLI \u00b6 Singularity\u2019s command line interface allows you to build and interact with containers transparently. You can run programs inside a container as if they were running on your host system. You can easily redirect IO, use pipes, pass arguments, and access files, sockets, and ports on the host system from within a container. help \u00b6 The help command gives an overview of Singularity options and subcommands as follows: $ singularity help pull Pull an image from a URI Usage: singularity pull [pull options...] [output file] <URI> Description: The 'pull' command allows you to download or build a container from a given URI. Supported URIs include: library: Pull an image from the currently configured library library://user/collection/container[:tag] docker: Pull a Docker/OCI image from Docker Hub, or another OCI registry. docker://user/image:tag shub: Pull an image from Singularity Hub shub://user/image:tag oras: Pull a SIF image from an OCI registry that supports ORAS. oras://registry/namespace/image:tag http, https: Pull an image using the http(s?) protocol https://library.sylabs.io/v1/imagefile/library/default/alpine:latest Options: --arch string architecture to pull from library (default \"amd64\") --dir string download images to the specific directory --disable-cache dont use cached images/blobs and dont create them --docker-login login to a Docker Repository interactively -F, --force overwrite an image file if it exists -h, --help help for pull --library string download images from the provided library --no-cleanup do NOT clean up bundle after failed build, can be helpful for debugging --no-https use http instead of https for docker:// oras:// and library://<hostname>/... URIs Examples: From Sylabs cloud library $ singularity pull alpine.sif library://alpine:latest From Docker $ singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest From Shub $ singularity pull singularity-images.sif shub://vsoch/singularity-images From supporting OCI registry (e.g. Azure Container Registry) $ singularity pull image.sif oras://<username>.azurecr.io/namespace/image:tag For additional help or support, please visit https://www.sylabs.io/docs/ search \u00b6 Just like with Docker, you can search the Singularity container registries for images. singularity search tensorflow pull \u00b6 The easiest way to use a Singularity is to pull an existing container from one of the Registries. singularity pull library://lolcow singularity pull docker://<yourusername>/cowsay Downloading pre-built images \u00b6 You can use the pull command to download pre-built images from a number of Container Registries, here we'll be focusing on the Singularity-Hub or DockerHub . Container Registries: library:// - images hosted on Sylabs Cloud docker:// - images hosted on Docker Hub localimage:// - images saved on your machine yum:// - yum based systems such as CentOS and Scientific Linux debootstrap:// - apt based systems such as Debian and Ubuntu arch:// - Arch Linux busybox:// - BusyBox zypper:// - zypper based systems such as Suse and OpenSuse shub:// - (archived) images hosted on Singularity Hub, no longer maintained Pulling an image from Singularity Hub \u00b6 Similar to previous example, in this example I am pulling a base Ubuntu container from Singularity-Hub: $ singularity pull shub://singularityhub/ubuntu WARNING: Authentication token file not found : Only pulls of public images will succeed 88.58 MiB / 88.58 MiB [===============================================================================================] 100.00% 31.86 MiB/s 2s You can rename the container using the --name flag: $ singularity pull --name ubuntu_test.simg shub://singularityhub/ubuntu WARNING: Authentication token file not found : Only pulls of public images will succeed 88.58 MiB / 88.58 MiB [===============================================================================================] 100.00% 35.12 MiB/s 2s The above command will save the alpine image from the Container Library as alpine.sif Pulling an image from Docker Hub \u00b6 This example pulls an ubuntu:16.04 image from DockerHub and saves it to the working directory. $ singularity pull docker://ubuntu:20.04 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 8f6b7df711c8 done Copying blob 0703c52b8763 done Copying blob 07304348ce1b done Copying blob 4795dceb8869 done Copying config 05ac933964 done Writing manifest to image destination Storing signatures 2020/03/09 16:14:12 info unpack layer: sha256:8f6b7df711c8a4733138390ff2aba1bfeb755bf4736c39c6e4858076c40fb5eb 2020/03/09 16:14:13 info unpack layer: sha256:0703c52b8763604318dcbb1730c82ad276a487335ecabde2f43f69a6222e8090 2020/03/09 16:14:13 info unpack layer: sha256:07304348ce1b6d24f136a3c4ebaa800297b804937a6942ce9e9fe0dac0b0ca74 2020/03/09 16:14:13 info unpack layer: sha256:4795dceb8869bdfa64f3742e1df492e6f31baf9cfc36f1a042a8f981607e99a2 INFO: Creating SIF file... INFO: Build complete: ubuntu_20.04.sif Warning Pulling Docker images reduces reproducibility. If you were to pull a Docker image today and then wait six months and pull again, you are not guaranteed to get the same image. If any of the source layers has changed the image will be altered. If reproducibility is a priority for you, try building your images from the Container Library. Pulling an image from Sylabs cloud library \u00b6 Let\u2019s use an easy example of alpine.sif image from the container library Tip You can use singularity search <name> command to locate groups, collections, and containers of interest on the Container Library Interact with images \u00b6 You can interact with images in several ways such as shell , exec and run . For these examples we will use a cowsay_latest.sif image that can be pulled from the Docker Hub. $ singularity pull docker://tswetnam/cowsay INFO: Downloading library image 67.00 MiB / 67.00 MiB [=====================================================================================================] 100.00% 5.45 MiB/s 12s WARNING: unable to verify container: cowsay_latest.sif WARNING: Skipping container verification $ sudo singularity run cowsay_latest.sif ________________________________________ / Expect a letter from a friend who will \\ \\ ask a favor of you. / ---------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || shell \u00b6 The shell command allows you to spawn a new shell within your container and interact with it as though it were a small virtual machine. $ singularity shell cowsay_latest.sif Singularity cowsay_latest.sif:~> The change in prompt indicates that you have entered the container (though you should not rely on that to determine whether you are in container or not). Once inside of a Singularity container, you are the same user as you are on the host system. $ Singularity cowsay_latest.sif:~> whoami tswetnam Warning shell also works with the library://, docker://, and shub:// URIs. This creates an ephemeral container that disappears when the shell is exited. exec \u00b6 The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the cowsay program within the cowsay_latest.sif container: $ singularity exec cowsay_latest.sif cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || exec also works with the library://, docker://, and shub:// URIs. singularity exec library://lolcow container camp 2022 This creates an ephemeral container that executes a command and disappears. run \u00b6 Singularity containers contain runscripts . These are user defined scripts that define the actions a container should perform when someone runs it. The runscript can be triggered with the run command, or simply by calling the container as though it were an executable. singularity run lolcow_latest.sif _________________________________________ / You will remember, Watson, how the \\ | dreadful business of the Abernetty | | family was first brought to my notice | | by the depth which the parsley had sunk | | into the butter upon a hot day. | | | \\ -- Sherlock Holmes / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || inspect \u00b6 The inspect command will provide information about labels, metadata, and environmental variables. singularity inspect lolcow.sif singularity inspect library://lolcow build \u00b6 See Next Section for details of build","title":"Introduction to Singularity"},{"location":"singularity/intro/#singularityce-installation","text":"Sylabs Singularity Community Edition (CE) homepage: https://www.sylabs.io/docs/ Apptainer Linux Foundation homepage: https://apptainer.org/","title":"SingularityCE Installation"},{"location":"singularity/intro/#conda","text":"SingularityCE or Apptainer can both be installed from Conda: conda install -c conda-forge singularityce conda install -c conda-forge apptainer","title":"Conda"},{"location":"singularity/intro/#install-locally","text":"To Install Singularity follow the instructions for your specific OS: SingularityCE","title":"Install Locally"},{"location":"singularity/intro/#module-loading-on-hpc","text":"If you are interested in working with SingularityCE on HPC, you may need to contact your systems administrator and request they install SingularityCE . Because SingularityCE ideally needs setuid, your admins may have some qualms about giving SingularityCE this privilege. If that is the case, you might consider forwarding this letter to your admins. Most HPC systems are running Environment Modules with the simple command module . You can check to see what is available: $ module avail singularity If Singularity is listed as being installed, load a specific version, e.g.: $ module load singularity/3/3.9","title":"Module loading on HPC"},{"location":"singularity/intro/#install-in-codespaces","text":"Let's use Conda (or optionally, Mamba) conda install -c conda-forge singularityce mamba install -c conda-forge singularityce","title":"Install in CodeSpaces"},{"location":"singularity/intro/#singularity-cli","text":"Singularity\u2019s command line interface allows you to build and interact with containers transparently. You can run programs inside a container as if they were running on your host system. You can easily redirect IO, use pipes, pass arguments, and access files, sockets, and ports on the host system from within a container.","title":"Singularity CLI"},{"location":"singularity/intro/#help","text":"The help command gives an overview of Singularity options and subcommands as follows: $ singularity help pull Pull an image from a URI Usage: singularity pull [pull options...] [output file] <URI> Description: The 'pull' command allows you to download or build a container from a given URI. Supported URIs include: library: Pull an image from the currently configured library library://user/collection/container[:tag] docker: Pull a Docker/OCI image from Docker Hub, or another OCI registry. docker://user/image:tag shub: Pull an image from Singularity Hub shub://user/image:tag oras: Pull a SIF image from an OCI registry that supports ORAS. oras://registry/namespace/image:tag http, https: Pull an image using the http(s?) protocol https://library.sylabs.io/v1/imagefile/library/default/alpine:latest Options: --arch string architecture to pull from library (default \"amd64\") --dir string download images to the specific directory --disable-cache dont use cached images/blobs and dont create them --docker-login login to a Docker Repository interactively -F, --force overwrite an image file if it exists -h, --help help for pull --library string download images from the provided library --no-cleanup do NOT clean up bundle after failed build, can be helpful for debugging --no-https use http instead of https for docker:// oras:// and library://<hostname>/... URIs Examples: From Sylabs cloud library $ singularity pull alpine.sif library://alpine:latest From Docker $ singularity pull tensorflow.sif docker://tensorflow/tensorflow:latest From Shub $ singularity pull singularity-images.sif shub://vsoch/singularity-images From supporting OCI registry (e.g. Azure Container Registry) $ singularity pull image.sif oras://<username>.azurecr.io/namespace/image:tag For additional help or support, please visit https://www.sylabs.io/docs/","title":" help"},{"location":"singularity/intro/#search","text":"Just like with Docker, you can search the Singularity container registries for images. singularity search tensorflow","title":" search"},{"location":"singularity/intro/#pull","text":"The easiest way to use a Singularity is to pull an existing container from one of the Registries. singularity pull library://lolcow singularity pull docker://<yourusername>/cowsay","title":" pull"},{"location":"singularity/intro/#downloading-pre-built-images","text":"You can use the pull command to download pre-built images from a number of Container Registries, here we'll be focusing on the Singularity-Hub or DockerHub . Container Registries: library:// - images hosted on Sylabs Cloud docker:// - images hosted on Docker Hub localimage:// - images saved on your machine yum:// - yum based systems such as CentOS and Scientific Linux debootstrap:// - apt based systems such as Debian and Ubuntu arch:// - Arch Linux busybox:// - BusyBox zypper:// - zypper based systems such as Suse and OpenSuse shub:// - (archived) images hosted on Singularity Hub, no longer maintained","title":"Downloading pre-built images"},{"location":"singularity/intro/#pulling-an-image-from-singularity-hub","text":"Similar to previous example, in this example I am pulling a base Ubuntu container from Singularity-Hub: $ singularity pull shub://singularityhub/ubuntu WARNING: Authentication token file not found : Only pulls of public images will succeed 88.58 MiB / 88.58 MiB [===============================================================================================] 100.00% 31.86 MiB/s 2s You can rename the container using the --name flag: $ singularity pull --name ubuntu_test.simg shub://singularityhub/ubuntu WARNING: Authentication token file not found : Only pulls of public images will succeed 88.58 MiB / 88.58 MiB [===============================================================================================] 100.00% 35.12 MiB/s 2s The above command will save the alpine image from the Container Library as alpine.sif","title":"Pulling an image from Singularity Hub"},{"location":"singularity/intro/#pulling-an-image-from-docker-hub","text":"This example pulls an ubuntu:16.04 image from DockerHub and saves it to the working directory. $ singularity pull docker://ubuntu:20.04 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 8f6b7df711c8 done Copying blob 0703c52b8763 done Copying blob 07304348ce1b done Copying blob 4795dceb8869 done Copying config 05ac933964 done Writing manifest to image destination Storing signatures 2020/03/09 16:14:12 info unpack layer: sha256:8f6b7df711c8a4733138390ff2aba1bfeb755bf4736c39c6e4858076c40fb5eb 2020/03/09 16:14:13 info unpack layer: sha256:0703c52b8763604318dcbb1730c82ad276a487335ecabde2f43f69a6222e8090 2020/03/09 16:14:13 info unpack layer: sha256:07304348ce1b6d24f136a3c4ebaa800297b804937a6942ce9e9fe0dac0b0ca74 2020/03/09 16:14:13 info unpack layer: sha256:4795dceb8869bdfa64f3742e1df492e6f31baf9cfc36f1a042a8f981607e99a2 INFO: Creating SIF file... INFO: Build complete: ubuntu_20.04.sif Warning Pulling Docker images reduces reproducibility. If you were to pull a Docker image today and then wait six months and pull again, you are not guaranteed to get the same image. If any of the source layers has changed the image will be altered. If reproducibility is a priority for you, try building your images from the Container Library.","title":"Pulling an image from Docker Hub"},{"location":"singularity/intro/#pulling-an-image-from-sylabs-cloud-library","text":"Let\u2019s use an easy example of alpine.sif image from the container library Tip You can use singularity search <name> command to locate groups, collections, and containers of interest on the Container Library","title":"Pulling an image from Sylabs cloud library"},{"location":"singularity/intro/#interact-with-images","text":"You can interact with images in several ways such as shell , exec and run . For these examples we will use a cowsay_latest.sif image that can be pulled from the Docker Hub. $ singularity pull docker://tswetnam/cowsay INFO: Downloading library image 67.00 MiB / 67.00 MiB [=====================================================================================================] 100.00% 5.45 MiB/s 12s WARNING: unable to verify container: cowsay_latest.sif WARNING: Skipping container verification $ sudo singularity run cowsay_latest.sif ________________________________________ / Expect a letter from a friend who will \\ \\ ask a favor of you. / ---------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":"Interact with images"},{"location":"singularity/intro/#shell","text":"The shell command allows you to spawn a new shell within your container and interact with it as though it were a small virtual machine. $ singularity shell cowsay_latest.sif Singularity cowsay_latest.sif:~> The change in prompt indicates that you have entered the container (though you should not rely on that to determine whether you are in container or not). Once inside of a Singularity container, you are the same user as you are on the host system. $ Singularity cowsay_latest.sif:~> whoami tswetnam Warning shell also works with the library://, docker://, and shub:// URIs. This creates an ephemeral container that disappears when the shell is exited.","title":" shell"},{"location":"singularity/intro/#exec","text":"The exec command allows you to execute a custom command within a container by specifying the image file. For instance, to execute the cowsay program within the cowsay_latest.sif container: $ singularity exec cowsay_latest.sif cowsay container camp rocks ______________________ < container camp rocks > ---------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || exec also works with the library://, docker://, and shub:// URIs. singularity exec library://lolcow container camp 2022 This creates an ephemeral container that executes a command and disappears.","title":" exec"},{"location":"singularity/intro/#run","text":"Singularity containers contain runscripts . These are user defined scripts that define the actions a container should perform when someone runs it. The runscript can be triggered with the run command, or simply by calling the container as though it were an executable. singularity run lolcow_latest.sif _________________________________________ / You will remember, Watson, how the \\ | dreadful business of the Abernetty | | family was first brought to my notice | | by the depth which the parsley had sunk | | into the butter upon a hot day. | | | \\ -- Sherlock Holmes / ----------------------------------------- \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || ||","title":" run"},{"location":"singularity/intro/#inspect","text":"The inspect command will provide information about labels, metadata, and environmental variables. singularity inspect lolcow.sif singularity inspect library://lolcow","title":" inspect"},{"location":"singularity/intro/#build","text":"See Next Section for details of build","title":" build"}]}